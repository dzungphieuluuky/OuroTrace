{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup libraries","metadata":{}},{"cell_type":"code","source":"# Clone the latest github repo version\nimport os\ndef configure_environment_paths():\n    \"\"\"Detect environment and configure paths\"\"\"\n    try:\n        if \"google.colab\" in str(get_ipython()):\n            print(\"‚úÖ Environment: Google Colab\")\n            base_data_path = \"/content/\"\n            base_output_path = \"/content/output/\"\n            environment_name = \"colab\"\n        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n            print(\"‚úÖ Environment: Kaggle\")\n            base_data_path = \"/kaggle/input/\"\n            base_output_path = \"/kaggle/working/\"\n            environment_name = \"kaggle\"\n        else:\n            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n            base_data_path = \"./data/\"\n            base_output_path = \"./output/\"\n            environment_name = \"local\"\n    except NameError:\n        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n        base_data_path = \"./data/\"\n        base_output_path = \"./output/\"\n        environment_name = \"local\"\n\n    os.makedirs(base_output_path, exist_ok=True)\n    print(f\"üìÇ Data Path: {base_data_path}\")\n    print(f\"üì¶ Output Path: {base_output_path}\")\n\n    return base_data_path, base_output_path, environment_name\n\nINPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()\n\n!rm -r -f OuroTrace\n!git clone --branch claude https://github.com/dzungphieuluuky/OuroTrace.git\n%cd OuroTrace","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip uninstall -y transformers tokenizers accelerate -q\n!pip install \"transformers==4.56.0\" \"protobuf==5.29.3\" -q\n!pip install torch datasets -q\n!pip install pandas matplotlib seaborn tqdm wandb pyyaml\n!pip install bitsandbytes accelerate\n# !pip install -r requirements.txt\n!pip install --force-reinstall --no-cache-dir \"numpy<2.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config input/output path and clone latest repo","metadata":{}},{"cell_type":"code","source":"# Suppress warnings for clean output\nimport warnings\nimport os\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nos.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\nprint(\"‚úÖ Packages installed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"Built-in libraries\"\nimport re\nimport sys\nimport gc\nimport time\nimport json\nimport hashlib\nimport glob\nimport zipfile\nfrom io import StringIO\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nimport yaml\nimport logging\nimport random\n\n\"Deep learning and NLP libraries\"\nimport torch\nimport torch.nn.functional as F\nfrom transformers import (\n    AutoConfig, \n    AutoTokenizer, \n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    GenerationConfig,\n    logging as hf_logging\n)\n\n\"Data processing libraries\"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport wandb\nfrom tqdm.auto import tqdm\nfrom IPython import get_ipython\n\n# Configure logging\nlogging.getLogger(\"ContinuousBatchingLogger\").setLevel(logging.ERROR)\nhf_logging.set_verbosity_error()\n\n\nprint(f\"Python Version: {sys.version}\")\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA Version: {torch.version.cuda}\")\n!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# WANDB CONFIG \nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom src.config_loader import load_config_from_json, post_process_config\n\n# this is the fused version when single and batch use the same predict function\nfrom src.new_runner import run_batch_experiment\n\n# this is the original version when single and batch use different functions\n# from src.runner import run_batch_experiment\n\nfrom src.evaluation import analyze_experiment_results\n\n\n# 1. Load Configuration from JSON\nconfig = load_config_from_json('configs/batch_ouro_1.4b_thinking.json')\n\n# 2. Post-process (Convert 'torch.float16' string to object, generate timestamps)\nconfig = post_process_config(config)\n\nconfig[\"INFERENCE_STEPS\"] = [4]\n# config['MODEL']['dtype'] = torch.bfloat16\nconfig[\"reasoning_primitives\"][\"num_samples\"] = 5\n\n# 4. Execute\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nprint(f\"üïí Timestamp: {timestamp}\")\nprint(\"üöÄ Starting Experiment...\")\nacc_results, ppl_results, hol_results = run_batch_experiment(config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Save Results\ndf_acc = pd.DataFrame(acc_results)\ndf_ppl = pd.DataFrame(ppl_results)\ndf_hol = pd.DataFrame(hol_results)\nRUN_RESULTS_NAME = f\"run_{timestamp}\"\nos.makedirs(os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME), exist_ok=True)\nacc_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"result_stats_{timestamp}.csv\")\nppl_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"result_ppl_{timestamp}.csv\")\nhol_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"result_hol_{timestamp}.csv\")\ncfg_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"result_config_{timestamp}.yaml\")\n\ndf_acc.to_csv(acc_path, index=False)\nif not df_ppl.empty:\n    df_ppl.to_csv(ppl_path, index=False)\nif not df_hol.empty:\n    df_hol.to_csv(hol_path, index=False)\n# Save Config\ndef sanitize_config(cfg):\n    \"\"\"Convert config to YAML-safe format\"\"\"\n    clean = {}\n    for k, v in cfg.items():\n        if isinstance(v, dict):\n            clean[k] = sanitize_config(v)\n        elif str(type(v)).find('torch.') != -1:\n            clean[k] = str(v)\n        else:\n            clean[k] = v\n    return clean\n\nwith open(cfg_path, 'w') as f:\n    yaml.dump(sanitize_config(config), f)\n\nprint(f\"\\nüíæ Results saved to {OUTPUT_PATH}\")\n\n# 4. Visualization & Reporting\nif not df_acc.empty:\n    print(\"\\n\" + \"=\"*50 + \"\\nüìä VISUALIZATION\\n\" + \"=\"*50)\n    \n    # Summary Tables\n    # NOTE: The variable 'results_acc' is used here, assuming it holds the raw data \n    # (list of dicts) required by 'analyze_experiment_results'.\n    summary = analyze_experiment_results(acc_results)\n    print(\"\\n--- Summary Statistics ---\")\n    print(summary)\n    \n    # Plotting\n    try:\n        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n        \n        # Plot 1: Accuracy\n        acc_summary = df_acc.groupby(['task_type', 'ut_steps'])['is_correct'].mean().reset_index()\n        sns.barplot(data=acc_summary, x='ut_steps', y='is_correct', hue='task_type', ax=axes[0])\n        axes[0].set_title('Accuracy by UT Steps')\n        axes[0].set_ylabel('Accuracy')\n        axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n        \n        # Plot 2: Time\n        time_summary = df_acc.groupby(['task_type', 'ut_steps'])['generation_time'].mean().reset_index()\n        sns.barplot(data=time_summary, x='ut_steps', y='generation_time', hue='task_type', ax=axes[1])\n        axes[1].set_title('Inference Time (s) by UT Steps')\n        \n        # Plot 3: Token Count\n        sns.boxplot(data=df_acc, x='ut_steps', y='generated_tokens', hue='task_type', ax=axes[2])\n        axes[2].set_title('Generated Tokens Distribution')\n        \n        plt.tight_layout()\n        plt.show()\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Visualization error: {e}\")\nelse:\n    print(\"‚ö†Ô∏è No results to visualize.\")\n\nprint(\"\\nüèÅ Experiment Complete.\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport zipfile\n\ndef zip_all_run_folders(output_base_path: str):\n    os.makedirs(output_base_path, exist_ok=True)\n    \n    search_pattern = os.path.join(output_base_path, \"run_*\")\n    run_folders = glob.glob(search_pattern)\n    run_directories = [d for d in run_folders if os.path.isdir(d)]\n\n    if not run_directories:\n        print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c n√†o b·∫Øt ƒë·∫ßu b·∫±ng 'run_' trong '{output_base_path}'.\")\n        return\n\n    print(f\"üîç T√¨m th·∫•y {len(run_directories)} th∆∞ m·ª•c k·∫øt qu·∫£ ƒë·ªÉ n√©n.\")\n    \n    successful_zips = 0\n\n    for folder_path in run_directories:\n        folder_name = os.path.basename(folder_path)\n        zip_filename = os.path.join(output_base_path, f\"{folder_name}.zip\")\n        \n        try:\n            print(f\"\\n   -> ƒêang n√©n th∆∞ m·ª•c: {folder_name}...\")\n            \n            with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                for root, _, files in os.walk(folder_path):\n                    for file in files:\n                        file_path = os.path.join(root, file)\n                        arcname = os.path.relpath(file_path, os.path.dirname(folder_path))\n                        zipf.write(file_path, arcname)\n                        \n            print(f\"   ‚úÖ ƒê√£ t·∫°o file ZIP: {os.path.basename(zip_filename)}\")\n            successful_zips += 1\n            \n        except Exception as e:\n            print(f\"   ‚ùå L·ªói khi n√©n th∆∞ m·ª•c {folder_name}: {e}\")\n\n    print(f\"\\n‚úÖ HO√ÄN T·∫§T! ƒê√£ n√©n th√†nh c√¥ng {successful_zips} tr√™n {len(run_directories)} th∆∞ m·ª•c k·∫øt qu·∫£.\")\n\n\ntry:\n    if 'OUTPUT_PATH' in globals():\n        zip_all_run_folders(OUTPUT_PATH)\n    else:\n        print(\"OUTPUT_PATH not defined.\")\n        \nexcept NameError:\n    print(\"OUTPUT_PATH not defined.\")\nexcept Exception as e:\n    print(f\"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh n√©n: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Final Inspection:\\n\")\nprint(\"Top 20 Accuracy Report:\\n\")\nprint(df_acc.head(20))\nprint(f\"Full Response:\\n\")\nprint(df_acc['full_response'])\nprint(\"Perplexity Report:\\n\")\nprint(df_ppl.head(20))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_acc[['full_response', 'generated_tokens']])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}