{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 1: Environment Setup & Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 1: Environment Setup & Package Installation\n",
    "Purpose: Install required packages and suppress warnings\n",
    "\"\"\"\n",
    "\n",
    "# Install core packages\n",
    "!pip install --upgrade pip\n",
    "!pip uninstall -y transformers tokenizers accelerate -q\n",
    "!pip install \"transformers==4.56.0\" \"protobuf>=5.29.4\" -q\n",
    "!pip install torch datasets -q\n",
    "!pip install pandas matplotlib seaborn tqdm wandb pyyaml\n",
    "!pip install bitsandbytes accelerate\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 2: Core Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 2: Core Imports\n",
    "Purpose: Import all necessary libraries\n",
    "\"\"\"\n",
    "\"Built-in libraries\"\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import glob\n",
    "import zipfile\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import yaml\n",
    "import logging\n",
    "import random\n",
    "\n",
    "\"Deep learning and NLP libraries\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    logging as hf_logging\n",
    ")\n",
    "\n",
    "\"Data processing libraries\"\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Configure logging\n",
    "logging.getLogger(\"ContinuousBatchingLogger\").setLevel(logging.ERROR)\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 3: Environment Detection & Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 3: Environment Detection & Path Configuration\n",
    "Purpose: Detect runtime environment (Colab/Kaggle/Local) and set paths\n",
    "\"\"\"\n",
    "\n",
    "def configure_environment_paths():\n",
    "    \"\"\"Detect environment and configure paths\"\"\"\n",
    "    try:\n",
    "        if 'google.colab' in str(get_ipython()):\n",
    "            print(\"‚úÖ Environment: Google Colab\")\n",
    "            base_data_path = '/content/'\n",
    "            base_output_path = '/content/output/'\n",
    "            environment_name = 'colab'\n",
    "        elif os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n",
    "            print(\"‚úÖ Environment: Kaggle\")\n",
    "            base_data_path = '/kaggle/input/'\n",
    "            base_output_path = '/kaggle/working/'\n",
    "            environment_name = 'kaggle'\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n",
    "            base_data_path = './data/'\n",
    "            base_output_path = './output/'\n",
    "            environment_name = 'local'\n",
    "    except NameError:\n",
    "        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n",
    "        base_data_path = './data/'\n",
    "        base_output_path = './output/'\n",
    "        environment_name = 'local'\n",
    "    \n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    print(f\"üìÇ Data Path: {base_data_path}\")\n",
    "    print(f\"üì¶ Output Path: {base_output_path}\")\n",
    "    \n",
    "    return base_data_path, base_output_path, environment_name\n",
    "\n",
    "def auto_unzip_colab_content(target_dir='/content/', zip_extension='*.zip'):\n",
    "    \"\"\"Auto-extract zip files in Colab environment\"\"\"\n",
    "    if 'google.colab' not in str(get_ipython()):\n",
    "        return\n",
    "    \n",
    "    print(f\"üîé Scanning for {zip_extension} files...\")\n",
    "    zip_files = glob.glob(os.path.join(target_dir, zip_extension))\n",
    "    \n",
    "    for zip_path in zip_files:\n",
    "        file_name = os.path.basename(zip_path)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        expected_output = os.path.join(target_dir, base_name)\n",
    "        \n",
    "        if os.path.exists(expected_output) and os.listdir(expected_output):\n",
    "            print(f\"‚û°Ô∏è Skipping '{file_name}' (already extracted)\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"üìÇ Extracting: {file_name}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(target_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Initialize paths\n",
    "DATA_PATH, OUTPUT_PATH, ENV = configure_environment_paths()\n",
    "auto_unzip_colab_content(DATA_PATH)\n",
    "\n",
    "# Optional: WandB login\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    wandb_key = userdata.get('WANDB_API_KEY')\n",
    "    if wandb_key:\n",
    "        wandb.login(key=wandb_key)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 4: Data Generation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 4: Data Generation Utilities\n",
    "Purpose: Functions to create test datasets and perplexity data\n",
    "\"\"\"\n",
    "\n",
    "def create_test_datasets(config: dict) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generate algorithmic test datasets strictly matching ICLR 2025 specs:\n",
    "    1. N-ary Addition: Input-Output pairs, 3-digit operands, sum.\n",
    "    2. P-hop Induction: Sequence length 256, Alphabet 4, Chain embedded at random sorted indices.\n",
    "    3. Symbolic i-GSM: Hierarchy depth 4, strict Level i -> Level i+1 dependency, Modulo 7.\n",
    "    \"\"\"\n",
    "    test_data = {}\n",
    "    \n",
    "    # 1. N-ARY ADDITION (Unchanged, matches paper description)\n",
    "    if 'n_ary' in config:\n",
    "        n_ary_data = []\n",
    "        ops_levels = config['n_ary'].get('ops_levels', [8, 16, 24, 32])\n",
    "        num_samples = config['n_ary'].get('num_samples_per_level', 30)\n",
    "        \n",
    "        for n in ops_levels:\n",
    "            for _ in range(num_samples):\n",
    "                # Paper: sample operands [0, 999] uniformly\n",
    "                nums_int = [random.randint(0, 999) for _ in range(n)]\n",
    "                # Format: \"315 + 120 + ... =\"\n",
    "                nums_str = [str(x).zfill(3) for x in nums_int]\n",
    "                \n",
    "                prompt_str = \" + \".join(nums_str) + \" =\"\n",
    "                target_str = str(sum(nums_int))\n",
    "\n",
    "                n_ary_data.append({\n",
    "                    \"prompt\": prompt_str,\n",
    "                    \"expected_answer\": target_str,\n",
    "                    \"difficulty\": f\"{n}_ops\",\n",
    "                    \"task_type\": \"n_ary\"\n",
    "                })\n",
    "        test_data['n_ary'] = n_ary_data\n",
    "\n",
    "    # 2. P-HOP INDUCTION\n",
    "    # Description: \"Picking the sequence of p-hops randomly... shuffling them around in a sequence... \n",
    "    # sample remaining characters in place of filler tokens while respecting the p-hop order.\"\n",
    "    if 'p_hop' in config:\n",
    "        p_hop_data = []\n",
    "        alphabet = ['A', 'B', 'C', 'D']\n",
    "        seq_len = 256\n",
    "        hop_levels = config['p_hop'].get('hop_levels', [16, 24, 32])\n",
    "        num_samples = config['p_hop'].get('num_samples_per_level', 30)\n",
    "\n",
    "        for p in hop_levels:\n",
    "            for _ in range(num_samples):\n",
    "                # 1. Define the Chain: v_0 -> v_1 -> ... -> v_p\n",
    "                # The 'hop' logic implies identifying v_i and finding v_{i+1}\n",
    "                chain = [random.choice(alphabet) for _ in range(p + 1)]\n",
    "                \n",
    "                # 2. Embed in Sequence\n",
    "                # \"Shuffling them around... respecting p-hop order\" implies:\n",
    "                # We pick p+1 random positions in the 256-length sequence \n",
    "                # and place the chain items there in strictly increasing order.\n",
    "                indices = random.sample(range(seq_len), p + 1)\n",
    "                indices.sort()\n",
    "                \n",
    "                # Initialize sequence with \"filler\" logic\n",
    "                # \"Sample remaining characters... in place of filler tokens\"\n",
    "                seq = [random.choice(alphabet) for _ in range(seq_len)]\n",
    "                \n",
    "                # Overwrite fillers with the chain at the selected indices\n",
    "                for k, idx in enumerate(indices):\n",
    "                    seq[idx] = chain[k]\n",
    "                \n",
    "                # 3. Construct Prompt\n",
    "                # Input: Sequence + Start Node. Goal: Output the p-th hop (last item in chain).\n",
    "                seq_str = \"\".join(seq)\n",
    "                start_node = chain[0]\n",
    "                expected = chain[-1]\n",
    "                \n",
    "                full_prompt = f\"Sequence: {seq_str}. Start: {start_node}. Hop {p} times.\"\n",
    "                \n",
    "                p_hop_data.append({\n",
    "                    \"prompt\": full_prompt,\n",
    "                    \"expected_answer\": expected,\n",
    "                    \"difficulty\": f\"{p}_hops\",\n",
    "                    \"task_type\": \"p_hop\"\n",
    "                })\n",
    "        test_data['p_hop'] = p_hop_data\n",
    "\n",
    "    # 3. SYMBOLIC i-GSM\n",
    "    # Description: \"Hierarchy of depth 4... edges connect level i to level i+1... \n",
    "    # instance parameter is integer... arithmetic modulo 7... symbolic language\"\n",
    "    if 'igsm' in config:\n",
    "        igsm_data = []\n",
    "        num_total = config['igsm'].get('num_samples_total', 50)\n",
    "        \n",
    "        # Variable naming pool (e.g., E#I, K#N)\n",
    "        chars = \"ABCDEFGHIJKLMNOP\"\n",
    "        def get_var_name():\n",
    "            return f\"{random.choice(chars)}#{random.choice(chars)}\"\n",
    "\n",
    "        for _ in range(num_total):\n",
    "            # 1. Structure: Hierarchy Depth 4 (Levels 0 to 4)\n",
    "            # We assign variables to specific levels\n",
    "            levels = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "            all_vars_data = {} # {name: val}\n",
    "            equations = []\n",
    "            \n",
    "            # 2. Level 0: Roots (Constants)\n",
    "            # Create ~4 root entities\n",
    "            for _ in range(4):\n",
    "                name = get_var_name()\n",
    "                val = random.randint(0, 6) # Modulo 7\n",
    "                levels[0].append(name)\n",
    "                all_vars_data[name] = val\n",
    "                equations.append(f\"{name} := {val}\")\n",
    "\n",
    "            # 3. Levels 1 to 4: Dependencies\n",
    "            # \"Edges connect entities in level i to those in level i+1\"\n",
    "            for i in range(1, 5):\n",
    "                # Number of variables in this level\n",
    "                num_vars_in_level = random.randint(2, 4)\n",
    "                \n",
    "                for _ in range(num_vars_in_level):\n",
    "                    target_var = get_var_name()\n",
    "                    \n",
    "                    # Ensure unique names\n",
    "                    while target_var in all_vars_data:\n",
    "                        target_var = get_var_name()\n",
    "                    \n",
    "                    # Select 1 or 2 operands from the PREVIOUS level (i-1)\n",
    "                    # This enforces the strict hierarchy described\n",
    "                    operands = random.choices(levels[i-1], k=random.randint(1, 2))\n",
    "                    op_vals = [all_vars_data[op] for op in operands]\n",
    "                    \n",
    "                    op_type = random.choice(['add', 'sub', 'mult', 'assign'])\n",
    "                    \n",
    "                    stmt = \"\"\n",
    "                    res = 0\n",
    "                    \n",
    "                    if op_type == 'assign' or len(operands) < 2:\n",
    "                        stmt = f\"{target_var} := {operands[0]}\"\n",
    "                        res = op_vals[0]\n",
    "                    elif op_type == 'add':\n",
    "                        stmt = f\"{target_var} := {operands[0]} + {operands[1]}\"\n",
    "                        res = (op_vals[0] + op_vals[1]) % 7\n",
    "                    elif op_type == 'sub':\n",
    "                        stmt = f\"{target_var} := {operands[0]} - {operands[1]}\"\n",
    "                        res = (op_vals[0] - op_vals[1]) % 7\n",
    "                    elif op_type == 'mult':\n",
    "                        stmt = f\"{target_var} := {operands[0]} * {operands[1]}\"\n",
    "                        res = (op_vals[0] * op_vals[1]) % 7\n",
    "                        \n",
    "                    equations.append(stmt)\n",
    "                    all_vars_data[target_var] = res\n",
    "                    levels[i].append(target_var)\n",
    "\n",
    "            # 4. Query\n",
    "            # \"Pick one of the nodes... compute value\"\n",
    "            # We pick from the deepest level (Level 4) to ensure the full chain is needed\n",
    "            target_var = random.choice(levels[4])\n",
    "            target_val = all_vars_data[target_var]\n",
    "            \n",
    "            # Shuffle equations to ensure model learns the dependency graph, not just order\n",
    "            random.shuffle(equations)\n",
    "            \n",
    "            # Format: \"Question. Eq1. Eq2. ... EqN. Target?\"\n",
    "            full_prompt = \"Question. \" + \". \".join(equations) + f\". {target_var}?\"\n",
    "            \n",
    "            igsm_data.append({\n",
    "                \"prompt\": full_prompt,\n",
    "                \"expected_answer\": str(target_val),\n",
    "                \"difficulty\": \"depth_4_hierarchical_mod_7\",\n",
    "                \"task_type\": \"igsm\"\n",
    "            })\n",
    "            \n",
    "    test_data['igsm'] = igsm_data\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "def create_perplexity_data(num_samples: int = 30) -> List[str]:\n",
    "    \"\"\"Generate reasoning traces for perplexity calculation\"\"\"\n",
    "    perplexity_texts = []\n",
    "    \n",
    "    # N-ARY traces\n",
    "    for _ in range(num_samples // 2):\n",
    "        n = random.choice([4, 6, 8])\n",
    "        nums = [random.randint(10, 99) for _ in range(n)]\n",
    "        trace = f\"System: You are a calculation engine.\\nUser: Sum: {nums}\\nAssistant: Current Sum: 0\\n\"\n",
    "        \n",
    "        current_sum = 0\n",
    "        for num in nums:\n",
    "            prev_sum = current_sum\n",
    "            current_sum += num\n",
    "            trace += f\"Add {num}: {prev_sum} + {num} = {current_sum}\\nCurrent Sum: {current_sum}\\n\"\n",
    "        \n",
    "        trace += f\"Final: {current_sum}\"\n",
    "        perplexity_texts.append(trace)\n",
    "    \n",
    "    # P-HOP traces\n",
    "    all_letters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "    for _ in range(num_samples // 2):\n",
    "        hops = random.choice([3, 4, 5])\n",
    "        nodes = random.sample(all_letters, hops + 1)\n",
    "        facts = [f\"{nodes[i]}->{nodes[i+1]}\" for i in range(len(nodes) - 1)]\n",
    "        facts_str = \", \".join(facts)\n",
    "        \n",
    "        trace = f\"System: Logic engine.\\nUser: Facts: {facts_str}. Start: {nodes[0]}. Find: {nodes[-1]}.\\n\"\n",
    "        trace += f\"Assistant: Current Node: {nodes[0]}\\n\"\n",
    "        \n",
    "        for i in range(hops):\n",
    "            trace += f\"Rule Matches: {nodes[i]} -> {nodes[i+1]}\\nNext Node: {nodes[i+1]}\\n\"\n",
    "        \n",
    "        trace += f\"Final: {nodes[-1]}\"\n",
    "        perplexity_texts.append(trace)\n",
    "    \n",
    "    return perplexity_texts\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Load existing test data from JSON or CSV\"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    \n",
    "    if file_path.endswith('.json'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "    elif file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        raw_data = df.to_dict('records')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format. Use .json or .csv\")\n",
    "    \n",
    "    processed_data = {'n_ary': [], 'p_hop': [], 'igsm': []}\n",
    "    \n",
    "    for record in raw_data:\n",
    "        task = record.get('task_type')\n",
    "        if task in processed_data:\n",
    "            if all(k in record for k in ['prompt', 'expected_answer', 'difficulty']):\n",
    "                processed_data[task].append(record)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded - N-ary: {len(processed_data['n_ary'])}, \"\n",
    "          f\"P-Hop: {len(processed_data['p_hop'])}, iGSM: {len(processed_data['igsm'])}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def generate_test_id(task_type: str, difficulty: str, prompt: str) -> str:\n",
    "    \"\"\"Generate unique test ID\"\"\"\n",
    "    unique_str = f\"{task_type}_{difficulty}_{prompt}\"\n",
    "    return hashlib.md5(unique_str.encode()).hexdigest()[:8]\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data generation utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 5: Core Experiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 5: Core Experiment Class\n",
    "Purpose: Main class for running Ouro model experiments (Updated for ICLR 2025 Formats)\n",
    "\"\"\"\n",
    "\n",
    "class OuroThinkingExperiment:\n",
    "    \"\"\"Core experiment class for Ouro model testing\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, dtype=torch.float16, \n",
    "                 use_4bit_quant: bool = False, use_torch_compile: bool = False):\n",
    "        torch.cuda.empty_cache()\n",
    "        self.model_path = model_path\n",
    "        self.dtype = dtype\n",
    "        self.use_4bit_quant = use_4bit_quant\n",
    "        self.use_torch_compile = use_torch_compile\n",
    "        self.tokenizer = None\n",
    "        self.task_templates = {}\n",
    "    \n",
    "    def load_model_with_ut_steps(self, total_ut_steps: int, early_exit_threshold: float):\n",
    "        \"\"\"Load model with specific UT steps configuration\"\"\"\n",
    "        quantization_config = None\n",
    "        if self.use_4bit_quant:\n",
    "            print(\"‚Üí Applying 4-bit quantization\")\n",
    "            quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        \n",
    "        print(f\"Loading model: UT steps={total_ut_steps}, Early exit={early_exit_threshold}\")\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        config.total_ut_steps = total_ut_steps\n",
    "        config.early_exit_threshold = early_exit_threshold\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_path,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"left\"\n",
    "        )\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            config=config,\n",
    "            device_map=\"cuda\",\n",
    "            attn_implementation=\"sdpa_paged\",\n",
    "            dtype=self.dtype if not self.use_4bit_quant else None,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "        \n",
    "        if self.use_torch_compile:\n",
    "            print(\"‚Üí Applying torch.compile()\")\n",
    "            model = torch.compile(model)\n",
    "        \n",
    "        model.eval()\n",
    "        print(f\"‚úÖ Model loaded on {model.device}\")\n",
    "        \n",
    "        return model, tokenizer, None, {\n",
    "            \"total_ut_steps\": total_ut_steps,\n",
    "            \"early_exit_threshold\": early_exit_threshold\n",
    "        }\n",
    "    \n",
    "    def _build_task_templates(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Pre-compute prompt templates for faster inference.\n",
    "        UPDATED: Added spaces/newlines to force_start to prevent token gluing/contamination.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        task_configs = {\n",
    "            # 1. N-ARY ADDITION\n",
    "            # Input: \"100 + 200 =\" -> Expect \" 0 +\"\n",
    "            # Added leading space to force start to avoid \"=0\" glueing\n",
    "            \"n_ary\": {\n",
    "                \"system\": \"You are a mechanical calculation engine. Output the accumulation steps strictly.\",\n",
    "                \"example_user\": \"10 + 20 + 30 =\",\n",
    "                \"example_asst\": \"Current: 0\\nAdd 10: 0 + 10 = 10\\nCurrent: 10\\nAdd 20: 10 + 20 = 30\\nCurrent: 30\\nAdd 30: 30 + 30 = 60\\nFinal: 60\",\n",
    "                \"force_start\": \" Current: 0\", \n",
    "                \"input_prefix\": \"\" \n",
    "            },\n",
    "            \n",
    "            # 2. P-HOP INDUCTION\n",
    "            # Input: \"Sequence: ... Hop N times.\" -> Expect \"\\nStart at\"\n",
    "            # Added newline to force separation\n",
    "            \"p_hop\": {\n",
    "                \"system\": \"You are an induction head mechanism. Trace the sequence occurrences step-by-step.\",\n",
    "                \"example_user\": \"Sequence: A B C D A B. Start: A. Hop 1 times.\",\n",
    "                \"example_asst\": \"\\nStart at A. Found 'A' in sequence. Next token is B. Final: B\",\n",
    "                \"force_start\": \"\\nStart at\", \n",
    "                \"input_prefix\": \"\" \n",
    "            },\n",
    "            \n",
    "            # 3. SYMBOLIC i-GSM\n",
    "            # Input: \"... H#J?\" -> Expect \"\\nAnswer with CoT.\"\n",
    "            # Added newline to force separation\n",
    "            \"igsm\": {\n",
    "                \"system\": \"You are a symbolic math solver. Solve the DAG modulo 7.\",\n",
    "                \"example_user\": \"Question. E#I := 4. E#J := E#I. F#K := E#J. H#J := E#J + F#K. H#J?\",\n",
    "                \"example_asst\": \"\\nAnswer with CoT. E#I = 4. ==> E#I = 4. E#J = E#I. ==> E#J = 4. F#K = E#J. ==> F#K = 4. H#J = E#J + F#K. ==> H#J = 1. Final: 1\",\n",
    "                \"force_start\": \"\\nAnswer with CoT.\", \n",
    "                \"input_prefix\": \"\" \n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for task_type, config in task_configs.items():\n",
    "            # 1. Build static context\n",
    "            static_messages = [\n",
    "                {\"role\": \"system\", \"content\": config[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": config[\"example_user\"]},\n",
    "                {\"role\": \"assistant\", \"content\": config[\"example_asst\"]}\n",
    "            ]\n",
    "            \n",
    "            static_prompt_text = tokenizer.apply_chat_template(\n",
    "                static_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            static_inputs = tokenizer(static_prompt_text, return_tensors=\"pt\")\n",
    "            \n",
    "            # 2. Tokenize Force Start\n",
    "            force_start_tokens = tokenizer(\n",
    "                config[\"force_start\"], \n",
    "                return_tensors=\"pt\", \n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            \n",
    "            self.task_templates[task_type] = {\n",
    "                \"static_input_ids\": static_inputs.input_ids,\n",
    "                \"static_attention_mask\": static_inputs.attention_mask,\n",
    "                \"force_start_ids\": force_start_tokens.input_ids,\n",
    "                \"input_prefix\": config[\"input_prefix\"],\n",
    "                \"force_start_text\": config[\"force_start\"]\n",
    "            }\n",
    "        \n",
    "        print(\"[+] Task templates pre-computed (Corrected with spacers)\")\n",
    "    \n",
    "    def _extract_final_answer(self, full_response: str, task_type: str) -> str:\n",
    "        \"\"\"Extract answer from model response\"\"\"\n",
    "        pred = \"0\"\n",
    "        \n",
    "        try:\n",
    "            if task_type == \"p_hop\":\n",
    "                patterns = [\n",
    "                    r\"Final\\s*:\\s*(\\w+)\",\n",
    "                    r\"Next token is\\s*(\\w+)\",\n",
    "                    r\"Answer\\s*:\\s*(\\w+)\",\n",
    "                ]\n",
    "                for pattern in patterns:\n",
    "                    match = re.search(pattern, full_response, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        pred = match.group(1).strip()\n",
    "                        break\n",
    "                else:\n",
    "                    pred = \"Error\"\n",
    "            else:\n",
    "                patterns = [\n",
    "                    r\"Final\\s*:\\s*([-+]?\\d*\\.?\\d+)\",\n",
    "                    r\"Answer\\s*:\\s*([-+]?\\d*\\.?\\d+)\",\n",
    "                    r\"=\\s*([-+]?\\d*\\.?\\d+)$\" # Catches \"5 + 2 = 7\" at end\n",
    "                ]\n",
    "                all_matches = []\n",
    "                for pattern in patterns:\n",
    "                    matches = re.findall(pattern, full_response, re.IGNORECASE)\n",
    "                    all_matches.extend(matches)\n",
    "                \n",
    "                if all_matches:\n",
    "                    pred = all_matches[-1]\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Parsing error: {e}\")\n",
    "            pred = \"ParseError\"\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_with_metrics_optimized(self, user_input: str, task_type: str, \n",
    "                                      model, tokenizer, ut_steps: int, \n",
    "                                      generation_config: dict = None):\n",
    "        \"\"\"Optimized prediction with repetition penalty to prevent loops\"\"\"\n",
    "        if not hasattr(self, 'task_templates') or task_type not in self.task_templates:\n",
    "            self._build_task_templates(tokenizer)\n",
    "        \n",
    "        template = self.task_templates[task_type]\n",
    "        device = model.device\n",
    "        \n",
    "        # Construct Input\n",
    "        input_ids = template[\"static_input_ids\"].to(device)\n",
    "        user_query = template[\"input_prefix\"] + user_input\n",
    "        user_tokens = tokenizer(user_query, return_tensors=\"pt\", \n",
    "                               add_special_tokens=False).input_ids.to(device)\n",
    "        force_start_ids = template[\"force_start_ids\"].to(device)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, user_tokens, force_start_ids], dim=1)\n",
    "        attention_mask = torch.ones_like(input_ids, device=device)\n",
    "        \n",
    "        # Improved Generation Config\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        gen_config = generation_config or {\n",
    "            'max_new_tokens': 1024,\n",
    "            'do_sample': False,\n",
    "            'num_beams': 1,\n",
    "            'min_length': 5\n",
    "        }\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "            **gen_config\n",
    "        )\n",
    "        \n",
    "        generation_time = time.perf_counter() - start_time\n",
    "        \n",
    "        # Decode\n",
    "        prompt_length = input_ids.shape[1]\n",
    "        generated_ids = outputs.sequences[0, prompt_length:]\n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        full_response = template[\"force_start_text\"] + generated_text\n",
    "        pred = self._extract_final_answer(full_response, task_type)\n",
    "        \n",
    "        return {\n",
    "            'full_response': full_response,\n",
    "            'prediction': pred,\n",
    "            'generation_time': generation_time,\n",
    "            'generated_tokens': generated_ids.shape[0],\n",
    "            'input_tokens': input_ids.shape[1],\n",
    "            'ut_steps': ut_steps\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calculate_perplexity(self, model, tokenizer, text_data: List[str], \n",
    "                            ut_steps: int, max_length: int = 2048, stride: int = 512):\n",
    "        \"\"\"Calculate perplexity using sliding window\"\"\"\n",
    "        device = model.device\n",
    "        model.eval()\n",
    "        \n",
    "        if not text_data or not text_data[0]:\n",
    "            return float('nan'), float('nan')\n",
    "        \n",
    "        text_concat = text_data[0]\n",
    "        encodings = tokenizer(text_concat, return_tensors='pt', \n",
    "                            max_length=max_length * 2, truncation=True)\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        attention_mask = encodings.attention_mask.to(device)\n",
    "        \n",
    "        if input_ids.size(1) < 2:\n",
    "            return float('nan'), float('nan')\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for i in tqdm(range(0, input_ids.size(1), stride), \n",
    "                     desc=f\"Calculating PPL (UT={ut_steps})\"):\n",
    "            end_loc = min(i + max_length, input_ids.size(1))\n",
    "            input_slice = input_ids[:, i:end_loc]\n",
    "            target_slice = input_slice.clone()\n",
    "            \n",
    "            if i > 0:\n",
    "                context_len = input_slice.size(1) - stride\n",
    "                if context_len > 0:\n",
    "                    target_slice[:, :context_len] = -100\n",
    "            \n",
    "            if (target_slice != -100).sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_slice,\n",
    "                attention_mask=attention_mask[:, i:end_loc],\n",
    "                labels=target_slice\n",
    "            )\n",
    "            \n",
    "            if torch.isnan(outputs.loss):\n",
    "                continue\n",
    "            \n",
    "            num_valid = (target_slice != -100).sum().item()\n",
    "            total_loss += (outputs.loss * num_valid).item()\n",
    "            total_tokens += num_valid\n",
    "        \n",
    "        if total_tokens == 0:\n",
    "            return float('nan'), float('nan')\n",
    "        \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "        \n",
    "        return perplexity, avg_loss\n",
    "\n",
    "\n",
    "print(\"‚úÖ OuroThinkingExperiment class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 6: Batch Experiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 6: Batch Experiment Class\n",
    "Purpose: Extended class with batch processing support\n",
    "\"\"\"\n",
    "\n",
    "class OuroBatchExperiment(OuroThinkingExperiment):\n",
    "    \"\"\"Extended experiment class with batch processing\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, dtype=torch.float16, \n",
    "                 use_4bit_quant: bool = False, use_torch_compile: bool = False,\n",
    "                 max_batch_size: int = 4, max_new_tokens: int = 1024):\n",
    "        super().__init__(model_path, dtype, use_4bit_quant, use_torch_compile)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "    \n",
    "    def prepare_batch_inputs(self, prompts: List[str], task_type: str) -> List[List[int]]:\n",
    "        \"\"\"Prepare inputs for batch generation\"\"\"\n",
    "        if task_type not in self.task_templates:\n",
    "            raise ValueError(\"Templates not built. Call _build_task_templates first.\")\n",
    "        \n",
    "        template = self.task_templates[task_type]\n",
    "        \n",
    "        batch_texts = [template[\"input_prefix\"] + p for p in prompts]\n",
    "        user_encodings = self.tokenizer(batch_texts, add_special_tokens=False)\n",
    "        \n",
    "        static_ids = template[\"static_input_ids\"].squeeze(0).tolist()\n",
    "        force_ids = template[\"force_start_ids\"].squeeze(0).tolist()\n",
    "        \n",
    "        input_id_lists = []\n",
    "        for user_ids in user_encodings['input_ids']:\n",
    "            full_seq = static_ids + user_ids + force_ids\n",
    "            input_id_lists.append(full_seq)\n",
    "        \n",
    "        return input_id_lists\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def batch_predict_with_metrics(self, prompts: List[str], task_type: str,\n",
    "                                   model, tokenizer, ut_steps: int,\n",
    "                                   generation_config: Optional[GenerationConfig] = None):\n",
    "        \"\"\"Batch prediction with metrics\"\"\"\n",
    "        if not prompts:\n",
    "            return []\n",
    "        \n",
    "        simple_batch_inputs = self.prepare_batch_inputs(prompts, task_type)\n",
    "        input_lengths = [len(ids) for ids in simple_batch_inputs]\n",
    "        \n",
    "        if not hasattr(model, 'generate_batch'):\n",
    "            print(\"‚ö†Ô∏è Model doesn't support generate_batch(). Using sequential.\")\n",
    "            return self._sequential_fallback(prompts, task_type, model, \n",
    "                                            tokenizer, ut_steps, generation_config)\n",
    "        \n",
    "        if generation_config is None:\n",
    "            generation_config = GenerationConfig(\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                use_cuda_graph=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=False,\n",
    "                max_batch_tokens=self.max_batch_size * self.max_new_tokens,\n",
    "            )\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        try:\n",
    "            batch_outputs = model.generate_batch(\n",
    "                inputs=simple_batch_inputs,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è generate_batch failed: {e}. Falling back.\")\n",
    "            return self._sequential_fallback(prompts, task_type, model, \n",
    "                                            tokenizer, ut_steps, generation_config)\n",
    "        \n",
    "        batch_time = time.perf_counter() - start_time\n",
    "        \n",
    "        template = self.task_templates[task_type]\n",
    "        results = [None] * len(prompts)\n",
    "        request_ids = list(batch_outputs.keys())\n",
    "        \n",
    "        # Map outputs to prompts\n",
    "        if all(isinstance(rid, int) for rid in request_ids):\n",
    "            for request_id in request_ids:\n",
    "                if 0 <= request_id < len(prompts):\n",
    "                    output = batch_outputs[request_id]\n",
    "                    results[request_id] = self._process_single_output(\n",
    "                        output, request_id, input_lengths[request_id],\n",
    "                        template, tokenizer, task_type, ut_steps,\n",
    "                        batch_time / len(prompts)\n",
    "                    )\n",
    "        else:\n",
    "            # String/UUID request IDs\n",
    "            input_to_index = {\n",
    "                \" \".join(map(str, inp)): idx \n",
    "                for idx, inp in enumerate(simple_batch_inputs)\n",
    "            }\n",
    "            \n",
    "            for request_id in request_ids:\n",
    "                output = batch_outputs[request_id]\n",
    "                \n",
    "                if hasattr(output, 'prompt_ids'):\n",
    "                    input_key = \" \".join(map(str, output.prompt_ids))\n",
    "                    if input_key in input_to_index:\n",
    "                        idx = input_to_index[input_key]\n",
    "                        results[idx] = self._process_single_output(\n",
    "                            output, idx, len(output.prompt_ids),\n",
    "                            template, tokenizer, task_type, ut_steps,\n",
    "                            batch_time / len(prompts)\n",
    "                        )\n",
    "        \n",
    "        # Fill missing results\n",
    "        for i in range(len(prompts)):\n",
    "            if results[i] is None:\n",
    "                results[i] = {\n",
    "                    'full_response': 'ERROR: No output',\n",
    "                    'prediction': 'ERROR',\n",
    "                    'generation_time': batch_time / len(prompts),\n",
    "                    'generated_tokens': 0,\n",
    "                    'input_tokens': input_lengths[i],\n",
    "                    'ut_steps': ut_steps\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _process_single_output(self, output, prompt_idx: int, input_length: int,\n",
    "                               template: dict, tokenizer, task_type: str,\n",
    "                               ut_steps: int, sample_time: float):\n",
    "        \"\"\"Process single batch output\"\"\"\n",
    "        if hasattr(output, 'generated_tokens'):\n",
    "            generated_ids = output.generated_tokens\n",
    "        elif hasattr(output, 'sequences') and len(output.sequences) > 0:\n",
    "            generated_ids = output.sequences[0]\n",
    "        else:\n",
    "            generated_ids = []\n",
    "        \n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        full_response = template[\"force_start_text\"] + generated_text\n",
    "        \n",
    "        pred = self._extract_final_answer(full_response, task_type)\n",
    "        \n",
    "        return {\n",
    "            'full_response': full_response,\n",
    "            'prediction': pred,\n",
    "            'generation_time': sample_time,\n",
    "            'generated_tokens': len(generated_ids),\n",
    "            'input_tokens': input_length,\n",
    "            'ut_steps': ut_steps,\n",
    "            'prompt_idx': prompt_idx\n",
    "        }\n",
    "    \n",
    "    def _sequential_fallback(self, prompts, task_type, model, tokenizer, \n",
    "                            ut_steps, generation_config):\n",
    "        \"\"\"Fallback to sequential processing\"\"\"\n",
    "        results = []\n",
    "        for prompt in tqdm(prompts, desc=f\"Sequential fallback ({task_type})\"):\n",
    "            result = self.predict_with_metrics_optimized(\n",
    "                user_input=prompt,\n",
    "                task_type=task_type,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                ut_steps=ut_steps,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ OuroBatchExperiment class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 7: Main Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 7: Main Experiment Runner\n",
    "Purpose: Function to execute the full experiment pipeline with batching and logging\n",
    "\"\"\"\n",
    "\n",
    "def run_batch_experiment(config: dict):\n",
    "    \"\"\"\n",
    "    Run experiment with batching support and W&B logging.\n",
    "    Returns: (accuracy_results, perplexity_results)\n",
    "    \"\"\"\n",
    "    # 1. Initialize W&B\n",
    "    use_wandb = config.get('WANDB', {}).get('enabled', False)\n",
    "    run = None\n",
    "    \n",
    "    if use_wandb:\n",
    "        wb_conf = config['WANDB']\n",
    "        print(f\"üîó Initializing W&B (timeout: {wb_conf.get('timeout', 30)}s)...\")\n",
    "        \n",
    "        os.environ['WANDB__SERVICE_WAIT'] = '300'\n",
    "        \n",
    "        try:\n",
    "            run = wandb.init(\n",
    "                project=wb_conf.get('project', 'ouro-looped-transformer'),\n",
    "                entity=wb_conf.get('entity', None),\n",
    "                name=wb_conf.get('run_name', f\"run_{int(time.time())}\"),\n",
    "                config=config,\n",
    "                mode=wb_conf.get('mode', 'online'),\n",
    "                settings=wandb.Settings(start_timeout=wb_conf.get('timeout', 30), _disable_stats=True)\n",
    "            )\n",
    "            print(\"‚úÖ W&B initialized\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è W&B failed: {e}. Continuing offline.\")\n",
    "            use_wandb = False\n",
    "            run = None\n",
    "\n",
    "    # 2. Extract Config\n",
    "    model_path = config['MODEL']['path']\n",
    "    ut_steps_list = config['INFERENCE_STEPS']\n",
    "    data_config = config['DATA']\n",
    "    eval_settings = config['EVAL_SETTINGS']\n",
    "    \n",
    "    # 3. Setup Experiment\n",
    "    experiment = OuroBatchExperiment(\n",
    "        model_path, \n",
    "        dtype=config['MODEL']['dtype'],\n",
    "        use_4bit_quant=config['MODEL'].get('use_4bit_quant', True),\n",
    "        use_torch_compile=config['MODEL'].get('use_torch_compile', True),\n",
    "        max_batch_size=config.get('OPTIMIZATION', {}).get('max_batch_size', 4),\n",
    "        max_new_tokens=config.get('OPTIMIZATION', {}).get('max_new_token', 1024)\n",
    "    )\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 4. Prepare Data\n",
    "    if data_config['load_existing']:\n",
    "        test_datasets = load_and_preprocess_data(data_config['data_file_path'])\n",
    "    else:\n",
    "        print(\"Generating new test datasets...\")\n",
    "        test_datasets = create_test_datasets(data_config)\n",
    "    \n",
    "    perplexity_results = []\n",
    "    perplexity_data = []\n",
    "    if eval_settings['calculate_perplexity']:\n",
    "        raw_ppl_data = create_perplexity_data(eval_settings['ppl_num_samples'])\n",
    "        perplexity_data = [\"\\n\\n\".join(raw_ppl_data)]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # 5. Main Loop\n",
    "    for ut_steps in ut_steps_list:\n",
    "        print(f\"\\n{'='*60}\\nüß™ EXPERIMENT: UT Steps = {ut_steps}\\n{'='*60}\")\n",
    "        \n",
    "        # Load fresh model\n",
    "        model, tokenizer, _, _ = experiment.load_model_with_ut_steps(\n",
    "            ut_steps, eval_settings['early_exit_threshold']\n",
    "        )\n",
    "        \n",
    "        if not hasattr(experiment, '_templates_precomputed'):\n",
    "            experiment._build_task_templates(tokenizer)\n",
    "            experiment._templates_precomputed = True\n",
    "        \n",
    "        # A. Perplexity\n",
    "        if perplexity_data:\n",
    "            print(f\"üìâ Calculating PPL...\")\n",
    "            ppl, avg_loss = experiment.calculate_perplexity(\n",
    "                model, tokenizer, perplexity_data, ut_steps,\n",
    "                max_length=eval_settings['ppl_max_length'],\n",
    "                stride=eval_settings['ppl_stride']\n",
    "            )\n",
    "            perplexity_results.append({'ut_steps': ut_steps, 'perplexity': ppl, 'avg_loss': avg_loss})\n",
    "            print(f\"‚úÖ PPL: {ppl:.4f} | Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            if use_wandb:\n",
    "                wandb.log({\"perplexity\": ppl, \"val_loss\": avg_loss, \"ut_steps\": ut_steps})\n",
    "        \n",
    "        # B. Accuracy & Time\n",
    "        enable_batch = config.get('OPTIMIZATION', {}).get('enable_batch', True)\n",
    "        \n",
    "        for task_type, items in test_datasets.items():\n",
    "            print(f\"\\nüìù Task: {task_type} ({len(items)} samples)\")\n",
    "            task_results = []\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Determine batch strategy\n",
    "            batch_size = 1\n",
    "            if enable_batch:\n",
    "                limits = {'n_ary': 8, 'p_hop': 4, 'igsm': 2}\n",
    "                batch_size = min(limits.get(task_type, 1), experiment.max_batch_size)\n",
    "            \n",
    "            # Process Loop\n",
    "            if batch_size > 1 and len(items) >= 2:\n",
    "                # Batched Processing\n",
    "                for i in range(0, len(items), batch_size):\n",
    "                    batch_items = items[i : i + batch_size]\n",
    "                    prompts = [item['prompt'] for item in batch_items]\n",
    "                    \n",
    "                    batch_out = experiment.batch_predict_with_metrics(\n",
    "                        prompts, task_type, model, tokenizer, ut_steps\n",
    "                    )\n",
    "                    \n",
    "                    for res, item in zip(batch_out, batch_items):\n",
    "                        res_entry = _create_result_entry(res, item, task_type, ut_steps)\n",
    "                        task_results.append(res_entry)\n",
    "                        all_results.append(res_entry)\n",
    "            else:\n",
    "                # Sequential Processing\n",
    "                for item in tqdm(items, desc=f\"  {task_type}\", leave=False):\n",
    "                    res = experiment.predict_with_metrics_optimized(\n",
    "                        item['prompt'], task_type, model, tokenizer, ut_steps\n",
    "                    )\n",
    "                    res_entry = _create_result_entry(res, item, task_type, ut_steps)\n",
    "                    task_results.append(res_entry)\n",
    "                    all_results.append(res_entry)\n",
    "            \n",
    "            # Logging\n",
    "            _log_task_summary(task_results, task_type, ut_steps, start_time, use_wandb)\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Final W&B Close\n",
    "    if use_wandb and run:\n",
    "        wandb.finish()\n",
    "        \n",
    "    return all_results, perplexity_results\n",
    "\n",
    "def _create_result_entry(result, item, task_type, ut_steps):\n",
    "    \"\"\"Helper to format result dictionary\"\"\"\n",
    "    pred = str(result['prediction']).strip().lower()\n",
    "    target = str(item['expected_answer']).strip().lower()\n",
    "    \n",
    "    is_correct = False\n",
    "    if task_type == 'p_hop':\n",
    "        is_correct = pred == target\n",
    "    else:\n",
    "        try:\n",
    "            is_correct = abs(float(pred) - float(target)) < 0.001\n",
    "        except:\n",
    "            is_correct = pred == target\n",
    "            \n",
    "    return {\n",
    "        'task_type': task_type,\n",
    "        'difficulty': item.get('difficulty', 'unknown'),\n",
    "        'test_input': item['prompt'],\n",
    "        'expected_answer': item['expected_answer'],\n",
    "        'is_correct': is_correct,\n",
    "        'test_id': generate_test_id(task_type, item.get('difficulty', ''), item['prompt']),\n",
    "        'ut_steps': ut_steps,\n",
    "        **result\n",
    "    }\n",
    "\n",
    "def _log_task_summary(results, task_type, ut_steps, start_time, use_wandb):\n",
    "    if not results: return\n",
    "    \n",
    "    acc = sum(r['is_correct'] for r in results) / len(results)\n",
    "    avg_time = sum(r['generation_time'] for r in results) / len(results)\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print(f\"    üìä Acc={acc:.2%} | Time/Sample={avg_time:.3f}s | Total={duration:.1f}s\")\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            f\"{task_type}/accuracy\": acc,\n",
    "            f\"{task_type}/avg_time\": avg_time,\n",
    "            \"ut_steps\": ut_steps\n",
    "        })\n",
    "\n",
    "print(\"‚úÖ Main experiment runner loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 8: Analysis Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 8: Analysis Utilities\n",
    "Purpose: Functions to analyze, clean, and merge results\n",
    "\"\"\"\n",
    "\n",
    "def analyze_experiment_results(accuracy_results: list, perplexity_results: list = None):\n",
    "    \"\"\"Generate summary statistics dataframe\"\"\"\n",
    "    if not accuracy_results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(accuracy_results)\n",
    "    \n",
    "    # Group by UT steps and task\n",
    "    summary = df.groupby(['ut_steps', 'task_type']).agg({\n",
    "        'is_correct': ['mean', 'count', 'std'],\n",
    "        'generation_time': ['mean', 'min', 'max'],\n",
    "        'generated_tokens': ['mean']\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten columns\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    return summary\n",
    "\n",
    "def load_and_process_results(file_path: str):\n",
    "    \"\"\"Load results CSV and add derived features\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Feature engineering\n",
    "    if 'generated_tokens' not in df.columns:\n",
    "        df['generated_tokens'] = df['full_response'].apply(\n",
    "            lambda x: len(re.findall(r'\\S+', str(x))) if pd.notna(x) else 0\n",
    "        )\n",
    "    \n",
    "    # Type conversion\n",
    "    df['is_correct'] = df['is_correct'].astype(bool)\n",
    "    df['ut_steps'] = pd.to_numeric(df['ut_steps'], errors='coerce').astype('Int64')\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Analysis utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 9: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 9: Configuration\n",
    "Purpose: Define experiment parameters\n",
    "\"\"\"\n",
    "\n",
    "BatchConfig = {\n",
    "    # --- Model Settings ---\n",
    "    'MODEL': {\n",
    "        'path': \"ByteDance/Ouro-1.4B-Thinking\",\n",
    "        'dtype': torch.float16,\n",
    "        'use_4bit_quant': False,      # Set True if low VRAM\n",
    "        'use_torch_compile': True     # Optimization\n",
    "    },\n",
    "\n",
    "    # --- Experiment Scope ---\n",
    "    'INFERENCE_STEPS': [1],           # List of loop counts to test (e.g., [1, 2, 4])\n",
    "\n",
    "    # --- Evaluation Logic ---\n",
    "    'EVAL_SETTINGS': {\n",
    "        'calculate_perplexity': True,\n",
    "        'early_exit_threshold': -1.0, # -1 disables early exit\n",
    "        'ppl_num_samples': 50,\n",
    "        'ppl_max_length': 2048,\n",
    "        'ppl_stride': 512,\n",
    "    },\n",
    "\n",
    "    # --- Logging ---\n",
    "    'WANDB': {\n",
    "        'enabled': True,\n",
    "        'project': \"ouro-looped-transformer\",\n",
    "        'run_name': f\"run_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        'entity': None,\n",
    "        'mode': 'offline',            # Use 'online' to sync\n",
    "    },\n",
    "\n",
    "    # --- Data Generation ---\n",
    "    'DATA': {\n",
    "        'load_existing': False,\n",
    "        'data_file_path': '',\n",
    "        'n_ary': {'ops_levels': [4, 8], 'num_samples_per_level': 10},\n",
    "        'p_hop': {'hop_levels': [2, 4], 'num_samples_per_level': 10},\n",
    "        'igsm': {'num_samples_total': 20}\n",
    "    },\n",
    "\n",
    "    # --- Performance Optimization ---\n",
    "    'OPTIMIZATION': {\n",
    "        'enable_batch': True,\n",
    "        'max_batch_size': 8,\n",
    "        'max_new_token': 1024\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 10: Execution & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 10: Execution & Visualization\n",
    "Purpose: Run the experiment, save results, and visualize outcomes\n",
    "\"\"\"\n",
    "\n",
    "# 1. Setup\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"üïí Timestamp: {timestamp}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nüöÄ STARTING EXPERIMENT\\n\" + \"=\"*50)\n",
    "\n",
    "# 2. Run\n",
    "results_acc, results_ppl = run_batch_experiment(BatchConfig)\n",
    "\n",
    "# 3. Save Results\n",
    "df_acc = pd.DataFrame(results_acc)\n",
    "df_ppl = pd.DataFrame(results_ppl)\n",
    "\n",
    "RUN_RESULTS_NAME = f\"run_{timestamp}\"\n",
    "os.makedirs(os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME))\n",
    "acc_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"ouro_acc_{timestamp}.csv\")\n",
    "ppl_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"ouro_ppl_{timestamp}.csv\")\n",
    "cfg_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"ouro_config_{timestamp}.yaml\")\n",
    "\n",
    "df_acc.to_csv(acc_path, index=False)\n",
    "if not df_ppl.empty:\n",
    "    df_ppl.to_csv(ppl_path, index=False)\n",
    "\n",
    "# Save Config\n",
    "def sanitize_config(cfg):\n",
    "    \"\"\"Convert config to YAML-safe format\"\"\"\n",
    "    clean = {}\n",
    "    for k, v in cfg.items():\n",
    "        if isinstance(v, dict):\n",
    "            clean[k] = sanitize_config(v)\n",
    "        elif str(type(v)).find('torch.') != -1:\n",
    "            clean[k] = str(v)\n",
    "        else:\n",
    "            clean[k] = v\n",
    "    return clean\n",
    "\n",
    "with open(cfg_path, 'w') as f:\n",
    "    yaml.dump(sanitize_config(BatchConfig), f)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to {OUTPUT_PATH}\")\n",
    "\n",
    "# 4. Visualization & Reporting\n",
    "if not df_acc.empty:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\nüìä VISUALIZATION\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Summary Tables\n",
    "    summary = analyze_experiment_results(results_acc)\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    print(summary)\n",
    "    \n",
    "    # Plotting\n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Plot 1: Accuracy\n",
    "        acc_summary = df_acc.groupby(['task_type', 'ut_steps'])['is_correct'].mean().reset_index()\n",
    "        sns.barplot(data=acc_summary, x='ut_steps', y='is_correct', hue='task_type', ax=axes[0])\n",
    "        axes[0].set_title('Accuracy by UT Steps')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "        \n",
    "        # Plot 2: Time\n",
    "        time_summary = df_acc.groupby(['task_type', 'ut_steps'])['generation_time'].mean().reset_index()\n",
    "        sns.barplot(data=time_summary, x='ut_steps', y='generation_time', hue='task_type', ax=axes[1])\n",
    "        axes[1].set_title('Inference Time (s) by UT Steps')\n",
    "        \n",
    "        # Plot 3: Token Count\n",
    "        sns.boxplot(data=df_acc, x='ut_steps', y='generated_tokens', hue='task_type', ax=axes[2])\n",
    "        axes[2].set_title('Generated Tokens Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Visualization error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to visualize.\")\n",
    "\n",
    "print(\"\\nüèÅ Experiment Complete.\\n\")\n",
    "print(\"Final Inspection:\\n\")\n",
    "print(\"Top 20 Accuracy Report:\\n\")\n",
    "print(df_acc.head(20))\n",
    "print(\"Perplexity Report:\\n\")\n",
    "print(df_ppl.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 11: Reasoning Primitives Generator (Holistic Eval)\n",
    "Purpose: Generate Depth-k Variable Assignment tasks (Math/Code) with 5-shot support\n",
    "\"\"\"\n",
    "\n",
    "def create_reasoning_primitives_data(config: dict) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generates 'Reasoning Primitives' datasets as described:\n",
    "    - Depth-0 Var Assignment: a=1, b=2, c=6. Query: b?\n",
    "    - Depth-1 Var Assignment: a=1, b=2, c=a. Query: c?\n",
    "    - Variants: Math ('Let a = 1') and Code ('a = 1')\n",
    "    \"\"\"\n",
    "    if 'reasoning_primitives' not in config:\n",
    "        return {}\n",
    "\n",
    "    primitives_data = {}\n",
    "    cfg = config['reasoning_primitives']\n",
    "    num_samples = cfg.get('num_samples', 50)\n",
    "    \n",
    "    # Configuration for variants\n",
    "    formats = {\n",
    "        'code': {'assign': \"{var} = {val}\", 'query': \"print({var})\", 'sep': \"\\n\"},\n",
    "        'math': {'assign': \"Let {var} = {val}.\", 'query': \"What is {var}?\", 'sep': \" \"}\n",
    "    }\n",
    "    \n",
    "    chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    \n",
    "    for depth in [0, 1]:\n",
    "        for variant in ['code', 'math']:\n",
    "            task_name = f\"var_assign_depth_{depth}_{variant}\"\n",
    "            samples = []\n",
    "            fmt = formats[variant]\n",
    "            \n",
    "            for _ in range(num_samples):\n",
    "                # 1. Setup variables (pool of 5-8 vars)\n",
    "                num_vars = random.randint(5, 8)\n",
    "                vars_subset = random.sample(chars, num_vars)\n",
    "                values = {v: random.randint(0, 99) for v in vars_subset}\n",
    "                \n",
    "                statements = []\n",
    "                # 2. Create Assignments\n",
    "                if depth == 0:\n",
    "                    # Depth 0: Direct assignment (a = 5)\n",
    "                    for v in vars_subset:\n",
    "                        stmt = fmt['assign'].format(var=v, val=values[v])\n",
    "                        statements.append(stmt)\n",
    "                    target_var = random.choice(vars_subset)\n",
    "                    expected = str(values[target_var])\n",
    "                    \n",
    "                elif depth == 1:\n",
    "                    # Depth 1: Indirection (a = 5, b = a)\n",
    "                    # First half: direct assignments\n",
    "                    roots = vars_subset[:num_vars//2]\n",
    "                    for v in roots:\n",
    "                        stmt = fmt['assign'].format(var=v, val=values[v])\n",
    "                        statements.append(stmt)\n",
    "                    \n",
    "                    # Second half: indirect assignments pointing to roots\n",
    "                    pointers = vars_subset[num_vars//2:]\n",
    "                    for v in pointers:\n",
    "                        src = random.choice(roots)\n",
    "                        stmt = fmt['assign'].format(var=v, val=src) # b = a\n",
    "                        statements.append(stmt)\n",
    "                        values[v] = values[src] # Propagate value\n",
    "                    \n",
    "                    # Target must be a pointer to test depth-1 reasoning\n",
    "                    target_var = random.choice(pointers)\n",
    "                    expected = str(values[target_var])\n",
    "\n",
    "                # 3. Shuffle statements (to ensure model reads context, not just order)\n",
    "                random.shuffle(statements)\n",
    "                \n",
    "                # 4. Construct Prompt\n",
    "                context = fmt['sep'].join(statements)\n",
    "                query = fmt['query'].format(var=target_var)\n",
    "                full_prompt = f\"{context}{fmt['sep']}{query}\"\n",
    "                \n",
    "                samples.append({\n",
    "                    \"prompt\": full_prompt,\n",
    "                    \"expected_answer\": expected,\n",
    "                    \"difficulty\": f\"depth_{depth}\",\n",
    "                    \"task_type\": task_name\n",
    "                })\n",
    "            \n",
    "            primitives_data[task_name] = samples\n",
    "\n",
    "    return primitives_data\n",
    "\n",
    "def format_5_shot_prompt(task_samples: List[Dict], current_sample: Dict) -> str:\n",
    "    \"\"\"Helper to prepend 5 random examples for few-shot evaluation\"\"\"\n",
    "    # Pick 5 random examples that are NOT the current one\n",
    "    pool = [s for s in task_samples if s['prompt'] != current_sample['prompt']]\n",
    "    if len(pool) < 5:\n",
    "        shots = pool\n",
    "    else:\n",
    "        shots = random.sample(pool, 5)\n",
    "        \n",
    "    demos = []\n",
    "    for shot in shots:\n",
    "        demos.append(f\"{shot['prompt']}\\nAnswer: {shot['expected_answer']}\")\n",
    "    \n",
    "    # Join demos + current prompt\n",
    "    few_shot_context = \"\\n\\n\".join(demos)\n",
    "    final_prompt = f\"{few_shot_context}\\n\\n{current_sample['prompt']}\\nAnswer:\"\n",
    "    return final_prompt\n",
    "\n",
    "print(\"‚úÖ Reasoning Primitives Generator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 12: Holistic Evaluation Runner (Standard Benchmarks + Primitives)\n",
    "Purpose: Unified entry point for running the 19 standard tasks + custom primitives\n",
    "\"\"\"\n",
    "\n",
    "def run_holistic_evaluation(model, tokenizer, config: dict):\n",
    "    \"\"\"\n",
    "    Runs the full evaluation suite:\n",
    "    1. Custom Reasoning Primitives (Depth-k Var Assign) - Running locally\n",
    "    2. Standard Benchmarks (QA/Math) - via lm-evaluation-harness\n",
    "    \"\"\"\n",
    "    holistic_results = []\n",
    "    \n",
    "    # --- PART 1: CUSTOM REASONING PRIMITIVES ---\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\nüß† Running Reasoning Primitives (5-shot)\\n\" + \"=\"*40)\n",
    "    \n",
    "    # Generate data\n",
    "    primitives = create_reasoning_primitives_data(config)\n",
    "    \n",
    "    for task_name, samples in primitives.items():\n",
    "        print(f\"  Task: {task_name} ({len(samples)} samples)\")\n",
    "        correct = 0\n",
    "        \n",
    "        for item in tqdm(samples, desc=task_name, leave=False):\n",
    "            # Apply 5-shot formatting\n",
    "            prompt = format_5_shot_prompt(samples, item)\n",
    "            \n",
    "            # Simple Generation (Greedy)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=10, \n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            # Extract output (naive splitting on 'Answer:')\n",
    "            full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # We look for the *last* \"Answer:\" because of few-shot context\n",
    "            if \"Answer:\" in full_text:\n",
    "                generated = full_text.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                generated = full_text.strip()\n",
    "            \n",
    "            # Check correctness\n",
    "            if generated == item['expected_answer']:\n",
    "                correct += 1\n",
    "            \n",
    "            holistic_results.append({\n",
    "                'task_category': 'Reasoning Primitive',\n",
    "                'task_name': task_name,\n",
    "                'prompt': prompt[-50:] + \"...\", # Log truncation\n",
    "                'prediction': generated,\n",
    "                'target': item['expected_answer'],\n",
    "                'is_correct': generated == item['expected_answer']\n",
    "            })\n",
    "            \n",
    "        acc = correct / len(samples)\n",
    "        print(f\"    ‚úÖ Accuracy: {acc:.2%}\")\n",
    "\n",
    "    # --- PART 2: STANDARD BENCHMARKS (lm-eval) ---\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\nüìö Running Standard Benchmarks (lm-eval)\\n\" + \"=\"*40)\n",
    "    \n",
    "    # Check if library is available\n",
    "    try:\n",
    "        import lm_eval\n",
    "        from lm_eval import evaluator\n",
    "        \n",
    "        # Map the 4 slices to lm-eval task names\n",
    "        # Note: Task names depend on specific lm-eval version. \n",
    "        # These are common identifiers.\n",
    "        standard_tasks = [\n",
    "            # Closed Book QA\n",
    "            \"triviaqa\", \"nq_open\", \"webqs\", \n",
    "            # Open Book QA\n",
    "            \"squadv2\", \"drop\", \"coqa\", \n",
    "            # Math Word Problems (5-shot usually handled by harness config)\n",
    "            \"gsm8k\", \"svamp\", \"asdiv\" \n",
    "        ]\n",
    "        \n",
    "        print(f\"Attempting to run tasks: {standard_tasks}\")\n",
    "        print(\"‚ö†Ô∏è Note: This may take a long time and download large datasets.\")\n",
    "        \n",
    "        # We wrap this in a try-block because running 19 benchmarks \n",
    "        # in a script can be heavy.\n",
    "        if config.get('ENABLE_HEAVY_BENCHMARKS', False):\n",
    "            results = evaluator.simple_evaluate(\n",
    "                model=\"hf\",\n",
    "                model_args=f\"pretrained={model.name_or_path},dtype=float16\",\n",
    "                tasks=standard_tasks,\n",
    "                num_fewshot=5, # Global 5-shot setting\n",
    "                batch_size=4\n",
    "            )\n",
    "            \n",
    "            # Log results\n",
    "            for task, res in results['results'].items():\n",
    "                acc = res.get('acc,none') or res.get('acc')\n",
    "                print(f\"  {task}: {acc:.2%}\")\n",
    "                holistic_results.append({\n",
    "                    'task_category': 'Standard Benchmark',\n",
    "                    'task_name': task,\n",
    "                    'is_correct': acc # Storing score directly\n",
    "                })\n",
    "        else:\n",
    "            print(\"‚è© Skipping heavy benchmarks (ENABLE_HEAVY_BENCHMARKS=False)\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è 'lm-evaluation-harness' not installed. Skipping Standard Benchmarks.\")\n",
    "        print(\"‚ÑπÔ∏è To run: pip install lm-evaluation-harness\")\n",
    "        \n",
    "    return holistic_results\n",
    "\n",
    "print(\"‚úÖ Holistic Evaluation Runner loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 13: Updated Configuration for Holistic Eval\n",
    "Purpose: Add settings for Reasoning Primitives and Benchmarks\n",
    "\"\"\"\n",
    "\n",
    "HolisticExperimentConfig = {\n",
    "    **BatchConfig,\n",
    "\n",
    "    # --- New Reasoning Primitives Config ---\n",
    "    'reasoning_primitives': {\n",
    "        'num_samples': 50  # 50 samples per primitive task\n",
    "    },\n",
    "    \n",
    "    # --- Standard Benchmarks Switch ---\n",
    "    # Set to True only if you have 'lm_evaluation_harness' installed and internet access\n",
    "    'ENABLE_HEAVY_BENCHMARKS': False, \n",
    "    \n",
    "    'WANDB': {'enabled': False}\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Holistic Configuration loaded\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
