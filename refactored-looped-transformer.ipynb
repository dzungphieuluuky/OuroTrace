{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 1: Environment Setup & Package Installation"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-15T16:58:29.751003Z",
     "iopub.status.busy": "2025-12-15T16:58:29.750766Z",
     "iopub.status.idle": "2025-12-15T17:00:16.393574Z",
     "shell.execute_reply": "2025-12-15T17:00:16.392746Z",
     "shell.execute_reply.started": "2025-12-15T16:58:29.750985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.3-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npeft 0.16.0 requires accelerate>=0.21.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 5.29.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npeft 0.16.0 requires accelerate>=0.21.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.3)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.3.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.5.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.3)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.12.4)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.5)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.15.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nCollecting accelerate\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.3)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.36.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\nDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\nInstalling collected packages: bitsandbytes, accelerate\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [accelerate]2\u001b[0m [accelerate]\n\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 bitsandbytes-0.49.0\n‚úÖ Packages installed successfully!\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 1: Environment Setup & Package Installation\n",
    "Purpose: Install required packages and suppress warnings\n",
    "\"\"\"\n",
    "\n",
    "# Install core packages\n",
    "!pip install --upgrade pip\n",
    "!pip uninstall -y transformers tokenizers accelerate -q\n",
    "!pip install \"transformers==4.56.0\" \"protobuf==5.29.3\" -q\n",
    "!pip install torch datasets -q\n",
    "!pip install pandas matplotlib seaborn tqdm wandb pyyaml\n",
    "!pip install bitsandbytes accelerate\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 2: Core Imports"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:00:16.394888Z",
     "iopub.status.busy": "2025-12-15T17:00:16.394640Z",
     "iopub.status.idle": "2025-12-15T17:00:36.772334Z",
     "shell.execute_reply": "2025-12-15T17:00:36.771348Z",
     "shell.execute_reply.started": "2025-12-15T17:00:16.394861Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Python Version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nPyTorch Version: 2.6.0+cu124\nCUDA Available: True\nCUDA Version: 12.4\nMon Dec 15 17:00:36 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 2: Core Imports\n",
    "Purpose: Import all necessary libraries\n",
    "\"\"\"\n",
    "\"Built-in libraries\"\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import glob\n",
    "import zipfile\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import yaml\n",
    "import logging\n",
    "import random\n",
    "\n",
    "\"Deep learning and NLP libraries\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    logging as hf_logging,\n",
    ")\n",
    "\n",
    "\"Data processing libraries\"\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Configure logging\n",
    "logging.getLogger(\"ContinuousBatchingLogger\").setLevel(logging.ERROR)\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 3: Environment Detection & Path Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:00:36.775027Z",
     "iopub.status.busy": "2025-12-15T17:00:36.774712Z",
     "iopub.status.idle": "2025-12-15T17:00:46.830370Z",
     "shell.execute_reply": "2025-12-15T17:00:46.829730Z",
     "shell.execute_reply.started": "2025-12-15T17:00:36.775003Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚úÖ Environment: Kaggle\nüìÇ Data Path: /kaggle/input/\nüì¶ Output Path: /kaggle/working/\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 3: Environment Detection & Path Configuration\n",
    "Purpose: Detect runtime environment (Colab/Kaggle/Local) and set paths\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def configure_environment_paths():\n",
    "    \"\"\"Detect environment and configure paths\"\"\"\n",
    "    try:\n",
    "        if \"google.colab\" in str(get_ipython()):\n",
    "            print(\"‚úÖ Environment: Google Colab\")\n",
    "            base_data_path = \"/content/\"\n",
    "            base_output_path = \"/content/output/\"\n",
    "            environment_name = \"colab\"\n",
    "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
    "            print(\"‚úÖ Environment: Kaggle\")\n",
    "            base_data_path = \"/kaggle/input/\"\n",
    "            base_output_path = \"/kaggle/working/\"\n",
    "            environment_name = \"kaggle\"\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n",
    "            base_data_path = \"./data/\"\n",
    "            base_output_path = \"./output/\"\n",
    "            environment_name = \"local\"\n",
    "    except NameError:\n",
    "        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n",
    "        base_data_path = \"./data/\"\n",
    "        base_output_path = \"./output/\"\n",
    "        environment_name = \"local\"\n",
    "\n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    print(f\"üìÇ Data Path: {base_data_path}\")\n",
    "    print(f\"üì¶ Output Path: {base_output_path}\")\n",
    "\n",
    "    return base_data_path, base_output_path, environment_name\n",
    "\n",
    "\n",
    "def auto_unzip_colab_content(target_dir=\"/content/\", zip_extension=\"*.zip\"):\n",
    "    \"\"\"Auto-extract zip files in Colab environment\"\"\"\n",
    "    if \"google.colab\" not in str(get_ipython()):\n",
    "        return\n",
    "\n",
    "    print(f\"üîé Scanning for {zip_extension} files...\")\n",
    "    zip_files = glob.glob(os.path.join(target_dir, zip_extension))\n",
    "\n",
    "    for zip_path in zip_files:\n",
    "        file_name = os.path.basename(zip_path)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        expected_output = os.path.join(target_dir, base_name)\n",
    "\n",
    "        if os.path.exists(expected_output) and os.listdir(expected_output):\n",
    "            print(f\"‚û°Ô∏è Skipping '{file_name}' (already extracted)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"üìÇ Extracting: {file_name}...\")\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(target_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "\n",
    "# Initialize paths\n",
    "DATA_PATH, OUTPUT_PATH, ENV = configure_environment_paths()\n",
    "auto_unzip_colab_content(DATA_PATH)\n",
    "\n",
    "# Optional: WandB login\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    wandb_key = userdata.get(\"WANDB_API_KEY\")\n",
    "    if wandb_key:\n",
    "        wandb.login(key=wandb_key)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 4: Data Generation Utilities"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:00:46.831418Z",
     "iopub.status.busy": "2025-12-15T17:00:46.831175Z",
     "iopub.status.idle": "2025-12-15T17:00:46.855713Z",
     "shell.execute_reply": "2025-12-15T17:00:46.855131Z",
     "shell.execute_reply.started": "2025-12-15T17:00:46.831400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚úÖ Data generation utilities loaded\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 4: Data Generation Utilities\n",
    "Purpose: Functions to create test datasets and perplexity data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_test_datasets(config: dict) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generate algorithmic test datasets strictly matching ICLR 2025 specs:\n",
    "    1. N-ary Addition: Input-Output pairs, 3-digit operands, sum.\n",
    "    2. P-hop Induction: Sequence length 256, Alphabet 4, Chain embedded at random sorted indices.\n",
    "    3. Symbolic i-GSM: Hierarchy depth 4, strict Level i -> Level i+1 dependency, Modulo 7.\n",
    "    \"\"\"\n",
    "    test_data = {}\n",
    "\n",
    "    # 1. N-ARY ADDITION (Unchanged, matches paper description)\n",
    "    if \"n_ary\" in config:\n",
    "        n_ary_data = []\n",
    "        ops_levels = config[\"n_ary\"].get(\"ops_levels\", [8, 16, 24, 32])\n",
    "        num_samples = config[\"n_ary\"].get(\"num_samples_per_level\", 30)\n",
    "\n",
    "        for n in ops_levels:\n",
    "            for _ in range(num_samples):\n",
    "                # Paper: sample operands [0, 999] uniformly\n",
    "                nums_int = [random.randint(0, 999) for _ in range(n)]\n",
    "                # Format: \"315 + 120 + ... =\"\n",
    "                nums_str = [str(x).zfill(3) for x in nums_int]\n",
    "\n",
    "                prompt_str = \" + \".join(nums_str) + \" =\"\n",
    "                target_str = str(sum(nums_int))\n",
    "\n",
    "                n_ary_data.append(\n",
    "                    {\n",
    "                        \"prompt\": prompt_str,\n",
    "                        \"expected_answer\": target_str,\n",
    "                        \"difficulty\": f\"{n}_ops\",\n",
    "                        \"task_type\": \"n_ary\",\n",
    "                    }\n",
    "                )\n",
    "        test_data[\"n_ary\"] = n_ary_data\n",
    "\n",
    "    # 2. P-HOP INDUCTION\n",
    "    # Description: \"Picking the sequence of p-hops randomly... shuffling them around in a sequence...\n",
    "    # sample remaining characters in place of filler tokens while respecting the p-hop order.\"\n",
    "    if \"p_hop\" in config:\n",
    "        p_hop_data = []\n",
    "        alphabet = [\"A\", \"B\", \"C\", \"D\"]\n",
    "        seq_len = 256\n",
    "        hop_levels = config[\"p_hop\"].get(\"hop_levels\", [16, 24, 32])\n",
    "        num_samples = config[\"p_hop\"].get(\"num_samples_per_level\", 30)\n",
    "\n",
    "        for p in hop_levels:\n",
    "            for _ in range(num_samples):\n",
    "                # 1. Define the Chain: v_0 -> v_1 -> ... -> v_p\n",
    "                # The 'hop' logic implies identifying v_i and finding v_{i+1}\n",
    "                chain = [random.choice(alphabet) for _ in range(p + 1)]\n",
    "\n",
    "                # 2. Embed in Sequence\n",
    "                # \"Shuffling them around... respecting p-hop order\" implies:\n",
    "                # We pick p+1 random positions in the 256-length sequence\n",
    "                # and place the chain items there in strictly increasing order.\n",
    "                indices = random.sample(range(seq_len), p + 1)\n",
    "                indices.sort()\n",
    "\n",
    "                # Initialize sequence with \"filler\" logic\n",
    "                # \"Sample remaining characters... in place of filler tokens\"\n",
    "                seq = [random.choice(alphabet) for _ in range(seq_len)]\n",
    "\n",
    "                # Overwrite fillers with the chain at the selected indices\n",
    "                for k, idx in enumerate(indices):\n",
    "                    seq[idx] = chain[k]\n",
    "\n",
    "                # 3. Construct Prompt\n",
    "                # Input: Sequence + Start Node. Goal: Output the p-th hop (last item in chain).\n",
    "                seq_str = \"\".join(seq)\n",
    "                start_node = chain[0]\n",
    "                expected = chain[-1]\n",
    "\n",
    "                full_prompt = (\n",
    "                    f\"Sequence: {seq_str}. Start: {start_node}. Hop {p} times.\"\n",
    "                )\n",
    "\n",
    "                p_hop_data.append(\n",
    "                    {\n",
    "                        \"prompt\": full_prompt,\n",
    "                        \"expected_answer\": expected,\n",
    "                        \"difficulty\": f\"{p}_hops\",\n",
    "                        \"task_type\": \"p_hop\",\n",
    "                    }\n",
    "                )\n",
    "        test_data[\"p_hop\"] = p_hop_data\n",
    "\n",
    "    # 3. SYMBOLIC i-GSM\n",
    "    # Description: \"Hierarchy of depth 4... edges connect level i to level i+1...\n",
    "    # instance parameter is integer... arithmetic modulo 7... symbolic language\"\n",
    "    if \"igsm\" in config:\n",
    "        igsm_data = []\n",
    "        num_total = config[\"igsm\"].get(\"num_samples_total\", 50)\n",
    "\n",
    "        # Variable naming pool (e.g., E#I, K#N)\n",
    "        chars = \"ABCDEFGHIJKLMNOP\"\n",
    "\n",
    "        def get_var_name():\n",
    "            return f\"{random.choice(chars)}#{random.choice(chars)}\"\n",
    "\n",
    "        for _ in range(num_total):\n",
    "            # 1. Structure: Hierarchy Depth 4 (Levels 0 to 4)\n",
    "            # We assign variables to specific levels\n",
    "            levels = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "            all_vars_data = {}  # {name: val}\n",
    "            equations = []\n",
    "\n",
    "            # 2. Level 0: Roots (Constants)\n",
    "            # Create ~4 root entities\n",
    "            for _ in range(4):\n",
    "                name = get_var_name()\n",
    "                val = random.randint(0, 6)  # Modulo 7\n",
    "                levels[0].append(name)\n",
    "                all_vars_data[name] = val\n",
    "                equations.append(f\"{name} := {val}\")\n",
    "\n",
    "            # 3. Levels 1 to 4: Dependencies\n",
    "            # \"Edges connect entities in level i to those in level i+1\"\n",
    "            for i in range(1, 5):\n",
    "                # Number of variables in this level\n",
    "                num_vars_in_level = random.randint(2, 4)\n",
    "\n",
    "                for _ in range(num_vars_in_level):\n",
    "                    target_var = get_var_name()\n",
    "\n",
    "                    # Ensure unique names\n",
    "                    while target_var in all_vars_data:\n",
    "                        target_var = get_var_name()\n",
    "\n",
    "                    # Select 1 or 2 operands from the PREVIOUS level (i-1)\n",
    "                    # This enforces the strict hierarchy described\n",
    "                    operands = random.choices(levels[i - 1], k=random.randint(1, 2))\n",
    "                    op_vals = [all_vars_data[op] for op in operands]\n",
    "\n",
    "                    op_type = random.choice([\"add\", \"sub\", \"mult\", \"assign\"])\n",
    "\n",
    "                    stmt = \"\"\n",
    "                    res = 0\n",
    "\n",
    "                    if op_type == \"assign\" or len(operands) < 2:\n",
    "                        stmt = f\"{target_var} := {operands[0]}\"\n",
    "                        res = op_vals[0]\n",
    "                    elif op_type == \"add\":\n",
    "                        stmt = f\"{target_var} := {operands[0]} + {operands[1]}\"\n",
    "                        res = (op_vals[0] + op_vals[1]) % 7\n",
    "                    elif op_type == \"sub\":\n",
    "                        stmt = f\"{target_var} := {operands[0]} - {operands[1]}\"\n",
    "                        res = (op_vals[0] - op_vals[1]) % 7\n",
    "                    elif op_type == \"mult\":\n",
    "                        stmt = f\"{target_var} := {operands[0]} * {operands[1]}\"\n",
    "                        res = (op_vals[0] * op_vals[1]) % 7\n",
    "\n",
    "                    equations.append(stmt)\n",
    "                    all_vars_data[target_var] = res\n",
    "                    levels[i].append(target_var)\n",
    "\n",
    "            # 4. Query\n",
    "            # \"Pick one of the nodes... compute value\"\n",
    "            # We pick from the deepest level (Level 4) to ensure the full chain is needed\n",
    "            target_var = random.choice(levels[4])\n",
    "            target_val = all_vars_data[target_var]\n",
    "\n",
    "            # Shuffle equations to ensure model learns the dependency graph, not just order\n",
    "            random.shuffle(equations)\n",
    "\n",
    "            # Format: \"Question. Eq1. Eq2. ... EqN. Target?\"\n",
    "            full_prompt = \"Question. \" + \". \".join(equations) + f\". {target_var}?\"\n",
    "\n",
    "            igsm_data.append(\n",
    "                {\n",
    "                    \"prompt\": full_prompt,\n",
    "                    \"expected_answer\": str(target_val),\n",
    "                    \"difficulty\": \"depth_4_hierarchical_mod_7\",\n",
    "                    \"task_type\": \"igsm\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    test_data[\"igsm\"] = igsm_data\n",
    "\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def create_perplexity_data(num_samples: int = 30) -> List[str]:\n",
    "    \"\"\"Generate reasoning traces for perplexity calculation\"\"\"\n",
    "    perplexity_texts = []\n",
    "\n",
    "    # N-ARY traces\n",
    "    for _ in range(num_samples // 2):\n",
    "        n = random.choice([4, 6, 8])\n",
    "        nums = [random.randint(10, 99) for _ in range(n)]\n",
    "        trace = f\"System: You are a calculation engine.\\nUser: Sum: {nums}\\nAssistant: Current Sum: 0\\n\"\n",
    "\n",
    "        current_sum = 0\n",
    "        for num in nums:\n",
    "            prev_sum = current_sum\n",
    "            current_sum += num\n",
    "            trace += f\"Add {num}: {prev_sum} + {num} = {current_sum}\\nCurrent Sum: {current_sum}\\n\"\n",
    "\n",
    "        trace += f\"Final: {current_sum}\"\n",
    "        perplexity_texts.append(trace)\n",
    "\n",
    "    # P-HOP traces\n",
    "    all_letters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "    for _ in range(num_samples // 2):\n",
    "        hops = random.choice([3, 4, 5])\n",
    "        nodes = random.sample(all_letters, hops + 1)\n",
    "        facts = [f\"{nodes[i]}->{nodes[i + 1]}\" for i in range(len(nodes) - 1)]\n",
    "        facts_str = \", \".join(facts)\n",
    "\n",
    "        trace = f\"System: Logic engine.\\nUser: Facts: {facts_str}. Start: {nodes[0]}. Find: {nodes[-1]}.\\n\"\n",
    "        trace += f\"Assistant: Current Node: {nodes[0]}\\n\"\n",
    "\n",
    "        for i in range(hops):\n",
    "            trace += f\"Rule Matches: {nodes[i]} -> {nodes[i + 1]}\\nNext Node: {nodes[i + 1]}\\n\"\n",
    "\n",
    "        trace += f\"Final: {nodes[-1]}\"\n",
    "        perplexity_texts.append(trace)\n",
    "\n",
    "    return perplexity_texts\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Load existing test data from JSON or CSV\"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "\n",
    "    if file_path.endswith(\".json\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(file_path)\n",
    "        raw_data = df.to_dict(\"records\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format. Use .json or .csv\")\n",
    "\n",
    "    processed_data = {\"n_ary\": [], \"p_hop\": [], \"igsm\": []}\n",
    "\n",
    "    for record in raw_data:\n",
    "        task = record.get(\"task_type\")\n",
    "        if task in processed_data:\n",
    "            if all(k in record for k in [\"prompt\", \"expected_answer\", \"difficulty\"]):\n",
    "                processed_data[task].append(record)\n",
    "\n",
    "    print(\n",
    "        f\"‚úÖ Loaded - N-ary: {len(processed_data['n_ary'])}, \"\n",
    "        f\"P-Hop: {len(processed_data['p_hop'])}, iGSM: {len(processed_data['igsm'])}\"\n",
    "    )\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def generate_test_id(task_type: str, difficulty: str, prompt: str) -> str:\n",
    "    \"\"\"Generate unique test ID\"\"\"\n",
    "    unique_str = f\"{task_type}_{difficulty}_{prompt}\"\n",
    "    return hashlib.md5(unique_str.encode()).hexdigest()[:8]\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data generation utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 5: Core Experiment Class"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:00:46.856707Z",
     "iopub.status.busy": "2025-12-15T17:00:46.856485Z",
     "iopub.status.idle": "2025-12-15T17:00:46.892667Z",
     "shell.execute_reply": "2025-12-15T17:00:46.891934Z",
     "shell.execute_reply.started": "2025-12-15T17:00:46.856691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚úÖ OuroThinkingExperiment class loaded\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 5: Core Experiment Class\n",
    "Purpose: Main class for running Ouro model experiments (Updated for ICLR 2025 Formats)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class OuroThinkingExperiment:\n",
    "    \"\"\"Core experiment class for Ouro model testing\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        dtype=torch.float16,\n",
    "        use_4bit_quant: bool = False,\n",
    "        use_torch_compile: bool = False,\n",
    "    ):\n",
    "        torch.cuda.empty_cache()\n",
    "        self.model_path = model_path\n",
    "        self.dtype = dtype\n",
    "        self.use_4bit_quant = use_4bit_quant\n",
    "        self.use_torch_compile = use_torch_compile\n",
    "        self.tokenizer = None\n",
    "        self.task_templates = {}\n",
    "\n",
    "    def load_model_with_ut_steps(\n",
    "        self, total_ut_steps: int, early_exit_threshold: float\n",
    "    ):\n",
    "        \"\"\"Load model with specific UT steps configuration\"\"\"\n",
    "        quantization_config = None\n",
    "        if self.use_4bit_quant:\n",
    "            print(\"‚Üí Applying 4-bit quantization\")\n",
    "            quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "        print(\n",
    "            f\"Loading model: UT steps={total_ut_steps}, Early exit={early_exit_threshold}\"\n",
    "        )\n",
    "\n",
    "        config = AutoConfig.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        config.total_ut_steps = total_ut_steps\n",
    "        config.early_exit_threshold = early_exit_threshold\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_path, trust_remote_code=True, padding_side=\"left\"\n",
    "        )\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            config=config,\n",
    "            device_map=\"cuda\",\n",
    "            attn_implementation=\"sdpa_paged\",\n",
    "            dtype=self.dtype if not self.use_4bit_quant else None,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "\n",
    "        if self.use_torch_compile:\n",
    "            print(\"‚Üí Applying torch.compile()\")\n",
    "            model = torch.compile(model)\n",
    "\n",
    "        model.eval()\n",
    "        print(f\"‚úÖ Model loaded on {model.device}\")\n",
    "\n",
    "        return (\n",
    "            model,\n",
    "            tokenizer,\n",
    "            None,\n",
    "            {\n",
    "                \"total_ut_steps\": total_ut_steps,\n",
    "                \"early_exit_threshold\": early_exit_threshold,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def _build_task_templates(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Pre-compute prompt templates for faster inference.\n",
    "        UPDATED: Refined Few-Shot examples to prevent babbling (added Step Prefixes and Guardrails).\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        task_configs = {\n",
    "            # 1. N-ARY ADDITION (T√çCH H·ª¢P STEP PREFIX V√Ä GUARDRAILS)\n",
    "            \"n_ary\": {\n",
    "                # Th√™m t·ª´ kh√≥a ki·ªÉm so√°t: MUST, DO NOT\n",
    "                \"system\": \"You are a mechanical calculation engine. Your output MUST be strictly sequential. DO NOT output introductions, explanations, or any text outside of the required calculation steps.\",\n",
    "                \"example_user\": \"10 + 20 + 30 =\",\n",
    "                # Th√™m [STEP X] v√† [FINAL]\n",
    "                \"example_asst\": \"[STEP 1] Current: 0\\n[STEP 2] Add 10: 0 + 10 = 10\\n[STEP 3] Current: 10\\n[STEP 4] Add 20: 10 + 20 = 30\\n[STEP 5] Current: 30\\n[STEP 6] Add 30: 30 + 30 = 60\\n[FINAL] 60\",\n",
    "                # B·∫Øt ƒë·∫ßu b·∫±ng ng·∫Øt d√≤ng v√† k√Ω hi·ªáu b∆∞·ªõc ƒë·∫ßu ti√™n\n",
    "                \"force_start\": \"\\n[STEP 1] Current: 0\",\n",
    "                \"input_prefix\": \"\",\n",
    "            },\n",
    "            # 2. P-HOP INDUCTION (R√∫t g·ªçn v√† Th√™m Guardrail)\n",
    "            \"p_hop\": {\n",
    "                # Th√™m t·ª´ kh√≥a ki·ªÉm so√°t v√† y√™u c·∫ßu k·∫øt th√∫c ch·ªâ v·ªõi token\n",
    "                \"system\": \"You are an induction head mechanism. Strictly trace the sequence occurrences step-by-step. Do not provide any commentary or auxiliary information. End your response ONLY with the final traced token.\",\n",
    "                \"example_user\": \"Sequence: A B C D A B. Start: A. Hop 1 times.\",\n",
    "                # R√∫t g·ªçn v√≠ d·ª•: d√πng [TRACE]\n",
    "                \"example_asst\": \"\\n[TRACE] Start at A. Found 'A' in sequence. Next token is B.\\n[FINAL] B\",\n",
    "                \"force_start\": \"\\n[TRACE] Start at\",\n",
    "                \"input_prefix\": \"\",\n",
    "            },\n",
    "            # 3. SYMBOLIC i-GSM (Th√™m Step Prefix v√† Guardrail)\n",
    "            \"igsm\": {\n",
    "                # TƒÉng c∆∞·ªùng Guardrail\n",
    "                \"system\": \"You are a symbolic math solver. You must solve the DAG modulo 7. Your reasoning MUST be concise, equation-based, and step-by-step. DO NOT generate preambles or verbose explanations.\",\n",
    "                \"example_user\": \"Question. E#I := 4. E#J := E#I. F#K := E#J. H#J := E#J + F#K. H#J?\",\n",
    "                # Th√™m [EQ X] cho t·ª´ng b∆∞·ªõc v√† [FINAL]\n",
    "                \"example_asst\": \"\\n[EQ 1] E#I = 4. [EQ 2] E#J = E#I. ==> E#J = 4. [EQ 3] F#K = E#J. ==> F#K = 4. [EQ 4] H#J = E#J + F#K. ==> H#J = 1.\\n[FINAL] 1\",\n",
    "                \"force_start\": \"\\n[EQ 1]\",\n",
    "                \"input_prefix\": \"\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        for task_type, config in task_configs.items():\n",
    "            # 1. Build static context (Unchanged logic)\n",
    "            static_messages = [\n",
    "                {\"role\": \"system\", \"content\": config[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": config[\"example_user\"]},\n",
    "                {\"role\": \"assistant\", \"content\": config[\"example_asst\"]},\n",
    "            ]\n",
    "\n",
    "            static_prompt_text = tokenizer.apply_chat_template(\n",
    "                static_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            static_inputs = tokenizer(static_prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "            # 2. Tokenize Force Start (Unchanged logic)\n",
    "            force_start_tokens = tokenizer(\n",
    "                config[\"force_start\"], return_tensors=\"pt\", add_special_tokens=False\n",
    "            )\n",
    "\n",
    "            self.task_templates[task_type] = {\n",
    "                \"static_input_ids\": static_inputs.input_ids,\n",
    "                \"static_attention_mask\": static_inputs.attention_mask,\n",
    "                \"force_start_ids\": force_start_tokens.input_ids,\n",
    "                \"input_prefix\": config[\"input_prefix\"],\n",
    "                \"force_start_text\": config[\"force_start\"],\n",
    "            }\n",
    "\n",
    "        print(\n",
    "            \"[+] Task templates pre-computed (Corrected with Step Prefixes and Guardrails)\"\n",
    "        )\n",
    "\n",
    "    def _extract_final_answer(self, full_response: str, task_type: str) -> str:\n",
    "        \"\"\"Extract answer from model response\"\"\"\n",
    "        pred = \"0\"\n",
    "\n",
    "        try:\n",
    "            if task_type == \"p_hop\":\n",
    "                patterns = [\n",
    "                    r\"Final\\s*:\\s*(\\w+)\",\n",
    "                    r\"Next token is\\s*(\\w+)\",\n",
    "                    r\"Answer\\s*:\\s*(\\w+)\",\n",
    "                ]\n",
    "                for pattern in patterns:\n",
    "                    match = re.search(pattern, full_response, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        pred = match.group(1).strip()\n",
    "                        break\n",
    "                else:\n",
    "                    pred = \"Error\"\n",
    "            else:\n",
    "                patterns = [\n",
    "                    r\"Final\\s*:\\s*([-+]?\\d*\\.?\\d+)\",\n",
    "                    r\"Answer\\s*:\\s*([-+]?\\d*\\.?\\d+)\",\n",
    "                    r\"=\\s*([-+]?\\d*\\.?\\d+)$\",  # Catches \"5 + 2 = 7\" at end\n",
    "                ]\n",
    "                all_matches = []\n",
    "                for pattern in patterns:\n",
    "                    matches = re.findall(pattern, full_response, re.IGNORECASE)\n",
    "                    all_matches.extend(matches)\n",
    "\n",
    "                if all_matches:\n",
    "                    pred = all_matches[-1]\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Parsing error: {e}\")\n",
    "            pred = \"ParseError\"\n",
    "\n",
    "        return pred\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict_with_metrics_optimized(\n",
    "        self,\n",
    "        user_input: str,\n",
    "        task_type: str,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        ut_steps: int,\n",
    "        generation_config: dict = None,\n",
    "    ):\n",
    "        \"\"\"Optimized prediction with repetition penalty to prevent loops\"\"\"\n",
    "        if not hasattr(self, \"task_templates\") or task_type not in self.task_templates:\n",
    "            self._build_task_templates(tokenizer)\n",
    "\n",
    "        template = self.task_templates[task_type]\n",
    "        device = model.device\n",
    "\n",
    "        # Construct Input\n",
    "        input_ids = template[\"static_input_ids\"].to(device)\n",
    "        user_query = template[\"input_prefix\"] + user_input\n",
    "        user_tokens = tokenizer(\n",
    "            user_query, return_tensors=\"pt\", add_special_tokens=False\n",
    "        ).input_ids.to(device)\n",
    "        force_start_ids = template[\"force_start_ids\"].to(device)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, user_tokens, force_start_ids], dim=1)\n",
    "        attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "        # Improved Generation Config\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        gen_config = generation_config or {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"do_sample\": False,\n",
    "            \"num_beams\": 1,\n",
    "            \"min_length\": 5,\n",
    "        }\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "            **gen_config,\n",
    "        )\n",
    "\n",
    "        generation_time = time.perf_counter() - start_time\n",
    "\n",
    "        # Decode\n",
    "        prompt_length = input_ids.shape[1]\n",
    "        generated_ids = outputs.sequences[0, prompt_length:]\n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        full_response = template[\"force_start_text\"] + generated_text\n",
    "        pred = self._extract_final_answer(full_response, task_type)\n",
    "\n",
    "        return {\n",
    "            \"full_response\": full_response,\n",
    "            \"prediction\": pred,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"generated_tokens\": generated_ids.shape[0],\n",
    "            \"input_tokens\": input_ids.shape[1],\n",
    "            \"ut_steps\": ut_steps,\n",
    "        }\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def calculate_perplexity(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        text_data: List[str],\n",
    "        ut_steps: int,\n",
    "        max_length: int = 2048,\n",
    "        stride: int = 512,\n",
    "    ):\n",
    "        \"\"\"Calculate perplexity using sliding window\"\"\"\n",
    "        device = model.device\n",
    "        model.eval()\n",
    "\n",
    "        if not text_data or not text_data[0]:\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "        text_concat = text_data[0]\n",
    "        encodings = tokenizer(\n",
    "            text_concat, return_tensors=\"pt\", max_length=max_length * 2, truncation=True\n",
    "        )\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        attention_mask = encodings.attention_mask.to(device)\n",
    "\n",
    "        if input_ids.size(1) < 2:\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for i in tqdm(\n",
    "            range(0, input_ids.size(1), stride), desc=f\"Calculating PPL (UT={ut_steps})\"\n",
    "        ):\n",
    "            end_loc = min(i + max_length, input_ids.size(1))\n",
    "            input_slice = input_ids[:, i:end_loc]\n",
    "            target_slice = input_slice.clone()\n",
    "\n",
    "            if i > 0:\n",
    "                context_len = input_slice.size(1) - stride\n",
    "                if context_len > 0:\n",
    "                    target_slice[:, :context_len] = -100\n",
    "\n",
    "            if (target_slice != -100).sum() == 0:\n",
    "                continue\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_slice,\n",
    "                attention_mask=attention_mask[:, i:end_loc],\n",
    "                labels=target_slice,\n",
    "            )\n",
    "\n",
    "            if torch.isnan(outputs.loss):\n",
    "                continue\n",
    "\n",
    "            num_valid = (target_slice != -100).sum().item()\n",
    "            total_loss += (outputs.loss * num_valid).item()\n",
    "            total_tokens += num_valid\n",
    "\n",
    "        if total_tokens == 0:\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "        return perplexity, avg_loss\n",
    "\n",
    "\n",
    "print(\"‚úÖ OuroThinkingExperiment class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 6: Batch Experiment Class"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:00:46.894045Z",
     "iopub.status.busy": "2025-12-15T17:00:46.893545Z",
     "iopub.status.idle": "2025-12-15T17:00:46.926448Z",
     "shell.execute_reply": "2025-12-15T17:00:46.925872Z",
     "shell.execute_reply.started": "2025-12-15T17:00:46.894026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚úÖ OuroBatchExperiment class loaded\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 6: Batch Experiment Class\n",
    "Purpose: Extended class with batch processing support\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class OuroBatchExperiment(OuroThinkingExperiment):\n",
    "    \"\"\"Extended experiment class with batch processing\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        dtype=torch.float16,\n",
    "        use_4bit_quant: bool = False,\n",
    "        use_torch_compile: bool = False,\n",
    "        max_batch_size: int = 4,\n",
    "        max_new_tokens: int = 1024,\n",
    "    ):\n",
    "        super().__init__(model_path, dtype, use_4bit_quant, use_torch_compile)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def prepare_batch_inputs(\n",
    "        self, prompts: List[str], task_type: str\n",
    "    ) -> List[List[int]]:\n",
    "        \"\"\"Prepare inputs for batch generation\"\"\"\n",
    "        if task_type not in self.task_templates:\n",
    "            raise ValueError(\"Templates not built. Call _build_task_templates first.\")\n",
    "\n",
    "        template = self.task_templates[task_type]\n",
    "\n",
    "        batch_texts = [template[\"input_prefix\"] + p for p in prompts]\n",
    "        user_encodings = self.tokenizer(batch_texts, add_special_tokens=False)\n",
    "\n",
    "        static_ids = template[\"static_input_ids\"].squeeze(0).tolist()\n",
    "        force_ids = template[\"force_start_ids\"].squeeze(0).tolist()\n",
    "\n",
    "        input_id_lists = []\n",
    "        for user_ids in user_encodings[\"input_ids\"]:\n",
    "            full_seq = static_ids + user_ids + force_ids\n",
    "            input_id_lists.append(full_seq)\n",
    "\n",
    "        return input_id_lists\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def batch_predict_with_metrics(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        task_type: str,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        ut_steps: int,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "    ):\n",
    "        \"\"\"Batch prediction with metrics\"\"\"\n",
    "        if not prompts:\n",
    "            return []\n",
    "\n",
    "        simple_batch_inputs = self.prepare_batch_inputs(prompts, task_type)\n",
    "        input_lengths = [len(ids) for ids in simple_batch_inputs]\n",
    "\n",
    "        if not hasattr(model, \"generate_batch\"):\n",
    "            print(\"‚ö†Ô∏è Model doesn't support generate_batch(). Using sequential.\")\n",
    "            return self._sequential_fallback(\n",
    "                prompts, task_type, model, tokenizer, ut_steps, generation_config\n",
    "            )\n",
    "\n",
    "        if generation_config is None:\n",
    "            generation_config = GenerationConfig(\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                use_cuda_graph=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=False,\n",
    "                max_batch_tokens=self.max_batch_size * self.max_new_tokens,\n",
    "            )\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        try:\n",
    "            batch_outputs = model.generate_batch(\n",
    "                inputs=simple_batch_inputs,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è generate_batch failed: {e}. Falling back.\")\n",
    "            return self._sequential_fallback(\n",
    "                prompts, task_type, model, tokenizer, ut_steps, generation_config\n",
    "            )\n",
    "\n",
    "        batch_time = time.perf_counter() - start_time\n",
    "\n",
    "        template = self.task_templates[task_type]\n",
    "        results = [None] * len(prompts)\n",
    "        request_ids = list(batch_outputs.keys())\n",
    "\n",
    "        # Map outputs to prompts\n",
    "        if all(isinstance(rid, int) for rid in request_ids):\n",
    "            for request_id in request_ids:\n",
    "                if 0 <= request_id < len(prompts):\n",
    "                    output = batch_outputs[request_id]\n",
    "                    results[request_id] = self._process_single_output(\n",
    "                        output,\n",
    "                        request_id,\n",
    "                        input_lengths[request_id],\n",
    "                        template,\n",
    "                        tokenizer,\n",
    "                        task_type,\n",
    "                        ut_steps,\n",
    "                        batch_time / len(prompts),\n",
    "                    )\n",
    "        else:\n",
    "            # String/UUID request IDs\n",
    "            input_to_index = {\n",
    "                \" \".join(map(str, inp)): idx\n",
    "                for idx, inp in enumerate(simple_batch_inputs)\n",
    "            }\n",
    "\n",
    "            for request_id in request_ids:\n",
    "                output = batch_outputs[request_id]\n",
    "\n",
    "                if hasattr(output, \"prompt_ids\"):\n",
    "                    input_key = \" \".join(map(str, output.prompt_ids))\n",
    "                    if input_key in input_to_index:\n",
    "                        idx = input_to_index[input_key]\n",
    "                        results[idx] = self._process_single_output(\n",
    "                            output,\n",
    "                            idx,\n",
    "                            len(output.prompt_ids),\n",
    "                            template,\n",
    "                            tokenizer,\n",
    "                            task_type,\n",
    "                            ut_steps,\n",
    "                            batch_time / len(prompts),\n",
    "                        )\n",
    "\n",
    "        # Fill missing results\n",
    "        for i in range(len(prompts)):\n",
    "            if results[i] is None:\n",
    "                results[i] = {\n",
    "                    \"full_response\": \"ERROR: No output\",\n",
    "                    \"prediction\": \"ERROR\",\n",
    "                    \"generation_time\": batch_time / len(prompts),\n",
    "                    \"generated_tokens\": 0,\n",
    "                    \"input_tokens\": input_lengths[i],\n",
    "                    \"ut_steps\": ut_steps,\n",
    "                }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _process_single_output(\n",
    "        self,\n",
    "        output,\n",
    "        prompt_idx: int,\n",
    "        input_length: int,\n",
    "        template: dict,\n",
    "        tokenizer,\n",
    "        task_type: str,\n",
    "        ut_steps: int,\n",
    "        sample_time: float,\n",
    "    ):\n",
    "        \"\"\"Process single batch output\"\"\"\n",
    "        if hasattr(output, \"generated_tokens\"):\n",
    "            generated_ids = output.generated_tokens\n",
    "        elif hasattr(output, \"sequences\") and len(output.sequences) > 0:\n",
    "            generated_ids = output.sequences[0]\n",
    "        else:\n",
    "            generated_ids = []\n",
    "\n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        full_response = template[\"force_start_text\"] + generated_text\n",
    "\n",
    "        pred = self._extract_final_answer(full_response, task_type)\n",
    "\n",
    "        return {\n",
    "            \"full_response\": full_response,\n",
    "            \"prediction\": pred,\n",
    "            \"generation_time\": sample_time,\n",
    "            \"generated_tokens\": len(generated_ids),\n",
    "            \"input_tokens\": input_length,\n",
    "            \"ut_steps\": ut_steps,\n",
    "            \"prompt_idx\": prompt_idx,\n",
    "        }\n",
    "\n",
    "    def _sequential_fallback(\n",
    "        self, prompts, task_type, model, tokenizer, ut_steps, generation_config\n",
    "    ):\n",
    "        \"\"\"Fallback to sequential processing\"\"\"\n",
    "        results = []\n",
    "        for prompt in tqdm(prompts, desc=f\"Sequential fallback ({task_type})\"):\n",
    "            result = self.predict_with_metrics_optimized(\n",
    "                user_input=prompt,\n",
    "                task_type=task_type,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                ut_steps=ut_steps,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ OuroBatchExperiment class loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 7: Main Experiment Runner"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:00:46.927501Z",
     "iopub.status.busy": "2025-12-15T17:00:46.927243Z",
     "iopub.status.idle": "2025-12-15T17:00:46.946911Z",
     "shell.execute_reply": "2025-12-15T17:00:46.946256Z",
     "shell.execute_reply.started": "2025-12-15T17:00:46.927484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚úÖ Main experiment runner loaded\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 7: Main Experiment Runner\n",
    "Purpose: Function to execute the full experiment pipeline with batching and logging\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_batch_experiment(config: dict):\n",
    "    \"\"\"\n",
    "    Run experiment with batching support and W&B logging.\n",
    "    Returns: (accuracy_results, perplexity_results)\n",
    "    \"\"\"\n",
    "    # 1. Initialize W&B\n",
    "    use_wandb = config.get(\"WANDB\", {}).get(\"enabled\", False)\n",
    "    run = None\n",
    "\n",
    "    if use_wandb:\n",
    "        wb_conf = config[\"WANDB\"]\n",
    "        print(f\"üîó Initializing W&B (timeout: {wb_conf.get('timeout', 30)}s)...\")\n",
    "\n",
    "        os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "        try:\n",
    "            run = wandb.init(\n",
    "                project=wb_conf.get(\"project\", \"ouro-looped-transformer\"),\n",
    "                entity=wb_conf.get(\"entity\", None),\n",
    "                name=wb_conf.get(\"run_name\", f\"run_{int(time.time())}\"),\n",
    "                config=config,\n",
    "                mode=wb_conf.get(\"mode\", \"online\"),\n",
    "                settings=wandb.Settings(\n",
    "                    start_timeout=wb_conf.get(\"timeout\", 30), _disable_stats=True\n",
    "                ),\n",
    "            )\n",
    "            print(\"‚úÖ W&B initialized\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è W&B failed: {e}. Continuing offline.\")\n",
    "            use_wandb = False\n",
    "            run = None\n",
    "\n",
    "    # 2. Extract Config\n",
    "    model_path = config[\"MODEL\"][\"path\"]\n",
    "    ut_steps_list = config[\"INFERENCE_STEPS\"]\n",
    "    data_config = config[\"DATA\"]\n",
    "    eval_settings = config[\"EVAL_SETTINGS\"]\n",
    "\n",
    "    # 3. Setup Experiment\n",
    "    experiment = OuroBatchExperiment(\n",
    "        model_path,\n",
    "        dtype=config[\"MODEL\"][\"dtype\"],\n",
    "        use_4bit_quant=config[\"MODEL\"].get(\"use_4bit_quant\", True),\n",
    "        use_torch_compile=config[\"MODEL\"].get(\"use_torch_compile\", True),\n",
    "        max_batch_size=config.get(\"OPTIMIZATION\", {}).get(\"max_batch_size\", 4),\n",
    "        max_new_tokens=config.get(\"OPTIMIZATION\", {}).get(\"max_new_tokens\", 1024),\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 4. Prepare Data\n",
    "    if data_config[\"load_existing\"]:\n",
    "        test_datasets = load_and_preprocess_data(data_config[\"data_file_path\"])\n",
    "    else:\n",
    "        print(\"Generating new test datasets...\")\n",
    "        test_datasets = create_test_datasets(data_config)\n",
    "\n",
    "    perplexity_results = []\n",
    "    perplexity_data = []\n",
    "    if eval_settings[\"calculate_perplexity\"]:\n",
    "        raw_ppl_data = create_perplexity_data(eval_settings[\"ppl_num_samples\"])\n",
    "        perplexity_data = [\"\\n\\n\".join(raw_ppl_data)]\n",
    "\n",
    "    stats_results = []\n",
    "\n",
    "    # 5. Main Loop\n",
    "    for ut_steps in ut_steps_list:\n",
    "        print(f\"\\n{'=' * 60}\\nüß™ EXPERIMENT: UT Steps = {ut_steps}\\n{'=' * 60}\")\n",
    "\n",
    "        # Load fresh model\n",
    "        model, tokenizer, _, _ = experiment.load_model_with_ut_steps(\n",
    "            ut_steps, eval_settings[\"early_exit_threshold\"]\n",
    "        )\n",
    "\n",
    "        if not hasattr(experiment, \"_templates_precomputed\"):\n",
    "            experiment._build_task_templates(tokenizer)\n",
    "            experiment._templates_precomputed = True\n",
    "\n",
    "        # A. Perplexity\n",
    "        if perplexity_data:\n",
    "            print(f\"üìâ Calculating PPL...\")\n",
    "            ppl, avg_loss = experiment.calculate_perplexity(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                perplexity_data,\n",
    "                ut_steps,\n",
    "                max_length=eval_settings[\"ppl_max_length\"],\n",
    "                stride=eval_settings[\"ppl_stride\"],\n",
    "            )\n",
    "            perplexity_results.append(\n",
    "                {\"ut_steps\": ut_steps, \"perplexity\": ppl, \"avg_loss\": avg_loss}\n",
    "            )\n",
    "            print(f\"‚úÖ PPL: {ppl:.4f} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\"perplexity\": ppl, \"val_loss\": avg_loss, \"ut_steps\": ut_steps}\n",
    "                )\n",
    "\n",
    "        # B. Accuracy & Time\n",
    "        enable_batch = config.get(\"OPTIMIZATION\", {}).get(\"enable_batch\", True)\n",
    "\n",
    "        for task_type, items in test_datasets.items():\n",
    "            print(f\"\\nüìù Task: {task_type} ({len(items)} samples)\")\n",
    "            task_results = []\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Determine batch strategy\n",
    "            batch_size = 1\n",
    "            if enable_batch:\n",
    "                limits = {\"n_ary\": 8, \"p_hop\": 4, \"igsm\": 2}\n",
    "                batch_size = min(limits.get(task_type, 1), experiment.max_batch_size)\n",
    "\n",
    "            # Process Loop\n",
    "            if batch_size > 1 and len(items) >= 2:\n",
    "                # Batched Processing\n",
    "                for i in range(0, len(items), batch_size):\n",
    "                    batch_items = items[i : i + batch_size]\n",
    "                    prompts = [item[\"prompt\"] for item in batch_items]\n",
    "\n",
    "                    batch_out = experiment.batch_predict_with_metrics(\n",
    "                        prompts, task_type, model, tokenizer, ut_steps\n",
    "                    )\n",
    "\n",
    "                    for res, item in zip(batch_out, batch_items):\n",
    "                        res_entry = _create_result_entry(res, item, task_type, ut_steps)\n",
    "                        task_results.append(res_entry)\n",
    "                        stats_results.append(res_entry)\n",
    "            else:\n",
    "                # Sequential Processing\n",
    "                for item in tqdm(items, desc=f\"  {task_type}\", leave=False):\n",
    "                    res = experiment.predict_with_metrics_optimized(\n",
    "                        item[\"prompt\"], task_type, model, tokenizer, ut_steps\n",
    "                    )\n",
    "                    res_entry = _create_result_entry(res, item, task_type, ut_steps)\n",
    "                    task_results.append(res_entry)\n",
    "                    stats_results.append(res_entry)\n",
    "\n",
    "            # Logging\n",
    "            _log_task_summary(task_results, task_type, ut_steps, start_time, use_wandb)\n",
    "\n",
    "        # Cleanup\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Final W&B Close\n",
    "    if use_wandb and run:\n",
    "        wandb.finish()\n",
    "\n",
    "    return stats_results, perplexity_results\n",
    "\n",
    "\n",
    "def _create_result_entry(result, item, task_type, ut_steps):\n",
    "    \"\"\"Helper to format result dictionary\"\"\"\n",
    "    pred = str(result[\"prediction\"]).strip().lower()\n",
    "    target = str(item[\"expected_answer\"]).strip().lower()\n",
    "\n",
    "    is_correct = False\n",
    "    if task_type == \"p_hop\":\n",
    "        is_correct = pred == target\n",
    "    else:\n",
    "        try:\n",
    "            is_correct = abs(float(pred) - float(target)) < 0.001\n",
    "        except:\n",
    "            is_correct = pred == target\n",
    "\n",
    "    return {\n",
    "        \"task_type\": task_type,\n",
    "        \"difficulty\": item.get(\"difficulty\", \"unknown\"),\n",
    "        \"test_input\": item[\"prompt\"],\n",
    "        \"expected_answer\": item[\"expected_answer\"],\n",
    "        \"is_correct\": is_correct,\n",
    "        \"test_id\": generate_test_id(\n",
    "            task_type, item.get(\"difficulty\", \"\"), item[\"prompt\"]\n",
    "        ),\n",
    "        \"ut_steps\": ut_steps,\n",
    "        **result,\n",
    "    }\n",
    "\n",
    "\n",
    "def _log_task_summary(results, task_type, ut_steps, start_time, use_wandb):\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    acc = sum(r[\"is_correct\"] for r in results) / len(results)\n",
    "    avg_time = sum(r[\"generation_time\"] for r in results) / len(results)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(f\"    üìä Acc={acc:.2%} | Time/Sample={avg_time:.3f}s | Total={duration:.1f}s\")\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.log(\n",
    "            {\n",
    "                f\"{task_type}/accuracy\": acc,\n",
    "                f\"{task_type}/avg_time\": avg_time,\n",
    "                \"ut_steps\": ut_steps,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Main experiment runner loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 8: Analysis Utilities"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:00:46.948055Z",
     "iopub.status.busy": "2025-12-15T17:00:46.947723Z",
     "iopub.status.idle": "2025-12-15T17:00:46.971758Z",
     "shell.execute_reply": "2025-12-15T17:00:46.971215Z",
     "shell.execute_reply.started": "2025-12-15T17:00:46.948021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚úÖ Analysis utilities loaded\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 8: Analysis Utilities\n",
    "Purpose: Functions to analyze, clean, and merge results\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def analyze_experiment_results(accuracy_results: list, perplexity_results: list = None):\n",
    "    \"\"\"Generate summary statistics dataframe\"\"\"\n",
    "    if not accuracy_results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(accuracy_results)\n",
    "\n",
    "    # Group by UT steps and task\n",
    "    summary = (\n",
    "        df.groupby([\"ut_steps\", \"task_type\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"is_correct\": [\"mean\", \"count\", \"std\"],\n",
    "                \"generation_time\": [\"mean\", \"min\", \"max\"],\n",
    "                \"generated_tokens\": [\"mean\"],\n",
    "            }\n",
    "        )\n",
    "        .round(3)\n",
    "    )\n",
    "\n",
    "    # Flatten columns\n",
    "    summary.columns = [\"_\".join(col).strip() for col in summary.columns.values]\n",
    "    return summary\n",
    "\n",
    "\n",
    "def load_and_process_results(file_path: str):\n",
    "    \"\"\"Load results CSV and add derived features\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Feature engineering\n",
    "    if \"generated_tokens\" not in df.columns:\n",
    "        df[\"generated_tokens\"] = df[\"full_response\"].apply(\n",
    "            lambda x: len(re.findall(r\"\\S+\", str(x))) if pd.notna(x) else 0\n",
    "        )\n",
    "\n",
    "    # Type conversion\n",
    "    df[\"is_correct\"] = df[\"is_correct\"].astype(bool)\n",
    "    df[\"ut_steps\"] = pd.to_numeric(df[\"ut_steps\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Analysis utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 9: Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:00:46.974191Z",
     "iopub.status.busy": "2025-12-15T17:00:46.973936Z",
     "iopub.status.idle": "2025-12-15T17:00:46.998394Z",
     "shell.execute_reply": "2025-12-15T17:00:46.997663Z",
     "shell.execute_reply.started": "2025-12-15T17:00:46.974173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚úÖ Configuration loaded\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 9: Configuration\n",
    "Purpose: Define experiment parameters\n",
    "\"\"\"\n",
    "\n",
    "BatchConfig = {\n",
    "    # --- Model Settings ---\n",
    "    \"MODEL\": {\n",
    "        \"path\": \"ByteDance/Ouro-1.4B-Thinking\",\n",
    "        \"dtype\": torch.bfloat16,\n",
    "        \"use_4bit_quant\": False,\n",
    "        \"use_torch_compile\": True,\n",
    "    },\n",
    "    \"INFERENCE_STEPS\": [2],\n",
    "    \"EVAL_SETTINGS\": {\n",
    "        \"calculate_perplexity\": True,\n",
    "        \"early_exit_threshold\": 1.0,\n",
    "        \"ppl_num_samples\": 50,\n",
    "        \"ppl_max_length\": 2048,\n",
    "        \"ppl_stride\": 512,\n",
    "    },\n",
    "    \"WANDB\": {\n",
    "        \"enabled\": True,\n",
    "        \"project\": \"ouro-1.4b-thinking\",\n",
    "        \"run_name\": \"auto_timestamp\",\n",
    "        \"entity\": None,\n",
    "        \"mode\": \"offline\",\n",
    "    },\n",
    "    \"DATA\": {\n",
    "        \"load_existing\": False,\n",
    "        \"data_file_path\": \"\",\n",
    "        \"n_ary\": {\"ops_levels\": [4, 8, 16], \"num_samples_per_level\": 4},\n",
    "        \"p_hop\": {\"hop_levels\": [2, 4, 8], \"num_samples_per_level\": 4},\n",
    "        \"igsm\": {\"num_samples_total\": 4},\n",
    "    },\n",
    "    \"OPTIMIZATION\": {\"enable_batch\": True, \"max_batch_size\": 4, \"max_new_tokens\": 512},\n",
    "    \"reasoning_primitives\": {\"num_samples\": 8},\n",
    "    \"ENABLE_HEAVY_BENCHMARKS\": False,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CELL 10: Execution & Visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-15T17:16:31.366Z",
     "iopub.execute_input": "2025-12-15T17:00:46.999446Z",
     "iopub.status.busy": "2025-12-15T17:00:46.999214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "üïí Timestamp: 20251215_170047\n\n==================================================\nüöÄ STARTING EXPERIMENT\n==================================================\nüîó Initializing W&B (timeout: 30s)...\n‚ö†Ô∏è W&B failed: 1 validation error for Settings\nstart_timeout\n  Extra inputs are not permitted [type=extra_forbidden, input_value=30, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden. Continuing offline.\nGenerating new test datasets...\n\n============================================================\nüß™ EXPERIMENT: UT Steps = 2\n============================================================\nLoading model: UT steps=2, Early exit=1.0\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a775fe74470d428aa77983af41c89806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "config.json: 0.00B [00:00, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3df450b7b1540daa1b5407eba179112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "configuration_ouro.py: 0.00B [00:00, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e16fa2e5454408a827fbd4f69a2782e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0261bdcc57904d39b135574e8edbb096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "vocab.json: 0.00B [00:00, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21572c06c8e34ba1b03b54dff1278c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "merges.txt: 0.00B [00:00, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd8ddac4f0742caa45d02e836edc63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0484df301bf846dcaa29f07aa51ff412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/965 [00:00<?, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619df33c6c40462d8bce36a2a45041ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "modeling_ouro.py: 0.00B [00:00, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2025-12-15 17:00:54.255378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765818054.640802      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765818054.744944      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c982b7f46034535946d0c21da9e7d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "model.safetensors:   0%|          | 0.00/2.87G [00:00<?, ?B/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚Üí Applying torch.compile()\n‚úÖ Model loaded on cuda:0\n[+] Task templates pre-computed (Corrected with Step Prefixes and Guardrails)\nüìâ Calculating PPL...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60dd285e40f4fc3833a32973ba6104c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Calculating PPL (UT=2):   0%|          | 0/8 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "W1215 17:02:13.673000 47 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "‚úÖ PPL: 1.7750 | Loss: 0.5738\n\nüìù Task: n_ary (12 samples)\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.20request/s]\nSolving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:26<00:00, 21.57s/request]\nSolving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [03:44<00:00, 56.09s/request]\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "    üìä Acc=0.00% | Time/Sample=26.210s | Total=314.5s\n\nüìù Task: p_hop (12 samples)\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [04:25<00:00, 66.35s/request] \nSolving 4 requests:   0%|          | 0/4 [01:53<?, ?request/s]\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 10: Execution & Visualization\n",
    "Purpose: Run the experiment, save results, and visualize outcomes\n",
    "\"\"\"\n",
    "\n",
    "# 1. Setup\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"üïí Timestamp: {timestamp}\")\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\nüöÄ STARTING EXPERIMENT\\n\" + \"=\" * 50)\n",
    "\n",
    "# 2. Run\n",
    "results_acc, results_ppl = run_batch_experiment(BatchConfig)\n",
    "\n",
    "# 3. Save Results\n",
    "df_acc = pd.DataFrame(results_acc)\n",
    "df_ppl = pd.DataFrame(results_ppl)\n",
    "\n",
    "RUN_RESULTS_NAME = f\"run_{timestamp}\"\n",
    "os.makedirs(os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME))\n",
    "acc_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"ouro_acc_{timestamp}.csv\")\n",
    "ppl_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"ouro_ppl_{timestamp}.csv\")\n",
    "cfg_path = os.path.join(OUTPUT_PATH, RUN_RESULTS_NAME, f\"ouro_config_{timestamp}.yaml\")\n",
    "\n",
    "df_acc.to_csv(acc_path, index=False)\n",
    "if not df_ppl.empty:\n",
    "    df_ppl.to_csv(ppl_path, index=False)\n",
    "\n",
    "\n",
    "# Save Config\n",
    "def sanitize_config(cfg):\n",
    "    \"\"\"Convert config to YAML-safe format\"\"\"\n",
    "    clean = {}\n",
    "    for k, v in cfg.items():\n",
    "        if isinstance(v, dict):\n",
    "            clean[k] = sanitize_config(v)\n",
    "        elif str(type(v)).find(\"torch.\") != -1:\n",
    "            clean[k] = str(v)\n",
    "        else:\n",
    "            clean[k] = v\n",
    "    return clean\n",
    "\n",
    "\n",
    "with open(cfg_path, \"w\") as f:\n",
    "    yaml.dump(sanitize_config(BatchConfig), f)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to {OUTPUT_PATH}\")\n",
    "\n",
    "# 4. Visualization & Reporting\n",
    "if not df_acc.empty:\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\nüìä VISUALIZATION\\n\" + \"=\" * 50)\n",
    "\n",
    "    # Summary Tables\n",
    "    summary = analyze_experiment_results(results_acc)\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    print(summary)\n",
    "\n",
    "    # Plotting\n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        # Plot 1: Accuracy\n",
    "        acc_summary = (\n",
    "            df_acc.groupby([\"task_type\", \"ut_steps\"])[\"is_correct\"].mean().reset_index()\n",
    "        )\n",
    "        sns.barplot(\n",
    "            data=acc_summary, x=\"ut_steps\", y=\"is_correct\", hue=\"task_type\", ax=axes[0]\n",
    "        )\n",
    "        axes[0].set_title(\"Accuracy by UT Steps\")\n",
    "        axes[0].set_ylabel(\"Accuracy\")\n",
    "        axes[0].yaxis.set_major_formatter(\n",
    "            plt.FuncFormatter(lambda y, _: \"{:.0%}\".format(y))\n",
    "        )\n",
    "\n",
    "        # Plot 2: Time\n",
    "        time_summary = (\n",
    "            df_acc.groupby([\"task_type\", \"ut_steps\"])[\"generation_time\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        sns.barplot(\n",
    "            data=time_summary,\n",
    "            x=\"ut_steps\",\n",
    "            y=\"generation_time\",\n",
    "            hue=\"task_type\",\n",
    "            ax=axes[1],\n",
    "        )\n",
    "        axes[1].set_title(\"Inference Time (s) by UT Steps\")\n",
    "\n",
    "        # Plot 3: Token Count\n",
    "        sns.boxplot(\n",
    "            data=df_acc, x=\"ut_steps\", y=\"generated_tokens\", hue=\"task_type\", ax=axes[2]\n",
    "        )\n",
    "        axes[2].set_title(\"Generated Tokens Distribution\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Visualization error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to visualize.\")\n",
    "\n",
    "print(\"\\nüèÅ Experiment Complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-15T17:16:31.367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Final Inspection:\\n\")\n",
    "print(\"Top 20 Accuracy Report:\\n\")\n",
    "print(df_acc.head(20))\n",
    "print(f\"Full Response:\\n\")\n",
    "print(df_acc[\"full_response\"])\n",
    "print(\"Perplexity Report:\\n\")\n",
    "print(df_ppl.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-15T17:16:31.367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df_acc[[\"full_response\", \"generated_tokens\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-15T17:16:31.367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def zip_all_run_folders(output_base_path: str):\n",
    "    os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "    search_pattern = os.path.join(output_base_path, \"run_*\")\n",
    "    run_folders = glob.glob(search_pattern)\n",
    "    run_directories = [d for d in run_folders if os.path.isdir(d)]\n",
    "\n",
    "    if not run_directories:\n",
    "        print(\n",
    "            f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c n√†o b·∫Øt ƒë·∫ßu b·∫±ng 'run_' trong '{output_base_path}'.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    print(f\"üîç T√¨m th·∫•y {len(run_directories)} th∆∞ m·ª•c k·∫øt qu·∫£ ƒë·ªÉ n√©n.\")\n",
    "\n",
    "    successful_zips = 0\n",
    "\n",
    "    for folder_path in run_directories:\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        zip_filename = os.path.join(output_base_path, f\"{folder_name}.zip\")\n",
    "\n",
    "        try:\n",
    "            print(f\"\\n   -> ƒêang n√©n th∆∞ m·ª•c: {folder_name}...\")\n",
    "\n",
    "            with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "                for root, _, files in os.walk(folder_path):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arcname = os.path.relpath(\n",
    "                            file_path, os.path.dirname(folder_path)\n",
    "                        )\n",
    "                        zipf.write(file_path, arcname)\n",
    "\n",
    "            print(f\"   ‚úÖ ƒê√£ t·∫°o file ZIP: {os.path.basename(zip_filename)}\")\n",
    "            successful_zips += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå L·ªói khi n√©n th∆∞ m·ª•c {folder_name}: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n‚úÖ HO√ÄN T·∫§T! ƒê√£ n√©n th√†nh c√¥ng {successful_zips} tr√™n {len(run_directories)} th∆∞ m·ª•c k·∫øt qu·∫£.\"\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    if \"OUTPUT_PATH\" in globals():\n",
    "        zip_all_run_folders(OUTPUT_PATH)\n",
    "    else:\n",
    "        print(\"OUTPUT_PATH not defined.\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"OUTPUT_PATH not defined.\")\n",
    "except Exception as e:\n",
    "    print(f\"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh n√©n: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 524172,
     "modelInstanceId": 509506,
     "sourceId": 672476,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}