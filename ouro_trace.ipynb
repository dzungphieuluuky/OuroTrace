{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0M-JZCdnzA6"
      },
      "source": [
        "# Setup libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T10:34:12.331839Z",
          "iopub.status.busy": "2025-12-22T10:34:12.331553Z",
          "iopub.status.idle": "2025-12-22T10:35:54.206257Z",
          "shell.execute_reply": "2025-12-22T10:35:54.205276Z",
          "shell.execute_reply.started": "2025-12-22T10:34:12.331818Z"
        },
        "id": "tjC_YOBlnzA-",
        "outputId": "f9ecf698-72bb-4905-e7d8-95fe2d9b1557",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mpip==25.3                                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2m                                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 42ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.45ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 89ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m95 packages\u001b[0m \u001b[2min 368ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `lm-eval==0.4.9.2` does not have an extra named `hf`\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 126ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 686ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 47ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --upgrade pip\n",
        "!uv pip uninstall transformers tokenizers accelerate -q\n",
        "\n",
        "!uv pip install \"transformers==4.56.0\" \"protobuf==5.29.3\" -q\n",
        "!uv pip install torch datasets -q\n",
        "!uv pip install pandas matplotlib seaborn tqdm wandb pyyaml\n",
        "!uv pip install bitsandbytes accelerate optimum lm_eval[hf]\n",
        "# !uv pip install -r requirements.txt\n",
        "!uv pip install --force-reinstall --no-cache-dir \"numpy<2.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t_XpUsOs_my"
      },
      "source": [
        "# Suppress warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T10:35:54.208095Z",
          "iopub.status.busy": "2025-12-22T10:35:54.207855Z",
          "iopub.status.idle": "2025-12-22T10:35:54.213473Z",
          "shell.execute_reply": "2025-12-22T10:35:54.212893Z",
          "shell.execute_reply.started": "2025-12-22T10:35:54.208067Z"
        },
        "id": "t3KSZamlnzA_",
        "outputId": "0cb8d794-2af7-4dc8-c7f7-2bcd695105a8",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Suppress warnings for clean output\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
        "print(\"âœ… Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abWE_VV3s_my"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T10:35:54.214603Z",
          "iopub.status.busy": "2025-12-22T10:35:54.214258Z",
          "iopub.status.idle": "2025-12-22T10:36:05.285524Z",
          "shell.execute_reply": "2025-12-22T10:36:05.284793Z",
          "shell.execute_reply.started": "2025-12-22T10:35:54.214572Z"
        },
        "id": "vW3-Anw9nzBA",
        "outputId": "60d6492e-c41c-4712-af0c-fde96fe0b1a7",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch Version: 2.9.0+cu126\n",
            "CUDA Available: True\n",
            "CUDA Version: 12.6\n",
            "Tue Dec 23 17:54:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\"Built-in libraries\"\n",
        "import re\n",
        "import sys\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import hashlib\n",
        "import glob\n",
        "import zipfile\n",
        "from io import StringIO\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "import yaml\n",
        "import logging\n",
        "import random\n",
        "\n",
        "\"Deep learning and NLP libraries\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    GenerationConfig,\n",
        "    logging as hf_logging,\n",
        ")\n",
        "\n",
        "\"Data processing libraries\"\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "from IPython import get_ipython\n",
        "\n",
        "# Configure logging\n",
        "logging.getLogger(\"ContinuousBatchingLogger\").setLevel(logging.ERROR)\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T10:36:05.288174Z",
          "iopub.status.busy": "2025-12-22T10:36:05.287690Z",
          "iopub.status.idle": "2025-12-22T10:36:05.294873Z",
          "shell.execute_reply": "2025-12-22T10:36:05.294103Z",
          "shell.execute_reply.started": "2025-12-22T10:36:05.288145Z"
        },
        "id": "NxqV6hTMs_mz",
        "outputId": "3b598e89-9595-4413-d69b-ba39b3202cf0",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Environment: Google Colab\n",
            "ğŸ“‚ Data Path: /content/\n",
            "ğŸ“¦ Output Path: /content/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def configure_environment_paths():\n",
        "    \"\"\"Detect environment and configure paths\"\"\"\n",
        "    try:\n",
        "        if \"google.colab\" in str(get_ipython()):\n",
        "            print(\"âœ… Environment: Google Colab\")\n",
        "            base_data_path = \"/content/\"\n",
        "            base_output_path = \"/content/\"\n",
        "            environment_name = \"colab\"\n",
        "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
        "            print(\"âœ… Environment: Kaggle\")\n",
        "            base_data_path = \"/kaggle/input/\"\n",
        "            base_output_path = \"/kaggle/working/\"\n",
        "            environment_name = \"kaggle\"\n",
        "        else:\n",
        "            print(\"âš ï¸ Environment: Local/Unknown\")\n",
        "            base_data_path = \"./data/\"\n",
        "            base_output_path = \"./output/\"\n",
        "            environment_name = \"local\"\n",
        "    except NameError:\n",
        "        print(\"âš ï¸ Non-interactive session. Using local paths.\")\n",
        "        base_data_path = \"./data/\"\n",
        "        base_output_path = \"./output/\"\n",
        "        environment_name = \"local\"\n",
        "\n",
        "    os.makedirs(base_output_path, exist_ok=True)\n",
        "    print(f\"ğŸ“‚ Data Path: {base_data_path}\")\n",
        "    print(f\"ğŸ“¦ Output Path: {base_output_path}\")\n",
        "\n",
        "    return base_data_path, base_output_path, environment_name\n",
        "\n",
        "\n",
        "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0HV8HMrs_mz"
      },
      "source": [
        "# Setup WANDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T10:36:05.296028Z",
          "iopub.status.busy": "2025-12-22T10:36:05.295715Z",
          "iopub.status.idle": "2025-12-22T10:36:12.599551Z",
          "shell.execute_reply": "2025-12-22T10:36:12.599012Z",
          "shell.execute_reply.started": "2025-12-22T10:36:05.296011Z"
        },
        "id": "UaOteqd3nzBA",
        "outputId": "876da774-e985-46b8-c3e8-307241cbec76",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdungngocpham171\u001b[0m (\u001b[33mdungngocpham171-university-of-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "if \"colab\" in ENV_NAME:\n",
        "    from google.colab import userdata\n",
        "\n",
        "    try:\n",
        "        # Ensure 'WANDB_API_KEY' is the exact name in your Colab Secrets (the key icon)\n",
        "        wandb_key = userdata.get(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Colab Secrets: {e}\")\n",
        "\n",
        "# 2. Check if running in Kaggle\n",
        "elif \"kaggle\" in ENV_NAME:\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "        user_secrets = UserSecretsClient()\n",
        "        wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Kaggle Secrets: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDA1HyzsnzA_"
      },
      "source": [
        "# Config input/output path and clone latest repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T10:36:12.600813Z",
          "iopub.status.busy": "2025-12-22T10:36:12.600317Z",
          "iopub.status.idle": "2025-12-22T10:36:12.735819Z",
          "shell.execute_reply": "2025-12-22T10:36:12.735108Z",
          "shell.execute_reply.started": "2025-12-22T10:36:12.600788Z"
        },
        "id": "qyaPdq3RnzA8",
        "outputId": "9a889848-b9eb-48c1-d45e-eea29a844ae0",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Clone the latest github repo version\n",
        "%cd {OUTPUT_PATH}\n",
        "torch.cuda.empty_cache()\n",
        "!rm -rf OuroTrace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T10:36:12.737588Z",
          "iopub.status.busy": "2025-12-22T10:36:12.737008Z",
          "iopub.status.idle": "2025-12-22T10:36:13.911046Z",
          "shell.execute_reply": "2025-12-22T10:36:13.910280Z",
          "shell.execute_reply.started": "2025-12-22T10:36:12.737561Z"
        },
        "id": "3S4kc_Vjs_m0",
        "outputId": "15067a46-f5ff-4aad-af6f-aba005ea3805",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OuroTrace'...\n",
            "remote: Enumerating objects: 1957, done.\u001b[K\n",
            "remote: Counting objects: 100% (174/174), done.\u001b[K\n",
            "remote: Compressing objects: 100% (126/126), done.\u001b[K\n",
            "remote: Total 1957 (delta 99), reused 110 (delta 48), pack-reused 1783 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1957/1957), 3.38 MiB | 7.42 MiB/s, done.\n",
            "Resolving deltas: 100% (1273/1273), done.\n",
            "/content/OuroTrace\n"
          ]
        }
      ],
      "source": [
        "!git clone --branch claude https://github.com/dzungphieuluuky/OuroTrace.git\n",
        "%cd OuroTrace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJUVG32s_m0"
      },
      "source": [
        "# Run Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "04f42ba483a648c6b59e66fd427893c5",
            "ebe4b9476c72476d9a43d16e141d5bad",
            "1aec8df45d61468cb23c6bada16df043",
            "1691ba8a23ff4d12934d046ba12f34d3",
            "ec7742a53a1e4b4ba2fdbc31d09b1a9d",
            "63741408f2c14ef1a3790e035116405a",
            "3b37a56d91ab4e4d9c822f41103cc2c6",
            "de3e095d652842b793224f32a87ce45b",
            "b51c8fa695824d0aad4c51589e991f58",
            "b482c8426e4a4156a9539305dfaefa1e",
            "49d0a0e235d64175aa9d90bcbff15185",
            "93ff506bf69e48819fe793c78198a369",
            "3c6166f9b9d740e798de9fda60550e15",
            "60bccad1908a4b99a07d1bd0153f0eb7",
            "83ad56e84ca6460a9a5defdbb6e8bf89",
            "ce2fe2ecfdd54106a12d4d233f37e07a",
            "f143319fb1f34265a73ab0a6bf0787ad",
            "464471dd3c1d4f2bade787f232de6418",
            "7216d4de5bd34dbeaa70a341550f1358",
            "43f58cef954e4f54ba62a452645cf8c6",
            "ef04026b20bc46c1af13a33bdb485530",
            "19ce5d31f6e043e5abdcc83c4bda2869"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T10:37:57.320449Z",
          "iopub.status.busy": "2025-12-22T10:37:57.320143Z",
          "iopub.status.idle": "2025-12-22T11:06:46.761002Z",
          "shell.execute_reply": "2025-12-22T11:06:46.760406Z",
          "shell.execute_reply.started": "2025-12-22T10:37:57.320425Z"
        },
        "id": "CKc8czt-nzBB",
        "outputId": "33dbd26f-667d-412d-bd38-3a5847c7ab2c",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting Experiment...\n",
            "ğŸ”— Initializing W&B (timeout: 30s)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251223_175415-fxvuxd84</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/fxvuxd84' target=\"_blank\">comfy-tree-131</a></strong> to <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/fxvuxd84' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/fxvuxd84</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… W&B initialized\n",
            "\n",
            "======================================================================\n",
            "ğŸ”§ EXPERIMENT CONFIGURATION\n",
            "======================================================================\n",
            "Model Path: ByteDance/Ouro-1.4B-Thinking\n",
            "UT Steps to Test: [12]\n",
            "Data Type: torch.bfloat16\n",
            "4-bit Quantization: False\n",
            "Torch Compile: False\n",
            "Max Batch Size: 8\n",
            "Max New Tokens: 16\n",
            "Batching: False\n",
            "Calculate Perplexity: False\n",
            "Early Exit: 1.0\n",
            "======================================================================\n",
            "\n",
            "[+] Quality monitor initialized:\n",
            "    â†’ Garbage threshold: 30%\n",
            "    â†’ Example similarity threshold: 85%\n",
            "    â†’ Min samples before check: 10\n",
            "ğŸ² Random seed set to 42\n",
            "\n",
            "======================================================================\n",
            "ğŸ“¦ LOADING TEST DATASETS\n",
            "======================================================================\n",
            "âš™ï¸ Generating new test datasets...\n",
            "âœ… Generated test datasets\n",
            "\n",
            "Dataset Summary:\n",
            "   n_ary       :    0 samples\n",
            "   p_hop       :    0 samples\n",
            "   igsm        :  100 samples\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ğŸ“‹ PAPER COMPLIANCE CHECK\n",
            "======================================================================\n",
            "Task Alignment: {'has_n_ary': True, 'has_p_hop': True, 'has_igsm': True, 'all_paper_tasks': True}\n",
            "UT Steps Coverage: {'min_ut': 12, 'max_ut': 12, 'covers_baseline': False, 'covers_paper_range': False, 'recommended_range': [1, 2, 4, 8]}\n",
            "======================================================================\n",
            "\n",
            "âœ… Configuration saved to ../results_20251223_175418_UT_12/config.json\n",
            "âœ… Task templates saved to ../results_20251223_175418_UT_12/task_templates.json\n",
            "\n",
            "======================================================================\n",
            "ğŸ§ª EXPERIMENT 1/1: UT Steps = 12\n",
            "======================================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "âš™ï¸  LOADING MODEL CONFIGURATION\n",
            "============================================================\n",
            "Model Path: ByteDance/Ouro-1.4B-Thinking\n",
            "Requested UT Steps: 12\n",
            "Data Type: torch.bfloat16\n",
            "4-bit Quantization: False\n",
            "Torch Compile: False\n",
            "\n",
            "â†’ Base config loaded\n",
            "   Original UT steps: 4\n",
            "   Original early exit: 1.0\n",
            "\n",
            "â†’ Modified config:\n",
            "   New UT steps: 12\n",
            "   Early exit threshold: 1.0 (from default)\n",
            "\n",
            "â†’ Tokenizer loaded\n",
            "   Vocab size: 49152\n",
            "   PAD token: <|im_end|>\n",
            "   EOS token: <|im_end|>\n",
            "\n",
            "â†’ Loading model weights...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸš€ APPLYING SAFE OPTIMIZATIONS\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "   âœ“ Flash Attention / SDPA enabled\n",
            "   âœ“ TF32 enabled for matmul\n",
            "   âœ“ cuDNN auto-tuning enabled\n",
            "   âœ“ Memory pool optimized\n",
            "   â†’ Running 3 warmup passes...\n",
            "   âœ“ Warmup complete\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "============================================================\n",
            "âœ… MODEL LOADED SUCCESSFULLY\n",
            "============================================================\n",
            "Device: cuda:0\n",
            "Model dtype: torch.bfloat16\n",
            "VERIFIED UT steps: 12\n",
            "VERIFIED early exit: 1.0\n",
            "============================================================\n",
            "\n",
            "ğŸ”§ Building task templates...\n",
            "[+] Task templates with pre-tokenized components computed.\n",
            "    System prompt N_ary tokens: 821 tokens\n",
            "    System prompt P_hop tokens: 686 tokens\n",
            "    System prompt IGSM tokens: 1140 tokens\n",
            "    User prefix tokens: 14 tokens\n",
            "    User suffix tokens: 6 tokens\n",
            "    Force start tokens: 4 tokens\n",
            "âœ… Task templates built\n",
            "\n",
            "âœ… Configuration saved to ../results_20251223_175418_UT_12/config.json\n",
            "âœ… Task templates saved to ../results_20251223_175418_UT_12/task_templates.json\n",
            "âœ… Experiment configuration saved with task templates\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ğŸ¯ ACCURACY EVALUATION\n",
            "======================================================================\n",
            "\n",
            "âš ï¸ Skipping n_ary - no test items\n",
            "\n",
            "âš ï¸ Skipping p_hop - no test items\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“ Task: IGSM\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Total Samples: 100\n",
            "Batch Size: 1 (Sequential)\n",
            "Strategy: Sequential Processing\n",
            "\n",
            "Batch size < 1 or not enough items, processing sequentially.\n",
            "Processing 100 items sequentially...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   igsm:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04f42ba483a648c6b59e66fd427893c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          test_input     full_response  \\\n",
            "0  Question. E#E := 4. D#G := 5. F#K := N#N. G#A ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. D#K := G#P - G#P. P#B := 4. G#P := J...  [FINAL]  6 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. B#L := I#D. N#H := F#L * F#L. F#L :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. M#O := F#I. C#M := M#G + M#G. B#G :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. I#N := 1. D#N := D#K - H#I. J#G := C...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. P#A := N#A + N#A. E#K := N#A * A#B. ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. N#A := O#D. K#P := 4. P#D := M#L. K#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. N#O := E#I + B#E. A#N := A#D - N#O. ...  [FINAL]  1 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#D := K#C. K#C := P#B. D#E := J#F *...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. F#K := O#D. D#E := E#C - E#C. A#D :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. P#K := B#I. A#J := H#P + K#L. D#M :=...  [FINAL]  4 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "âœ… Periodic save: simple reasoning results to ../results_20251223_175418_UT_12/simple_reasoning.csv\n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#E := 6. G#E := B#O. H#G := G#E. I#...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. J#H := B#D - A#B. A#I := 4. A#P := J...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. B#K := N#A * M#O. P#L := 0. M#O := C...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#M := L#L. I#F := E#A. C#G := O#G. ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. A#N := O#J. B#E := L#P - A#K. M#I :=...  [FINAL]  1 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. P#C := B#H. I#G := B#H. F#F := 5. K#...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. H#L := C#J. J#B := G#M - I#L. C#K :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. A#H := B#E. N#G := F#H. K#E := 6. F#...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. F#I := 3. P#O := F#I. C#H := 6. P#F ...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#I := 6. B#O := O#I. I#F := 1. K#K ...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. C#A := K#B + G#D. E#I := 0. K#B := 3...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#G := F#N + E#F. O#B := N#B + N#B. ...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. E#C := K#K. I#P := J#N * F#M. K#K :=...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "âœ… Periodic save: simple reasoning results to ../results_20251223_175418_UT_12/simple_reasoning.csv\n",
            "                                          test_input     full_response  \\\n",
            "0  Question. E#D := G#M * H#M. H#K := L#O. J#D :=...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. M#G := G#O + G#O. H#E := I#G * E#B. ...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. E#B := 5. F#B := J#C. E#E := J#C. L#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. G#L := H#H. M#A := 2. F#E := A#B. A#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#E := H#H - H#B. A#G := P#I. D#M :=...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. M#O := 0. C#M := E#C. G#J := H#G. M#...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#J := L#F. H#E := 3. M#A := L#J * C...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. M#D := E#O. O#N := 4. E#O := O#N. D#...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#F := 1. O#P := N#F * C#P. E#O := O...  [FINAL]  6 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#E := L#A. C#J := 6. G#L := 6. N#L ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. B#E := 3. C#I := L#C. I#C := L#C. L#...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. M#I := P#H. G#E := C#A. B#P := K#N. ...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. P#F := 4. J#C := P#B + J#J. N#F := J...  [FINAL]  6 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "âœ… Periodic save: simple reasoning results to ../results_20251223_175418_UT_12/simple_reasoning.csv\n",
            "                                          test_input     full_response  \\\n",
            "0  Question. D#H := O#P. C#H := M#C. G#G := 6. A#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. C#P := 1. O#N := F#D. J#O := L#B. G#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#N := 6. P#J := E#H. H#H := O#N. G#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. I#C := P#C + O#P. G#L := O#P * C#J. ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. C#L := N#J. F#M := N#J - C#P. H#J :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. J#N := M#E. M#E := 1. L#F := I#M. M#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. M#L := F#P. J#F := N#N. M#I := L#J. ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. N#P := K#N + L#A. E#O := P#L. P#L :=...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#I := J#I. H#D := O#I + L#H. A#D :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. K#L := L#O. G#L := E#I - B#O. H#E :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. C#N := A#I. N#P := P#J - C#N. A#I :=...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. G#N := F#D. J#I := D#E. D#D := I#I -...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. N#A := A#F. C#C := I#F. A#F := 5. I#...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "âœ… Periodic save: simple reasoning results to ../results_20251223_175418_UT_12/simple_reasoning.csv\n",
            "                                          test_input     full_response  \\\n",
            "0  Question. D#P := 2. K#D := G#I. E#D := D#I. M#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. I#B := O#C + P#E. N#B := 1. F#M := F...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. F#B := H#L * H#L. N#E := L#D + I#O. ...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. A#F := B#L. K#E := D#M. M#M := D#I. ...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. E#A := N#E. K#J := N#H + J#N. O#H :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. D#J := L#H. D#L := J#M - E#E. I#E :=...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. K#K := 2. N#K := 3. E#P := K#K. B#I ...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. B#O := D#B * E#D. L#M := D#B. E#G :=...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. H#I := F#I + C#N. O#M := 6. J#K := L...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. B#I := A#C. I#G := K#B. A#O := 0. M#...  [FINAL]  6 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. J#E := P#B * P#B. A#J := A#C + C#D. ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. I#G := 3. A#B := B#G. O#G := 3. J#B ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. I#G := L#P * J#C. C#O := I#J. L#P :=...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "âœ… Periodic save: simple reasoning results to ../results_20251223_175418_UT_12/simple_reasoning.csv\n",
            "                                          test_input     full_response  \\\n",
            "0  Question. N#F := O#H - O#H. A#C := K#E. C#M :=...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. F#G := A#B. O#I := J#A + P#M. H#E :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. A#O := H#I. C#J := B#K. N#C := B#K. ...  [FINAL]  1 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. H#D := 5. G#A := K#L. I#J := H#F + M...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. I#P := 6. B#K := I#C * I#P. F#O := L...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. M#F := A#A. L#F := L#C. K#I := 1. F#...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. N#I := B#D. M#G := P#L + M#F. J#J :=...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. C#P := G#O. E#I := K#M - A#O. C#I :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. P#K := K#C * G#G. I#J := E#H * E#H. ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. B#B := 6. I#E := D#F. B#L := M#H. O#...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. B#E := O#G. J#G := N#F. I#M := G#G +...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#N := P#C. A#I := M#N. N#P := G#N. ...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. H#N := 1. E#F := 6. F#O := M#N * H#C...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "âœ… Periodic save: simple reasoning results to ../results_20251223_175418_UT_12/simple_reasoning.csv\n",
            "                                          test_input     full_response  \\\n",
            "0  Question. A#M := N#E * B#F. P#F := 2. I#F := 1...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#N := 0. C#A := F#E. L#A := N#M. I#...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. N#K := L#K * L#K. J#E := B#L. G#D :=...  [FINAL]  6 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#D := J#G. D#G := L#D. K#P := F#H *...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#J := O#E. C#K := 3. C#A := 5. J#A ...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. G#G := 2. J#J := O#L. P#G := A#L. O#...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. M#D := C#N. P#P := K#N. N#A := M#A -...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. N#L := 1. A#A := N#L - E#J. P#H := N...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. F#E := O#D. N#G := E#H + C#L. J#C :=...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. B#F := L#O. G#I := 4. O#M := 1. O#A ...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. J#I := C#L. H#N := C#L. I#K := 1. C#...  [FINAL]  1 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#N := 3. C#J := N#J * E#D. J#B := I...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. O#D := B#F + O#I. J#O := B#F. M#B :=...  [FINAL]  1 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "âœ… Periodic save: simple reasoning results to ../results_20251223_175418_UT_12/simple_reasoning.csv\n",
            "                                          test_input     full_response  \\\n",
            "0  Question. A#G := M#F. M#F := N#L + N#L. M#I :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. J#A := 4. B#B := I#E. O#L := B#L * B...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. A#N := 1. G#M := L#H + L#H. C#G := 0...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. G#M := N#A. K#O := G#M + G#M. J#E :=...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. D#G := B#L - I#J. I#J := J#A * G#O. ...  [FINAL]  5 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. K#I := D#O. D#L := D#O + D#O. D#O :=...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. L#M := J#I. I#J := C#K. P#M := D#P. ...  [FINAL]  1 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. I#G := 3. A#E := C#F. B#N := E#G - G...  [FINAL]  0 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. G#B := I#D * F#A. I#D := L#K. F#A :=...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. K#I := H#H. H#H := F#D. I#A := B#O. ...  [FINAL]  3 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "                                          test_input     full_response  \\\n",
            "0  Question. I#F := J#I. H#N := I#G. I#G := C#F +...  [FINAL]  2 [END]   \n",
            "\n",
            "   generated_tokens  \n",
            "0                 5  \n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š Summary for IGSM\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Accuracy:             18.00% (18/100)\n",
            "Avg Gen Time:        23.514s\n",
            "Avg Tokens:             5.0\n",
            "Total Duration:      2352.3s\n",
            "Throughput:            0.04 samples/sec\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“‹ Sample Results for IGSM (first 10):\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "                                                  test_input    full_response generated_tokens  is_correct  is_degenerate\n",
            "Question. E#E := 4. D#G := 5. F#K := N#N. G#A := I#O. I#O := [FINAL]  2 [END]                5       False          False\n",
            "Question. D#K := G#P - G#P. P#B := 4. G#P := J#I. E#H := 2.  [FINAL]  6 [END]                5       False          False\n",
            "Question. B#L := I#D. N#H := F#L * F#L. F#L := O#L. I#D := F [FINAL]  2 [END]                5       False          False\n",
            "Question. M#O := F#I. C#M := M#G + M#G. B#G := D#F. D#F := F [FINAL]  2 [END]                5       False          False\n",
            "Question. I#N := 1. D#N := D#K - H#I. J#G := C#J. F#L := E#F [FINAL]  2 [END]                5       False          False\n",
            "Question. P#A := N#A + N#A. E#K := N#A * A#B. G#L := A#L + A [FINAL]  2 [END]                5        True          False\n",
            "Question. N#A := O#D. K#P := 4. P#D := M#L. K#J := 3. I#G := [FINAL]  3 [END]                5       False          False\n",
            "Question. N#O := E#I + B#E. A#N := A#D - N#O. M#B := N#O + N [FINAL]  1 [END]                5       False          False\n",
            "Question. O#D := K#C. K#C := P#B. D#E := J#F * O#D. F#C := I [FINAL]  5 [END]                5       False          False\n",
            "Question. F#K := O#D. D#E := E#C - E#C. A#D := M#N * I#P. A# [FINAL]  2 [END]                5       False          False\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ğŸ¯ REASONING PRIMITIVES EVALUATION\n",
            "======================================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "ğŸ§  Running Reasoning Primitives (5-shot)\n",
            "============================================================\n",
            "\n",
            "ğŸ“‹ Task: var_assign_depth_0_code (100 samples)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  var_assign_depth_0_code:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93ff506bf69e48819fe793c78198a369"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "âŒ EXPERIMENT INTERRUPTED BY USER\n",
            "======================================================================\n",
            "\n",
            "ğŸ”— Finalizing W&B...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>igsm/accuracy</td><td>â–</td></tr><tr><td>igsm/avg_generation_time</td><td>â–</td></tr><tr><td>igsm/avg_tokens</td><td>â–</td></tr><tr><td>igsm/num_degenerate</td><td>â–</td></tr><tr><td>igsm/num_samples</td><td>â–</td></tr><tr><td>igsm/throughput</td><td>â–</td></tr><tr><td>ut_steps</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>igsm/accuracy</td><td>0.18</td></tr><tr><td>igsm/avg_generation_time</td><td>23.51405</td></tr><tr><td>igsm/avg_tokens</td><td>5</td></tr><tr><td>igsm/num_degenerate</td><td>0</td></tr><tr><td>igsm/num_samples</td><td>100</td></tr><tr><td>igsm/throughput</td><td>0.04251</td></tr><tr><td>ut_steps</td><td>12</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">comfy-tree-131</strong> at: <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/fxvuxd84' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/fxvuxd84</a><br> View project at: <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>/content/wandb/run-20251223_175415-fxvuxd84/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… W&B session closed\n",
            "======================================================================\n",
            "\n",
            "âœ… Periodic save: simple reasoning results to ../results_20251223_175418_UT_12/simple_reasoning.csv\n",
            "âœ… Configuration saved to ../results_20251223_175418_UT_12/config.json\n",
            "âœ… Task templates saved to ../results_20251223_175418_UT_12/task_templates.json\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from src.config_loader import load_config_from_json, post_process_config\n",
        "\n",
        "# this is the fused version when single and batch use the same predict function\n",
        "from src.new_runner import run_batch_experiment\n",
        "\n",
        "# this is the original version when single and batch use different functions\n",
        "# from src.runner import run_batch_experiment\n",
        "\n",
        "from src.evaluation_metrics import analyze_experiment_results\n",
        "\n",
        "\n",
        "# 1. Load Configuration from JSON\n",
        "config = load_config_from_json(\"configs/batch_ouro_1.4b_thinking.json\")\n",
        "\n",
        "# 2. Post-process (Convert 'torch.float16' string to object, generate timestamps)\n",
        "config = post_process_config(config)\n",
        "\n",
        "config[\"INFERENCE_STEPS\"] = [12]\n",
        "config[\"EVAL_SETTINGS\"][\"calculate_perplexity\"] = False\n",
        "config[\"OPTIMIZATION\"][\"enable_batch\"] = False\n",
        "config[\"DATA\"][\"n_ary\"][\"num_samples_per_level\"] = 0\n",
        "config[\"DATA\"][\"p_hop\"][\"num_samples_per_level\"] = 0\n",
        "# config[\"DATA\"][\"igsm\"][\"num_samples\"] = 0\n",
        "# config[\"DATA\"][\"reasoning_primitives\"][\"num_samples\"] = 0\n",
        "# 4. Execute\n",
        "print(\"ğŸš€ Starting Experiment...\")\n",
        "try:\n",
        "    del model, tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    acc_results, ppl_results, hol_results = run_batch_experiment(config)\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T11:06:46.763212Z",
          "iopub.status.busy": "2025-12-22T11:06:46.763008Z",
          "iopub.status.idle": "2025-12-22T11:06:46.774826Z",
          "shell.execute_reply": "2025-12-22T11:06:46.774138Z",
          "shell.execute_reply.started": "2025-12-22T11:06:46.763195Z"
        },
        "id": "8inXWmVwnzBC",
        "outputId": "d085bd92-42e1-46a0-c961-8079a195a1b3",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Found 7 result folder(s) to zip.\n",
            "   -> Zipping folder: results_20251223_171222_UT_12...\n",
            "   âœ… Created ZIP: results_20251223_171222_UT_12.zip\n",
            "   -> Zipping folder: results_20251223_175418_UT_12...\n",
            "   âœ… Created ZIP: results_20251223_175418_UT_12.zip\n",
            "   -> Zipping folder: results_20251223_172008_UT_12...\n",
            "   âœ… Created ZIP: results_20251223_172008_UT_12.zip\n",
            "   -> Zipping folder: results_20251223_165048_UT_12...\n",
            "   âœ… Created ZIP: results_20251223_165048_UT_12.zip\n",
            "   -> Zipping folder: results_20251223_174040_UT_12...\n",
            "   âœ… Created ZIP: results_20251223_174040_UT_12.zip\n",
            "   -> Zipping folder: results_20251223_173746_UT_12...\n",
            "   âœ… Created ZIP: results_20251223_173746_UT_12.zip\n",
            "   -> Zipping folder: results_20251223_170218_UT_12...\n",
            "   âœ… Created ZIP: results_20251223_170218_UT_12.zip\n",
            "\n",
            "âœ… DONE! Successfully zipped 7 out of 7 folder(s).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def find_result_folders(base_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Return a list of absolute paths to all directories under `base_path`\n",
        "    whose names start with 'results_'.\n",
        "    \"\"\"\n",
        "    pattern = os.path.join(base_path, \"results_*\")\n",
        "    # glob returns both files and directories; filter to directories only\n",
        "    return [p for p in glob.glob(pattern) if os.path.isdir(p)]\n",
        "\n",
        "\n",
        "def zip_folder(folder_path: str, output_base_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Zip the contents of `folder_path` into a file named\n",
        "    <folder_name>.zip` inside `output_base_path`.\n",
        "\n",
        "    Returns True on success, False otherwise.\n",
        "    \"\"\"\n",
        "    folder_name = os.path.basename(folder_path)\n",
        "    zip_path = os.path.join(output_base_path, f\"{folder_name}.zip\")\n",
        "\n",
        "    try:\n",
        "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
        "        with zipfile.ZipFile(\n",
        "            zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED\n",
        "        ) as zipf:\n",
        "            for root, _, files in os.walk(folder_path):\n",
        "                for file in files:\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    # Preserve relative path inside the zip\n",
        "                    arcname = os.path.relpath(full_path, os.path.dirname(folder_path))\n",
        "                    zipf.write(full_path, arcname)\n",
        "        print(f\"   âœ… Created ZIP: {os.path.basename(zip_path)}\")\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f\"   âŒ Failed to zip {folder_name}: {exc}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def zip_stats_results_folders(output_base_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Main driver: locate all result folders and zip each one.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_base_path, exist_ok=True)\n",
        "\n",
        "    result_folders = find_result_folders(output_base_path)\n",
        "\n",
        "    if not result_folders:\n",
        "        print(f\"âš ï¸ No folders starting with 'results_' found in '{output_base_path}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸ” Found {len(result_folders)} result folder(s) to zip.\")\n",
        "    successful = 0\n",
        "\n",
        "    for folder in result_folders:\n",
        "        if zip_folder(folder, output_base_path):\n",
        "            successful += 1\n",
        "\n",
        "    print(\n",
        "        f\"\\nâœ… DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Prefer an environment variable; fall back to a global if defined\n",
        "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
        "        if not output_root:\n",
        "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
        "\n",
        "        # The script expects a subâ€‘folder named 'OuroTrace' under OUTPUT_PATH\n",
        "        target_path = os.path.join(output_root, \"\")\n",
        "        zip_stats_results_folders(target_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "execution": {
          "iopub.execute_input": "2025-12-22T11:06:46.776557Z",
          "iopub.status.busy": "2025-12-22T11:06:46.776139Z",
          "iopub.status.idle": "2025-12-22T11:06:46.841571Z",
          "shell.execute_reply": "2025-12-22T11:06:46.840802Z",
          "shell.execute_reply.started": "2025-12-22T11:06:46.776541Z"
        },
        "id": "0p4QBYsDnzBB",
        "outputId": "b35e7aa7-ca6a-4295-ab13-301af868c103",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ğŸ“Š VISUALIZATION\n",
            "==================================================\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š COMPREHENSIVE METRICS ANALYSIS\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3738634459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# NOTE: The variable 'results_acc' is used here, assuming it holds the raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# (list of dicts) required by 'analyze_experiment_results'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_experiment_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Summary Statistics ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OuroTrace/src/evaluation_metrics.py\u001b[0m in \u001b[0;36manalyze_experiment_results\u001b[0;34m(results_folder, save_plots, save_dir)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnhancedOuroMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"plots\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;31m# --- Load results CSV ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
          ]
        }
      ],
      "source": [
        "# 3. Save Results\n",
        "df_acc = pd.DataFrame(acc_results)\n",
        "df_ppl = pd.DataFrame(ppl_results)\n",
        "df_hol = pd.DataFrame(hol_results)\n",
        "# 4. Visualization & Reporting\n",
        "if not df_acc.empty:\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\nğŸ“Š VISUALIZATION\\n\" + \"=\" * 50)\n",
        "\n",
        "    # Summary Tables\n",
        "    # NOTE: The variable 'results_acc' is used here, assuming it holds the raw data\n",
        "    # (list of dicts) required by 'analyze_experiment_results'.\n",
        "    summary = analyze_experiment_results(acc_results)\n",
        "    print(\"\\n--- Summary Statistics ---\")\n",
        "    print(summary)\n",
        "\n",
        "    # Plotting\n",
        "    try:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "        # Plot 1: Accuracy\n",
        "        acc_summary = (\n",
        "            df_acc.groupby([\"task_type\", \"ut_steps\"])[\"is_correct\"].mean().reset_index()\n",
        "        )\n",
        "        sns.barplot(\n",
        "            data=acc_summary, x=\"ut_steps\", y=\"is_correct\", hue=\"task_type\", ax=axes[0]\n",
        "        )\n",
        "        axes[0].set_title(\"Accuracy by UT Steps\")\n",
        "        axes[0].set_ylabel(\"Accuracy\")\n",
        "        axes[0].yaxis.set_major_formatter(\n",
        "            plt.FuncFormatter(lambda y, _: \"{:.0%}\".format(y))\n",
        "        )\n",
        "\n",
        "        # Plot 2: Time\n",
        "        time_summary = (\n",
        "            df_acc.groupby([\"task_type\", \"ut_steps\"])[\"generation_time\"]\n",
        "            .mean()\n",
        "            .reset_index()\n",
        "        )\n",
        "        sns.barplot(\n",
        "            data=time_summary,\n",
        "            x=\"ut_steps\",\n",
        "            y=\"generation_time\",\n",
        "            hue=\"task_type\",\n",
        "            ax=axes[1],\n",
        "        )\n",
        "        axes[1].set_title(\"Inference Time (s) by UT Steps\")\n",
        "\n",
        "        # Plot 3: Token Count\n",
        "        sns.boxplot(\n",
        "            data=df_acc, x=\"ut_steps\", y=\"generated_tokens\", hue=\"task_type\", ax=axes[2]\n",
        "        )\n",
        "        axes[2].set_title(\"Generated Tokens Distribution\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Visualization error: {e}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No results to visualize.\")\n",
        "\n",
        "print(\"\\nğŸ Experiment Complete.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-22T11:06:46.843117Z",
          "iopub.status.busy": "2025-12-22T11:06:46.842836Z",
          "iopub.status.idle": "2025-12-22T11:06:46.872898Z",
          "shell.execute_reply": "2025-12-22T11:06:46.871994Z",
          "shell.execute_reply.started": "2025-12-22T11:06:46.843094Z"
        },
        "id": "3zpz9ccInzBD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"Final Inspection:\\n\")\n",
        "print(\"Top 20 Accuracy Report:\\n\")\n",
        "print(df_acc.head(20))\n",
        "print(f\"Full Response:\\n\")\n",
        "print(df_acc[\"full_response\"])\n",
        "print(\"Perplexity Report:\\n\")\n",
        "print(df_ppl.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-12-22T11:06:46.873485Z",
          "iopub.status.idle": "2025-12-22T11:06:46.873923Z",
          "shell.execute_reply": "2025-12-22T11:06:46.873749Z",
          "shell.execute_reply.started": "2025-12-22T11:06:46.873732Z"
        },
        "id": "EDWCUkMqnzBD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(df_acc[[\"full_response\", \"generated_tokens\"]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04f42ba483a648c6b59e66fd427893c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebe4b9476c72476d9a43d16e141d5bad",
              "IPY_MODEL_1aec8df45d61468cb23c6bada16df043",
              "IPY_MODEL_1691ba8a23ff4d12934d046ba12f34d3"
            ],
            "layout": "IPY_MODEL_ec7742a53a1e4b4ba2fdbc31d09b1a9d"
          }
        },
        "ebe4b9476c72476d9a43d16e141d5bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63741408f2c14ef1a3790e035116405a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b37a56d91ab4e4d9c822f41103cc2c6",
            "value": "â€‡â€‡â€‡igsm:â€‡100%"
          }
        },
        "1aec8df45d61468cb23c6bada16df043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de3e095d652842b793224f32a87ce45b",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b51c8fa695824d0aad4c51589e991f58",
            "value": 100
          }
        },
        "1691ba8a23ff4d12934d046ba12f34d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b482c8426e4a4156a9539305dfaefa1e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_49d0a0e235d64175aa9d90bcbff15185",
            "value": "â€‡100/100â€‡[39:12&lt;00:00,â€‡23.39s/it]"
          }
        },
        "ec7742a53a1e4b4ba2fdbc31d09b1a9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "63741408f2c14ef1a3790e035116405a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b37a56d91ab4e4d9c822f41103cc2c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de3e095d652842b793224f32a87ce45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51c8fa695824d0aad4c51589e991f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b482c8426e4a4156a9539305dfaefa1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d0a0e235d64175aa9d90bcbff15185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93ff506bf69e48819fe793c78198a369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c6166f9b9d740e798de9fda60550e15",
              "IPY_MODEL_60bccad1908a4b99a07d1bd0153f0eb7",
              "IPY_MODEL_83ad56e84ca6460a9a5defdbb6e8bf89"
            ],
            "layout": "IPY_MODEL_ce2fe2ecfdd54106a12d4d233f37e07a"
          }
        },
        "3c6166f9b9d740e798de9fda60550e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f143319fb1f34265a73ab0a6bf0787ad",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_464471dd3c1d4f2bade787f232de6418",
            "value": "â€‡â€‡var_assign_depth_0_code:â€‡â€‡26%"
          }
        },
        "60bccad1908a4b99a07d1bd0153f0eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7216d4de5bd34dbeaa70a341550f1358",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43f58cef954e4f54ba62a452645cf8c6",
            "value": 26
          }
        },
        "83ad56e84ca6460a9a5defdbb6e8bf89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef04026b20bc46c1af13a33bdb485530",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_19ce5d31f6e043e5abdcc83c4bda2869",
            "value": "â€‡26/100â€‡[03:10&lt;08:50,â€‡â€‡7.17s/it]"
          }
        },
        "ce2fe2ecfdd54106a12d4d233f37e07a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f143319fb1f34265a73ab0a6bf0787ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "464471dd3c1d4f2bade787f232de6418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7216d4de5bd34dbeaa70a341550f1358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43f58cef954e4f54ba62a452645cf8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef04026b20bc46c1af13a33bdb485530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19ce5d31f6e043e5abdcc83c4bda2869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}