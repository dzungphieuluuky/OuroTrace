{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef92f2bc151b4288a8b849d43c83f53e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_454290f4c59c430f87c5457c695f00bf",
              "IPY_MODEL_3265a61b55954ce6aead7dfd4b716574",
              "IPY_MODEL_a2359f2feba346c285640a9d563fb978"
            ],
            "layout": "IPY_MODEL_2259429e5d534c228b4df6475b4ae337"
          }
        },
        "454290f4c59c430f87c5457c695f00bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ffcc357d35e42f0bad07feee5395cbe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5f72ac54802e42e09093dd2bdb96368a",
            "value": "â€‡â€‡â€‡n_ary:â€‡â€‡â€‡2%"
          }
        },
        "3265a61b55954ce6aead7dfd4b716574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_386786d9ab64458fb1f435151655bd72",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30a7bc39d0054376afc124bfbad6db54",
            "value": 8
          }
        },
        "a2359f2feba346c285640a9d563fb978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01740e30a1e4f73a2258ffb2631bbbd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_070510a8fc544415b22064f02f3d10b1",
            "value": "â€‡8/500â€‡[01:45&lt;1:46:02,â€‡12.93s/it]"
          }
        },
        "2259429e5d534c228b4df6475b4ae337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ffcc357d35e42f0bad07feee5395cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f72ac54802e42e09093dd2bdb96368a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "386786d9ab64458fb1f435151655bd72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a7bc39d0054376afc124bfbad6db54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f01740e30a1e4f73a2258ffb2631bbbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "070510a8fc544415b22064f02f3d10b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "<a href=\"https://www.kaggle.com/code/dzung271828/ouro-trace?scriptVersionId=288557226\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
      ],
      "metadata": {
        "id": "PFjYdqo8Q-E1"
      },
      "cell_type": "markdown"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup libraries"
      ],
      "metadata": {
        "id": "S0M-JZCdnzA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --upgrade pip\n",
        "!uv pip uninstall transformers tokenizers accelerate -q\n",
        "\n",
        "!uv pip install \"transformers==4.56.0\" \"protobuf==5.29.3\" -q\n",
        "!uv pip install torch datasets -q\n",
        "!uv pip install pandas matplotlib seaborn tqdm wandb pyyaml\n",
        "!uv pip install bitsandbytes accelerate optimum lm_eval\n",
        "# !uv pip install -r requirements.txt\n",
        "!uv pip install --force-reinstall --no-cache-dir \"numpy<2.0\""
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-12-26T15:01:26.62614Z",
          "iopub.execute_input": "2025-12-26T15:01:26.626721Z",
          "iopub.status.idle": "2025-12-26T15:01:29.159696Z",
          "shell.execute_reply.started": "2025-12-26T15:01:26.626698Z",
          "shell.execute_reply": "2025-12-26T15:01:29.158969Z"
        },
        "id": "tjC_YOBlnzA-",
        "outputId": "7d998f0c-a3c1-4eea-c08e-b6a4a678a8df",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mpip==25.3                                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2m                                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 47ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.27ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 139ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m95 packages\u001b[0m \u001b[2min 312ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 109ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 411ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suppress warnings"
      ],
      "metadata": {
        "id": "7t_XpUsOs_my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings for clean output\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
        "print(\"âœ… Packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-12-26T15:01:29.161259Z",
          "iopub.execute_input": "2025-12-26T15:01:29.161514Z",
          "iopub.status.idle": "2025-12-26T15:01:29.166226Z",
          "shell.execute_reply.started": "2025-12-26T15:01:29.161489Z",
          "shell.execute_reply": "2025-12-26T15:01:29.165483Z"
        },
        "id": "t3KSZamlnzA_",
        "outputId": "67ccee26-9c48-43d7-d429-3f262ee264e4",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Packages installed successfully!\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "abWE_VV3s_my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Built-in libraries\"\n",
        "import re\n",
        "import sys\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import hashlib\n",
        "import glob\n",
        "import zipfile\n",
        "from io import StringIO\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "import yaml\n",
        "import logging\n",
        "import random\n",
        "\n",
        "\"Deep learning and NLP libraries\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    GenerationConfig,\n",
        "    logging as hf_logging,\n",
        ")\n",
        "\n",
        "\"Data processing libraries\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "from IPython import get_ipython\n",
        "\n",
        "# Configure logging\n",
        "logging.getLogger(\"ContinuousBatchingLogger\").setLevel(logging.ERROR)\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-12-26T15:01:29.167072Z",
          "iopub.execute_input": "2025-12-26T15:01:29.167333Z",
          "iopub.status.idle": "2025-12-26T15:01:29.435225Z",
          "shell.execute_reply.started": "2025-12-26T15:01:29.167308Z",
          "shell.execute_reply": "2025-12-26T15:01:29.434416Z"
        },
        "id": "vW3-Anw9nzBA",
        "outputId": "b112d497-5b40-4269-9a01-f75c989f09c1",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch Version: 2.9.0+cu126\n",
            "CUDA Available: True\n",
            "CUDA Version: 12.6\n",
            "Sat Dec 27 04:07:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P0             30W /   70W |     150MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def configure_environment_paths():\n",
        "    \"\"\"Detect environment and configure paths\"\"\"\n",
        "    try:\n",
        "        if \"google.colab\" in str(get_ipython()):\n",
        "            print(\"âœ… Environment: Google Colab\")\n",
        "            base_data_path = \"/content/\"\n",
        "            base_output_path = \"/content/\"\n",
        "            environment_name = \"colab\"\n",
        "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
        "            print(\"âœ… Environment: Kaggle\")\n",
        "            base_data_path = \"/kaggle/input/\"\n",
        "            base_output_path = \"/kaggle/working/\"\n",
        "            environment_name = \"kaggle\"\n",
        "        else:\n",
        "            print(\"âš ï¸ Environment: Local/Unknown\")\n",
        "            base_data_path = \"./data/\"\n",
        "            base_output_path = \"./output/\"\n",
        "            environment_name = \"local\"\n",
        "    except NameError:\n",
        "        print(\"âš ï¸ Non-interactive session. Using local paths.\")\n",
        "        base_data_path = \"./data/\"\n",
        "        base_output_path = \"./output/\"\n",
        "        environment_name = \"local\"\n",
        "\n",
        "    os.makedirs(base_output_path, exist_ok=True)\n",
        "    print(f\"ðŸ“‚ Data Path: {base_data_path}\")\n",
        "    print(f\"ðŸ“¦ Output Path: {base_output_path}\")\n",
        "\n",
        "    return base_data_path, base_output_path, environment_name\n",
        "\n",
        "\n",
        "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-12-26T15:01:29.43644Z",
          "iopub.execute_input": "2025-12-26T15:01:29.436701Z",
          "iopub.status.idle": "2025-12-26T15:01:29.443721Z",
          "shell.execute_reply.started": "2025-12-26T15:01:29.436675Z",
          "shell.execute_reply": "2025-12-26T15:01:29.442967Z"
        },
        "id": "NxqV6hTMs_mz",
        "outputId": "3fc8f675-cf3d-42b5-8205-568f643055fa",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Environment: Google Colab\n",
            "ðŸ“‚ Data Path: /content/\n",
            "ðŸ“¦ Output Path: /content/\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup WANDB"
      ],
      "metadata": {
        "id": "m0HV8HMrs_mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "if \"colab\" in ENV_NAME:\n",
        "    from google.colab import userdata\n",
        "\n",
        "    try:\n",
        "        # Ensure 'WANDB_API_KEY' is the exact name in your Colab Secrets (the key icon)\n",
        "        wandb_key = userdata.get(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Colab Secrets: {e}\")\n",
        "\n",
        "# 2. Check if running in Kaggle\n",
        "elif \"kaggle\" in ENV_NAME:\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "        user_secrets = UserSecretsClient()\n",
        "        wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Kaggle Secrets: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-12-26T15:01:29.44586Z",
          "iopub.execute_input": "2025-12-26T15:01:29.446153Z",
          "iopub.status.idle": "2025-12-26T15:01:29.637392Z",
          "shell.execute_reply.started": "2025-12-26T15:01:29.446136Z",
          "shell.execute_reply": "2025-12-26T15:01:29.63687Z"
        },
        "id": "UaOteqd3nzBA",
        "outputId": "1ccccb83-55a1-4d6d-e283-1392a0c8074e",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config input/output path and clone latest repo"
      ],
      "metadata": {
        "id": "wDA1HyzsnzA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the latest github repo version\n",
        "%cd {OUTPUT_PATH}\n",
        "torch.cuda.empty_cache()\n",
        "!rm -rf OuroTrace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-12-26T15:01:29.637985Z",
          "iopub.execute_input": "2025-12-26T15:01:29.638187Z",
          "iopub.status.idle": "2025-12-26T15:01:29.782772Z",
          "shell.execute_reply.started": "2025-12-26T15:01:29.638171Z",
          "shell.execute_reply": "2025-12-26T15:01:29.781898Z"
        },
        "id": "qyaPdq3RnzA8",
        "outputId": "3c2188b8-b655-47b6-9586-4320a38fc543",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch claude https://github.com/dzungphieuluuky/OuroTrace.git\n",
        "%cd OuroTrace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-12-26T15:01:29.783774Z",
          "iopub.execute_input": "2025-12-26T15:01:29.784001Z",
          "iopub.status.idle": "2025-12-26T15:01:30.609172Z",
          "shell.execute_reply.started": "2025-12-26T15:01:29.783978Z",
          "shell.execute_reply": "2025-12-26T15:01:30.608411Z"
        },
        "id": "3S4kc_Vjs_m0",
        "outputId": "e2428bf8-5703-4306-a964-c8e00feb8a35",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OuroTrace'...\n",
            "remote: Enumerating objects: 2422, done.\u001b[K\n",
            "remote: Counting objects: 100% (304/304), done.\u001b[K\n",
            "remote: Compressing objects: 100% (246/246), done.\u001b[K\n",
            "remote: Total 2422 (delta 173), reused 177 (delta 58), pack-reused 2118 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2422/2422), 7.46 MiB | 14.02 MiB/s, done.\n",
            "Resolving deltas: 100% (1530/1530), done.\n",
            "/content/OuroTrace\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Benchmark"
      ],
      "metadata": {
        "id": "GmJUVG32s_m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.config_loader import load_config_from_json, post_process_config\n",
        "from src.new_refactored_runner import run_experiment\n",
        "from src.evaluation_analysis import analyze_experiment_results\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    random.seed(seed)                          # Python random\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)  # Python hash seed\n",
        "    np.random.seed(seed)                      # NumPy\n",
        "    torch.manual_seed(seed)                   # PyTorch CPU & GPU\n",
        "\n",
        "    # Additional GPU-specific settings\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)      # For multi-GPU\n",
        "\n",
        "set_all_seeds(1415)\n",
        "# 1. Load Configuration from JSON\n",
        "config = load_config_from_json(\"configs/ouro_1.4b_thinking.json\")\n",
        "\n",
        "# 2. Post-process (Convert 'torch.float16' string to object, generate timestamps)\n",
        "config = post_process_config(config)\n",
        "\n",
        "config[\"INFERENCE_STEPS\"] = [9]\n",
        "config[\"OPTIMIZATION\"][\"enable_batch\"] = False\n",
        "config[\"EVAL_SETTINGS\"][\"calculate_perplexity\"] = False\n",
        "# config[\"DATA\"][\"n_ary\"][\"num_samples_per_level\"] = 0\n",
        "# config[\"DATA\"][\"p_hop\"][\"num_samples_per_level\"] = 0\n",
        "config[\"DATA\"][\"igsm\"][\"num_samples\"] = 0\n",
        "config[\"DATA\"][\"reasoning_primitives\"][\"num_samples\"] = 0\n",
        "\n",
        "# 4. Execute\n",
        "print(\"ðŸš€ Starting Experiment...\")\n",
        "try:\n",
        "    del model, tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    simple_reasoning_results, ppl_results, primitives_results, benchmark_results = run_experiment(config)\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ef92f2bc151b4288a8b849d43c83f53e",
            "454290f4c59c430f87c5457c695f00bf",
            "3265a61b55954ce6aead7dfd4b716574",
            "a2359f2feba346c285640a9d563fb978",
            "2259429e5d534c228b4df6475b4ae337",
            "9ffcc357d35e42f0bad07feee5395cbe",
            "5f72ac54802e42e09093dd2bdb96368a",
            "386786d9ab64458fb1f435151655bd72",
            "30a7bc39d0054376afc124bfbad6db54",
            "f01740e30a1e4f73a2258ffb2631bbbd",
            "070510a8fc544415b22064f02f3d10b1"
          ]
        },
        "execution": {
          "iopub.status.busy": "2025-12-26T15:01:30.610233Z",
          "iopub.execute_input": "2025-12-26T15:01:30.610463Z"
        },
        "id": "CKc8czt-nzBB",
        "outputId": "642f029b-808b-4223-ca83-fb7fb65c08c8",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting Experiment...\n",
            "Initializing W&B (timeout: 30s)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251227_041315-t9sy323x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/t9sy323x' target=\"_blank\">clear-firefly-162</a></strong> to <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/t9sy323x' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/t9sy323x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B initialized successfully\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT CONFIGURATION\n",
            "======================================================================\n",
            "Model Path: ByteDance/Ouro-1.4B-Thinking\n",
            "UT Steps to Test: [9]\n",
            "Data Type: torch.bfloat16\n",
            "4-bit Quantization: False\n",
            "Torch Compile: False\n",
            "Max Batch Size: 8\n",
            "Max New Tokens: 16\n",
            "Batching: False\n",
            "Calculate Perplexity: False\n",
            "Early Exit: 1.0\n",
            "======================================================================\n",
            "\n",
            "Quality monitor initialized:\n",
            "    Garbage threshold: 30%\n",
            "    Example similarity threshold: 85%\n",
            "    Min samples before check: 10\n",
            "\n",
            "======================================================================\n",
            "LOADING TEST DATASETS\n",
            "======================================================================\n",
            "Generating new test datasets...\n",
            "Generated test datasets successfully\n",
            "\n",
            "Dataset Summary:\n",
            "   n_ary       :  500 samples\n",
            "   p_hop       :  300 samples\n",
            "   igsm        :    0 samples\n",
            "======================================================================\n",
            "\n",
            "âœ… Configuration saved to ../results_20251227_041316_UT_9/config.json\n",
            "âœ… Task templates saved to ../results_20251227_041316_UT_9/task_templates.json\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT 1/1: UT Steps = 9\n",
            "======================================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "LOADING MODEL CONFIGURATION\n",
            "============================================================\n",
            "Model Path: ByteDance/Ouro-1.4B-Thinking\n",
            "Requested UT Steps: 9\n",
            "Data Type: torch.bfloat16\n",
            "4-bit Quantization: False\n",
            "Torch Compile: False\n",
            "\n",
            "Base config loaded\n",
            "   Original UT steps: 4\n",
            "   Original early exit: 1.0\n",
            "\n",
            "Modified config:\n",
            "   New UT steps: 9\n",
            "   Early exit threshold: 1.0 (from default)\n",
            "\n",
            "Tokenizer loaded\n",
            "   Vocab size: 49152\n",
            "   PAD token: <|im_end|>\n",
            "   EOS token: <|im_end|>\n",
            "\n",
            "Loading model weights...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "APPLYING SAFE OPTIMIZATIONS\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "   Flash Attention / SDPA enabled\n",
            "   TF32 enabled for matmul\n",
            "   cuDNN auto-tuning enabled\n",
            "   Memory pool optimized\n",
            "   Running 3 warmup passes...\n",
            "   Warmup complete\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "============================================================\n",
            "MODEL LOADED SUCCESSFULLY\n",
            "============================================================\n",
            "Device: cuda:0\n",
            "Model dtype: torch.bfloat16\n",
            "VERIFIED UT steps: 9\n",
            "VERIFIED early exit: 1.0\n",
            "============================================================\n",
            "\n",
            "Building task templates...\n",
            "Task templates with pre-tokenized components computed.\n",
            "    System prompt N_ary tokens: 813 tokens\n",
            "    System prompt P_hop tokens: 682 tokens\n",
            "    System prompt IGSM tokens: 1136 tokens\n",
            "    User prefix tokens: 14 tokens\n",
            "    User suffix tokens: 6 tokens\n",
            "    Force start tokens: 4 tokens\n",
            "Task templates built successfully\n",
            "\n",
            "âœ… Configuration saved to ../results_20251227_041316_UT_9/config.json\n",
            "âœ… Task templates saved to ../results_20251227_041316_UT_9/task_templates.json\n",
            "Experiment configuration saved with task templates\n",
            "\n",
            "======================================================================\n",
            "ACCURACY EVALUATION\n",
            "======================================================================\n",
            "\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Task: N_ARY\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Total Samples: 500\n",
            "Batch Size: 1\n",
            "Strategy: Sequential Processing\n",
            "\n",
            "Processing 500 items sequentially...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   n_ary:   0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef92f2bc151b4288a8b849d43c83f53e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    test_input        full_response  generated_tokens\n",
            "0  871 + 556 =  [FINAL]  1427 [END]                 8\n",
            "    test_input        full_response  generated_tokens\n",
            "0  888 + 759 =  [FINAL]  1647 [END]                 8\n",
            "    test_input        full_response  generated_tokens\n",
            "0  670 + 418 =  [FINAL]  1088 [END]                 8\n",
            "    test_input        full_response  generated_tokens\n",
            "0  661 + 544 =  [FINAL]  1205 [END]                 8\n",
            "    test_input        full_response  generated_tokens\n",
            "0  328 + 902 =  [FINAL]  1230 [END]                 8\n",
            "    test_input       full_response  generated_tokens\n",
            "0  696 + 243 =  [FINAL]  939 [END]                 7\n",
            "    test_input        full_response  generated_tokens\n",
            "0  810 + 357 =  [FINAL]  1167 [END]                 8\n",
            "    test_input       full_response  generated_tokens\n",
            "0  191 + 780 =  [FINAL]  971 [END]                 7\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "from typing import List\n",
        "def find_result_folders(base_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Return a list of absolute paths to all directories under `base_path`\n",
        "    whose names start with 'results_'.\n",
        "    \"\"\"\n",
        "    pattern = os.path.join(base_path, \"results_*\")\n",
        "    # glob returns both files and directories; filter to directories only\n",
        "    return [p for p in glob.glob(pattern) if os.path.isdir(p)]\n",
        "def zip_folder(folder_path: str, output_base_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Zip the contents of `folder_path` into a file named\n",
        "    <folder_name>.zip` inside `output_base_path`.\n",
        "\n",
        "    Returns True on success, False otherwise.\n",
        "    \"\"\"\n",
        "    folder_name = os.path.basename(folder_path)\n",
        "    zip_path = os.path.join(output_base_path, f\"{folder_name}.zip\")\n",
        "    try:\n",
        "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
        "        with zipfile.ZipFile(\n",
        "            zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED\n",
        "        ) as zipf:\n",
        "            for root, _, files in os.walk(folder_path):\n",
        "                for file in files:\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    # Preserve relative path inside the zip\n",
        "                    arcname = os.path.relpath(full_path, os.path.dirname(folder_path))\n",
        "                    zipf.write(full_path, arcname)\n",
        "        print(f\"   âœ… Created ZIP: {os.path.basename(zip_path)}\")\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f\"   âŒ Failed to zip {folder_name}: {exc}\")\n",
        "        return False\n",
        "def zip_stats_results_folders(output_base_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Main driver: locate all result folders and zip each one.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_base_path, exist_ok=True)\n",
        "    result_folders = find_result_folders(output_base_path)\n",
        "    if not result_folders:\n",
        "        print(f\"âš ï¸ No folders starting with 'results_' found in '{output_base_path}'.\")\n",
        "        return\n",
        "    print(f\"ðŸ” Found {len(result_folders)} result folder(s) to zip.\")\n",
        "    successful = 0\n",
        "    for folder in result_folders:\n",
        "        if zip_folder(folder, output_base_path):\n",
        "            successful += 1\n",
        "    print(\n",
        "        f\"\\nâœ… DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\"\n",
        "    )\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Prefer an environment variable; fall back to a global if defined\n",
        "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
        "        if not output_root:\n",
        "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
        "        # The script expects a subâ€‘folder named 'OuroTrace' under OUTPUT_PATH\n",
        "        target_path = os.path.join(output_root, \"\")\n",
        "        zip_stats_results_folders(target_path)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8inXWmVwnzBC",
        "outputId": "6302a19d-5042-4b2b-c1f0-2353ce63e948",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Found 2 result folder(s) to zip.\n",
            "   -> Zipping folder: results_20251227_030057_UT_4...\n",
            "   âœ… Created ZIP: results_20251227_030057_UT_4.zip\n",
            "   -> Zipping folder: results_20251227_040751_UT_12...\n",
            "   âœ… Created ZIP: results_20251227_040751_UT_12.zip\n",
            "\n",
            "âœ… DONE! Successfully zipped 2 out of 2 folder(s).\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "df_simple = pd.DataFrame(simple_reasoning_results)\n",
        "df_ppl = pd.DataFrame(ppl_results)\n",
        "\n",
        "print(\"Final Inspection:\\n\")\n",
        "print(\"Top 20 Accuracy Report:\\n\")\n",
        "print(df_simple.head(20))\n",
        "print(f\"Full Response:\\n\")\n",
        "print(df_simple[\"full_response\"])\n",
        "print(\"Perplexity Report:\\n\")\n",
        "print(df_ppl.head(20))"
      ],
      "metadata": {
        "id": "3zpz9ccInzBD",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7367fb72-b55e-440b-a15b-6f1b6e9e5d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Inspection:\n",
            "\n",
            "Top 20 Accuracy Report:\n",
            "\n",
            "   task_type difficulty   test_input expected_answer prediction  is_correct  \\\n",
            "0      n_ary      2_ops  871 + 556 =            1427       1427        True   \n",
            "1      n_ary      2_ops  888 + 759 =            1647       1647        True   \n",
            "2      n_ary      2_ops  670 + 418 =            1088       1088        True   \n",
            "3      n_ary      2_ops  661 + 544 =            1205       1205        True   \n",
            "4      n_ary      2_ops  328 + 902 =            1230       1230        True   \n",
            "5      n_ary      2_ops  696 + 243 =             939        939        True   \n",
            "6      n_ary      2_ops  810 + 357 =            1167       1167        True   \n",
            "7      n_ary      2_ops  191 + 780 =             971        971        True   \n",
            "8      n_ary      2_ops  933 + 652 =            1585       1585        True   \n",
            "9      n_ary      2_ops  995 + 551 =            1546       1546        True   \n",
            "10     n_ary      2_ops  227 + 885 =            1112       1112        True   \n",
            "11     n_ary      2_ops  395 + 913 =            1308       1308        True   \n",
            "12     n_ary      2_ops  661 + 800 =            1461       1461        True   \n",
            "13     n_ary      2_ops  262 + 254 =             516        516        True   \n",
            "14     n_ary      2_ops  244 + 977 =            1221       1221        True   \n",
            "\n",
            "     test_id  ut_steps        full_response  generation_time  \\\n",
            "0   79c641e4        12  [FINAL]  1427 [END]        16.918739   \n",
            "1   5ca9e316        12  [FINAL]  1647 [END]        17.499172   \n",
            "2   f45d66a6        12  [FINAL]  1088 [END]        16.454716   \n",
            "3   d333d682        12  [FINAL]  1205 [END]        16.465338   \n",
            "4   508bcc7b        12  [FINAL]  1230 [END]        16.994182   \n",
            "5   7d6f48ef        12   [FINAL]  939 [END]        16.808230   \n",
            "6   a04564cd        12  [FINAL]  1167 [END]        16.998931   \n",
            "7   2beef17d        12   [FINAL]  971 [END]        16.312245   \n",
            "8   ae954ab6        12  [FINAL]  1585 [END]        16.784240   \n",
            "9   cf933cbf        12  [FINAL]  1546 [END]        16.533372   \n",
            "10  3c7a8694        12  [FINAL]  1112 [END]        16.978234   \n",
            "11  1534f4e1        12  [FINAL]  1308 [END]        17.043410   \n",
            "12  acea4092        12  [FINAL]  1461 [END]        16.467082   \n",
            "13  c6d77350        12   [FINAL]  516 [END]        16.870500   \n",
            "14  7d21bd53        12  [FINAL]  1221 [END]        17.041259   \n",
            "\n",
            "    generated_tokens  input_tokens  is_degenerate  \n",
            "0                  8           846          False  \n",
            "1                  8           846          False  \n",
            "2                  8           846          False  \n",
            "3                  8           846          False  \n",
            "4                  8           846          False  \n",
            "5                  7           846          False  \n",
            "6                  8           846          False  \n",
            "7                  7           846          False  \n",
            "8                  8           846          False  \n",
            "9                  8           846          False  \n",
            "10                 8           846          False  \n",
            "11                 8           846          False  \n",
            "12                 8           846          False  \n",
            "13                 7           846          False  \n",
            "14                 8           846          False  \n",
            "Full Response:\n",
            "\n",
            "0     [FINAL]  1427 [END]\n",
            "1     [FINAL]  1647 [END]\n",
            "2     [FINAL]  1088 [END]\n",
            "3     [FINAL]  1205 [END]\n",
            "4     [FINAL]  1230 [END]\n",
            "5      [FINAL]  939 [END]\n",
            "6     [FINAL]  1167 [END]\n",
            "7      [FINAL]  971 [END]\n",
            "8     [FINAL]  1585 [END]\n",
            "9     [FINAL]  1546 [END]\n",
            "10    [FINAL]  1112 [END]\n",
            "11    [FINAL]  1308 [END]\n",
            "12    [FINAL]  1461 [END]\n",
            "13     [FINAL]  516 [END]\n",
            "14    [FINAL]  1221 [END]\n",
            "Name: full_response, dtype: object\n",
            "Perplexity Report:\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_simple[[\"full_response\", \"generated_tokens\"]])"
      ],
      "metadata": {
        "id": "EDWCUkMqnzBD",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731156b0-54c4-41d2-a3ad-c6c93193fef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          full_response  generated_tokens\n",
            "0   [FINAL]  1427 [END]                 8\n",
            "1   [FINAL]  1647 [END]                 8\n",
            "2   [FINAL]  1088 [END]                 8\n",
            "3   [FINAL]  1205 [END]                 8\n",
            "4   [FINAL]  1230 [END]                 8\n",
            "5    [FINAL]  939 [END]                 7\n",
            "6   [FINAL]  1167 [END]                 8\n",
            "7    [FINAL]  971 [END]                 7\n",
            "8   [FINAL]  1585 [END]                 8\n",
            "9   [FINAL]  1546 [END]                 8\n",
            "10  [FINAL]  1112 [END]                 8\n",
            "11  [FINAL]  1308 [END]                 8\n",
            "12  [FINAL]  1461 [END]                 8\n",
            "13   [FINAL]  516 [END]                 7\n",
            "14  [FINAL]  1221 [END]                 8\n"
          ]
        }
      ],
      "execution_count": 21
    }
  ]
}