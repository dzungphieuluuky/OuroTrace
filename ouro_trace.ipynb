{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0M-JZCdnzA6"
   },
   "source": [
    "# Setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:34:12.331839Z",
     "iopub.status.busy": "2025-12-22T10:34:12.331553Z",
     "iopub.status.idle": "2025-12-22T10:35:54.206257Z",
     "shell.execute_reply": "2025-12-22T10:35:54.205276Z",
     "shell.execute_reply.started": "2025-12-22T10:34:12.331818Z"
    },
    "id": "tjC_YOBlnzA-",
    "outputId": "77f8888b-ad5a-48a0-b7e5-06ef67012b57",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "peft 0.16.0 requires accelerate>=0.21.0, which is not installed.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 5.29.3 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "peft 0.16.0 requires accelerate>=0.21.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.12.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting optimum\n",
      "  Downloading optimum-2.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum) (4.56.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading optimum-2.1.0-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: optimum, bitsandbytes, accelerate\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [accelerate]3\u001b[0m [accelerate]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 bitsandbytes-0.49.0 optimum-2.1.0\n",
      "Collecting numpy<2.0\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m200.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip uninstall -y transformers tokenizers accelerate -q\n",
    "!pip install uv\n",
    "\n",
    "!uv pip install \"transformers==4.56.0\" \"protobuf==5.29.3\" -q\n",
    "!uv pip install torch datasets -q\n",
    "!uv pip install pandas matplotlib seaborn tqdm wandb pyyaml\n",
    "!uv pip install bitsandbytes accelerate optimum lm_eval[hf]\n",
    "# !uv pip install -r requirements.txt\n",
    "!uv pip install --force-reinstall --no-cache-dir \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t_XpUsOs_my"
   },
   "source": [
    "# Suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:35:54.208095Z",
     "iopub.status.busy": "2025-12-22T10:35:54.207855Z",
     "iopub.status.idle": "2025-12-22T10:35:54.213473Z",
     "shell.execute_reply": "2025-12-22T10:35:54.212893Z",
     "shell.execute_reply.started": "2025-12-22T10:35:54.208067Z"
    },
    "id": "t3KSZamlnzA_",
    "outputId": "b130842e-71a7-4f2c-a0d6-57e6b2d7ff01",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
    "print(\"âœ… Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abWE_VV3s_my"
   },
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:35:54.214603Z",
     "iopub.status.busy": "2025-12-22T10:35:54.214258Z",
     "iopub.status.idle": "2025-12-22T10:36:05.285524Z",
     "shell.execute_reply": "2025-12-22T10:36:05.284793Z",
     "shell.execute_reply.started": "2025-12-22T10:35:54.214572Z"
    },
    "id": "vW3-Anw9nzBA",
    "outputId": "bb49e0bc-2108-4508-cf37-5be5c99af1c9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.4\n",
      "Mon Dec 22 10:36:05 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   36C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\"Built-in libraries\"\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import glob\n",
    "import zipfile\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import yaml\n",
    "import logging\n",
    "import random\n",
    "\n",
    "\"Deep learning and NLP libraries\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    logging as hf_logging\n",
    ")\n",
    "\n",
    "\"Data processing libraries\"\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Configure logging\n",
    "logging.getLogger(\"ContinuousBatchingLogger\").setLevel(logging.ERROR)\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:36:05.288174Z",
     "iopub.status.busy": "2025-12-22T10:36:05.287690Z",
     "iopub.status.idle": "2025-12-22T10:36:05.294873Z",
     "shell.execute_reply": "2025-12-22T10:36:05.294103Z",
     "shell.execute_reply.started": "2025-12-22T10:36:05.288145Z"
    },
    "id": "NxqV6hTMs_mz",
    "outputId": "8e13eba6-33c3-4421-e878-0dc625409452",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment: Kaggle\n",
      "ğŸ“‚ Data Path: /kaggle/input/\n",
      "ğŸ“¦ Output Path: /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def configure_environment_paths():\n",
    "    \"\"\"Detect environment and configure paths\"\"\"\n",
    "    try:\n",
    "        if \"google.colab\" in str(get_ipython()):\n",
    "            print(\"âœ… Environment: Google Colab\")\n",
    "            base_data_path = \"/content/\"\n",
    "            base_output_path = \"/content/\"\n",
    "            environment_name = \"colab\"\n",
    "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
    "            print(\"âœ… Environment: Kaggle\")\n",
    "            base_data_path = \"/kaggle/input/\"\n",
    "            base_output_path = \"/kaggle/working/\"\n",
    "            environment_name = \"kaggle\"\n",
    "        else:\n",
    "            print(\"âš ï¸ Environment: Local/Unknown\")\n",
    "            base_data_path = \"./data/\"\n",
    "            base_output_path = \"./output/\"\n",
    "            environment_name = \"local\"\n",
    "    except NameError:\n",
    "        print(\"âš ï¸ Non-interactive session. Using local paths.\")\n",
    "        base_data_path = \"./data/\"\n",
    "        base_output_path = \"./output/\"\n",
    "        environment_name = \"local\"\n",
    "\n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    print(f\"ğŸ“‚ Data Path: {base_data_path}\")\n",
    "    print(f\"ğŸ“¦ Output Path: {base_output_path}\")\n",
    "\n",
    "    return base_data_path, base_output_path, environment_name\n",
    "\n",
    "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0HV8HMrs_mz"
   },
   "source": [
    "# Setup WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:36:05.296028Z",
     "iopub.status.busy": "2025-12-22T10:36:05.295715Z",
     "iopub.status.idle": "2025-12-22T10:36:12.599551Z",
     "shell.execute_reply": "2025-12-22T10:36:12.599012Z",
     "shell.execute_reply.started": "2025-12-22T10:36:05.296011Z"
    },
    "id": "UaOteqd3nzBA",
    "outputId": "8559c14a-f8cd-488c-eabd-b12fd23a235f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdungngocpham171\u001b[0m (\u001b[33mdungngocpham171-university-of-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "if \"colab\" in ENV_NAME:\n",
    "    from google.colab import userdata\n",
    "    try:\n",
    "        # Ensure 'WANDB_API_KEY' is the exact name in your Colab Secrets (the key icon)\n",
    "        wandb_key = userdata.get('WANDB_API_KEY')\n",
    "        wandb.login(key=wandb_key)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve W&B API key from Colab Secrets: {e}\")\n",
    "\n",
    "# 2. Check if running in Kaggle\n",
    "elif \"kaggle\" in ENV_NAME:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "        wandb.login(key=wandb_key)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve W&B API key from Kaggle Secrets: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDA1HyzsnzA_"
   },
   "source": [
    "# Config input/output path and clone latest repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:36:12.600813Z",
     "iopub.status.busy": "2025-12-22T10:36:12.600317Z",
     "iopub.status.idle": "2025-12-22T10:36:12.735819Z",
     "shell.execute_reply": "2025-12-22T10:36:12.735108Z",
     "shell.execute_reply.started": "2025-12-22T10:36:12.600788Z"
    },
    "id": "qyaPdq3RnzA8",
    "outputId": "ba5d02c2-c991-4048-85ed-413aace6220a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "# Clone the latest github repo version\n",
    "%cd {OUTPUT_PATH}\n",
    "torch.cuda.empty_cache()\n",
    "!rm -rf OuroTrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:36:12.737588Z",
     "iopub.status.busy": "2025-12-22T10:36:12.737008Z",
     "iopub.status.idle": "2025-12-22T10:36:13.911046Z",
     "shell.execute_reply": "2025-12-22T10:36:13.910280Z",
     "shell.execute_reply.started": "2025-12-22T10:36:12.737561Z"
    },
    "id": "3S4kc_Vjs_m0",
    "outputId": "f7b1f45a-6b90-46c0-e496-171319accb33",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'OuroTrace'...\n",
      "remote: Enumerating objects: 1637, done.\u001b[K\n",
      "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
      "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
      "remote: Total 1637 (delta 72), reused 68 (delta 35), pack-reused 1518 (from 2)\u001b[K\n",
      "Receiving objects: 100% (1637/1637), 2.37 MiB | 18.96 MiB/s, done.\n",
      "Resolving deltas: 100% (1057/1057), done.\n",
      "/kaggle/working/OuroTrace\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch claude https://github.com/dzungphieuluuky/OuroTrace.git\n",
    "%cd OuroTrace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmJUVG32s_m0"
   },
   "source": [
    "# Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "32572e222d4a407891a59991e68fecc6",
      "847f95ebf29c407ea86f9b23b4749fcb",
      "481a106b49eb45b1bb182d7c934baec5",
      "12bd084409f44a93b549d39a37309fcb",
      "f29d9969129545cd95aced039907b29a",
      "b0bc92e668d5462f96da18eee379006d",
      "cf726bb506474e67b4e53f80cc9feb91",
      "a4eadf41e27a45aeae336f60589b6d05",
      "7475d66c08764d0995c587070193f7cb",
      "b369e93198094a8bab3605096fb80fcd",
      "676f6b70f0b944cd8d970ac495a10a8e",
      "b36f3772bae04cc7881247c2ce805e9d",
      "ce6aab4843c64132a27d50956de5f6ac",
      "fa420ed2c6a1464ebd45dd8852c4b32e",
      "9b21f79ab1374fa5b25b85d7bb022b1b",
      "815e577a099b4964abe93c0a564affd7",
      "0a2fd9c293cd4618b98c7ed3dd3745b9",
      "7dd46686419b4028b92fcb05ce851381",
      "8a39c3b2a3be4ed2a802273ad0052cf1",
      "b429942217314e78aca11d0ec912dab8",
      "47b79b19cdd043d38f13ec5402995184",
      "fce981edebdb47b58b68ca70a139b1fe"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:37:57.320449Z",
     "iopub.status.busy": "2025-12-22T10:37:57.320143Z",
     "iopub.status.idle": "2025-12-22T11:06:46.761002Z",
     "shell.execute_reply": "2025-12-22T11:06:46.760406Z",
     "shell.execute_reply.started": "2025-12-22T10:37:57.320425Z"
    },
    "id": "CKc8czt-nzBB",
    "outputId": "61429a8f-f834-4a2a-e587-04370dabd193",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Experiment...\n",
      "ğŸ”— Initializing W&B (timeout: 30s)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251222_103757-dik9ig06</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/dik9ig06' target=\"_blank\">eager-sunset-101</a></strong> to <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/dik9ig06' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/dik9ig06</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… W&B initialized\n",
      "\n",
      "======================================================================\n",
      "ğŸ”§ EXPERIMENT CONFIGURATION\n",
      "======================================================================\n",
      "Model Path: ByteDance/Ouro-1.4B-Thinking\n",
      "UT Steps to Test: [4]\n",
      "Data Type: torch.bfloat16\n",
      "4-bit Quantization: False\n",
      "Torch Compile: False\n",
      "Max Batch Size: 12\n",
      "Max New Tokens: 512\n",
      "Batching: True\n",
      "Calculate Perplexity: True\n",
      "Early Exit: 1.0\n",
      "======================================================================\n",
      "\n",
      "[+] Quality monitor initialized:\n",
      "    â†’ Garbage threshold: 30%\n",
      "    â†’ Example similarity threshold: 85%\n",
      "    â†’ Min samples before check: 10\n",
      "ğŸ² Random seed set to 42\n",
      "\n",
      "======================================================================\n",
      "ğŸ“¦ LOADING TEST DATASETS\n",
      "======================================================================\n",
      "âš™ï¸ Generating new test datasets...\n",
      "âœ… Generated test datasets\n",
      "\n",
      "Dataset Summary:\n",
      "   n_ary       :    0 samples\n",
      "   p_hop       :    0 samples\n",
      "   igsm        :    0 samples\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‹ PAPER COMPLIANCE CHECK\n",
      "======================================================================\n",
      "Task Alignment: {'has_n_ary': True, 'has_p_hop': True, 'has_igsm': True, 'all_paper_tasks': True}\n",
      "UT Steps Coverage: {'min_ut': 4, 'max_ut': 4, 'covers_baseline': False, 'covers_paper_range': False, 'recommended_range': [1, 2, 4, 8]}\n",
      "======================================================================\n",
      "\n",
      "ğŸ“š Preparing perplexity evaluation data...\n",
      "âœ… Prepared 50 samples for PPL\n",
      "\n",
      "âœ… Configuration saved to ../results_20251222_103803/config.json\n",
      "âœ… Task templates saved to ../results_20251222_103803/task_templates.json\n",
      "\n",
      "======================================================================\n",
      "ğŸ§ª EXPERIMENT 1/1: UT Steps = 4\n",
      "======================================================================\n",
      "\n",
      "âš™ï¸  AUTO-OPTIMIZATION SETTINGS:\n",
      "   Batch Processing: âœ… ENABLED\n",
      "   Torch Compile: âŒ DISABLED\n",
      "   NOTE: torch.compile is not the culprit, batching with generate(), not with generate_batch() function.\n",
      "\n",
      "\n",
      "============================================================\n",
      "âš™ï¸  LOADING MODEL CONFIGURATION\n",
      "============================================================\n",
      "Model Path: ByteDance/Ouro-1.4B-Thinking\n",
      "Requested UT Steps: 4\n",
      "Data Type: torch.bfloat16\n",
      "4-bit Quantization: False\n",
      "Torch Compile: False\n",
      "\n",
      "â†’ Base config loaded\n",
      "   Original UT steps: 4\n",
      "   Original early exit: 1.0\n",
      "\n",
      "â†’ Modified config:\n",
      "   New UT steps: 4\n",
      "   Early exit threshold: 1.0 (from default)\n",
      "\n",
      "â†’ Tokenizer loaded\n",
      "   Vocab size: 49152\n",
      "   PAD token: <|im_end|>\n",
      "   EOS token: <|im_end|>\n",
      "\n",
      "â†’ Loading model weights...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸš€ APPLYING SAFE OPTIMIZATIONS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   âœ“ Flash Attention / SDPA enabled\n",
      "   âœ“ TF32 enabled for matmul\n",
      "   âœ“ cuDNN auto-tuning enabled\n",
      "   âœ“ Memory pool optimized\n",
      "   â†’ Running 3 warmup passes...\n",
      "   âœ“ Warmup complete\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "============================================================\n",
      "âœ… MODEL LOADED SUCCESSFULLY\n",
      "============================================================\n",
      "Device: cuda:0\n",
      "Model dtype: torch.bfloat16\n",
      "VERIFIED UT steps: 4\n",
      "VERIFIED early exit: 1.0\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ Building task templates...\n",
      "[+] Task templates with strict format enforcement preâ€‘computed.\n",
      "âœ… Task templates built\n",
      "\n",
      "âœ… Configuration saved to ../results_20251222_103803/config.json\n",
      "âœ… Task templates saved to ../results_20251222_103803/task_templates.json\n",
      "âœ… Experiment configuration saved with task templates\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‰ PERPLEXITY EVALUATION\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc4defc84ad416abb4e6e7dfcf0d47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating PPL (UT=4):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Perplexity Results:\n",
      "   Perplexity: 0.4488\n",
      "   Avg Loss:   1.5664\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ ACCURACY EVALUATION\n",
      "======================================================================\n",
      "\n",
      "âš ï¸ Skipping n_ary - no test items\n",
      "\n",
      "âš ï¸ Skipping p_hop - no test items\n",
      "\n",
      "âš ï¸ Skipping igsm - no test items\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ HOLISTIC EVALUATION\n",
      "======================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ§  Running Reasoning Primitives (5-shot)\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Task: var_assign_depth_0_code (100 samples)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  var_assign_depth_0_code:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… Accuracy: 44.00% (44/100)\n",
      "\n",
      "ğŸ“‹ Task: var_assign_depth_0_math (100 samples)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  var_assign_depth_0_math:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… Accuracy: 24.00% (24/100)\n",
      "\n",
      "ğŸ“‹ Task: var_assign_depth_0_equation (100 samples)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  var_assign_depth_0_equation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… Accuracy: 26.00% (26/100)\n",
      "\n",
      "ğŸ“‹ Task: var_assign_depth_1_code (100 samples)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  var_assign_depth_1_code:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… Accuracy: 43.00% (43/100)\n",
      "\n",
      "ğŸ“‹ Task: var_assign_depth_1_math (100 samples)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  var_assign_depth_1_math:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… Accuracy: 39.00% (39/100)\n",
      "\n",
      "ğŸ“‹ Task: var_assign_depth_1_equation (100 samples)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  var_assign_depth_1_equation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âœ… Accuracy: 34.00% (34/100)\n",
      "\n",
      "============================================================\n",
      "ğŸ“š Running Standard Benchmarks (lm-evaluation-harness)\n",
      "============================================================\n",
      "âš ï¸ 'lm-evaluation-harness' not installed. Skipping Standard Benchmarks.\n",
      "â„¹ï¸  Install with: pip install lm-eval\n",
      "âš ï¸ Holistic evaluation failed: list indices must be integers or slices, not str\n",
      "\n",
      "âœ… Periodic save: perplexity results to ../results_20251222_103803/perplexity_latest.csv\n",
      "======================================================================\n",
      "ğŸ§¹ Cleaning up GPU memory...\n",
      "âœ… GPU memory freed\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š FINAL EXPERIMENT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‰ Perplexity by UT Steps:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      " ut_steps  perplexity  avg_loss\n",
      "        4    0.448759  1.566367\n",
      "\n",
      "======================================================================\n",
      "ğŸ”— Finalizing W&B...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>perplexity</td><td>â–</td></tr><tr><td>ut_steps</td><td>â–</td></tr><tr><td>val_loss</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>perplexity</td><td>0.44876</td></tr><tr><td>ut_steps</td><td>4</td></tr><tr><td>val_loss</td><td>1.56637</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sunset-101</strong> at: <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/dik9ig06' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking/runs/dik9ig06</a><br> View project at: <a href='https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking' target=\"_blank\">https://wandb.ai/dungngocpham171-university-of-science/ouro-1.4b-thinking</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/kaggle/working/wandb/run-20251222_103757-dik9ig06/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… W&B session closed\n",
      "======================================================================\n",
      "\n",
      "âœ… Periodic save: perplexity results to ../results_20251222_103803/perplexity_latest.csv\n",
      "âœ… Configuration saved to ../results_20251222_103803/config.json\n",
      "âœ… Task templates saved to ../results_20251222_103803/task_templates.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.config_loader import load_config_from_json, post_process_config\n",
    "\n",
    "# this is the fused version when single and batch use the same predict function\n",
    "from src.new_runner import run_batch_experiment\n",
    "\n",
    "# this is the original version when single and batch use different functions\n",
    "# from src.runner import run_batch_experiment\n",
    "\n",
    "from src.evaluation import analyze_experiment_results\n",
    "\n",
    "\n",
    "# 1. Load Configuration from JSON\n",
    "config = load_config_from_json('configs/batch_ouro_1.4b_thinking.json')\n",
    "\n",
    "# 2. Post-process (Convert 'torch.float16' string to object, generate timestamps)\n",
    "config = post_process_config(config)\n",
    "\n",
    "config[\"INFERENCE_STEPS\"] = [4]\n",
    "config[\"MODEL\"][\"use_torch_compile\"] = False\n",
    "config[\"DATA\"][\"n_ary\"][\"num_samples_per_level\"] = 0\n",
    "config[\"DATA\"][\"p_hop\"][\"num_samples_per_level\"] = 0\n",
    "config[\"DATA\"][\"igsm\"][\"num_samples_total\"] = 0\n",
    "# config[\"reasoning_primitives\"][\"num_samples\"] = 0\n",
    "# 4. Execute\n",
    "print(\"ğŸš€ Starting Experiment...\")\n",
    "try:\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "  acc_results, ppl_results, hol_results = run_batch_experiment(config)\n",
    "except Exception as e:\n",
    "  print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T11:06:46.763212Z",
     "iopub.status.busy": "2025-12-22T11:06:46.763008Z",
     "iopub.status.idle": "2025-12-22T11:06:46.774826Z",
     "shell.execute_reply": "2025-12-22T11:06:46.774138Z",
     "shell.execute_reply.started": "2025-12-22T11:06:46.763195Z"
    },
    "id": "8inXWmVwnzBC",
    "outputId": "34d74913-10ce-42bd-d695-311b7169c565",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Found 2 result folder(s) to zip.\n",
      "   -> Zipping folder: results_20251222_103803...\n",
      "   âœ… Created ZIP: results_20251222_103803.zip\n",
      "   -> Zipping folder: results_20251222_103620...\n",
      "   âœ… Created ZIP: results_20251222_103620.zip\n",
      "\n",
      "âœ… DONE! Successfully zipped 2 out of 2 folder(s).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def find_result_folders(base_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return a list of absolute paths to all directories under `base_path`\n",
    "    whose names start with 'results_'.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(base_path, \"results_*\")\n",
    "    # glob returns both files and directories; filter to directories only\n",
    "    return [p for p in glob.glob(pattern) if os.path.isdir(p)]\n",
    "\n",
    "\n",
    "def zip_folder(folder_path: str, output_base_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Zip the contents of `folder_path` into a file named\n",
    "    <folder_name>.zip` inside `output_base_path`.\n",
    "\n",
    "    Returns True on success, False otherwise.\n",
    "    \"\"\"\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    zip_path = os.path.join(output_base_path, f\"{folder_name}.zip\")\n",
    "\n",
    "    try:\n",
    "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
    "        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    # Preserve relative path inside the zip\n",
    "                    arcname = os.path.relpath(full_path, os.path.dirname(folder_path))\n",
    "                    zipf.write(full_path, arcname)\n",
    "        print(f\"   âœ… Created ZIP: {os.path.basename(zip_path)}\")\n",
    "        return True\n",
    "    except Exception as exc:\n",
    "        print(f\"   âŒ Failed to zip {folder_name}: {exc}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def zip_stats_results_folders(output_base_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Main driver: locate all result folders and zip each one.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "    result_folders = find_result_folders(output_base_path)\n",
    "\n",
    "    if not result_folders:\n",
    "        print(f\"âš ï¸ No folders starting with 'results_' found in '{output_base_path}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ” Found {len(result_folders)} result folder(s) to zip.\")\n",
    "    successful = 0\n",
    "\n",
    "    for folder in result_folders:\n",
    "        if zip_folder(folder, output_base_path):\n",
    "            successful += 1\n",
    "\n",
    "    print(f\"\\nâœ… DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Prefer an environment variable; fall back to a global if defined\n",
    "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
    "        if not output_root:\n",
    "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
    "\n",
    "        # The script expects a subâ€‘folder named 'OuroTrace' under OUTPUT_PATH\n",
    "        target_path = os.path.join(output_root, \"\")\n",
    "        zip_stats_results_folders(target_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T11:06:46.776557Z",
     "iopub.status.busy": "2025-12-22T11:06:46.776139Z",
     "iopub.status.idle": "2025-12-22T11:06:46.841571Z",
     "shell.execute_reply": "2025-12-22T11:06:46.840802Z",
     "shell.execute_reply.started": "2025-12-22T11:06:46.776541Z"
    },
    "id": "0p4QBYsDnzBB",
    "outputId": "86d7072c-990c-4384-9bbd-02f6ef3fb599",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No results to visualize.\n",
      "\n",
      "ğŸ Experiment Complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Save Results\n",
    "df_acc = pd.DataFrame(acc_results)\n",
    "df_ppl = pd.DataFrame(ppl_results)\n",
    "df_hol = pd.DataFrame(hol_results)\n",
    "# 4. Visualization & Reporting\n",
    "if not df_acc.empty:\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\nğŸ“Š VISUALIZATION\\n\" + \"=\"*50)\n",
    "\n",
    "    # Summary Tables\n",
    "    # NOTE: The variable 'results_acc' is used here, assuming it holds the raw data\n",
    "    # (list of dicts) required by 'analyze_experiment_results'.\n",
    "    summary = analyze_experiment_results(acc_results)\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    print(summary)\n",
    "\n",
    "    # Plotting\n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        # Plot 1: Accuracy\n",
    "        acc_summary = df_acc.groupby(['task_type', 'ut_steps'])['is_correct'].mean().reset_index()\n",
    "        sns.barplot(data=acc_summary, x='ut_steps', y='is_correct', hue='task_type', ax=axes[0])\n",
    "        axes[0].set_title('Accuracy by UT Steps')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "\n",
    "        # Plot 2: Time\n",
    "        time_summary = df_acc.groupby(['task_type', 'ut_steps'])['generation_time'].mean().reset_index()\n",
    "        sns.barplot(data=time_summary, x='ut_steps', y='generation_time', hue='task_type', ax=axes[1])\n",
    "        axes[1].set_title('Inference Time (s) by UT Steps')\n",
    "\n",
    "        # Plot 3: Token Count\n",
    "        sns.boxplot(data=df_acc, x='ut_steps', y='generated_tokens', hue='task_type', ax=axes[2])\n",
    "        axes[2].set_title('Generated Tokens Distribution')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Visualization error: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No results to visualize.\")\n",
    "\n",
    "print(\"\\nğŸ Experiment Complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T11:06:46.843117Z",
     "iopub.status.busy": "2025-12-22T11:06:46.842836Z",
     "iopub.status.idle": "2025-12-22T11:06:46.872898Z",
     "shell.execute_reply": "2025-12-22T11:06:46.871994Z",
     "shell.execute_reply.started": "2025-12-22T11:06:46.843094Z"
    },
    "id": "3zpz9ccInzBD",
    "outputId": "b79c713a-a570-4517-b386-fe806f4cd257",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Inspection:\n",
      "\n",
      "Top 20 Accuracy Report:\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Full Response:\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'full_response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/2955619643.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Full Response:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Perplexity Report:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ppl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_response'"
     ]
    }
   ],
   "source": [
    "print(\"Final Inspection:\\n\")\n",
    "print(\"Top 20 Accuracy Report:\\n\")\n",
    "print(df_acc.head(20))\n",
    "print(f\"Full Response:\\n\")\n",
    "print(df_acc['full_response'])\n",
    "print(\"Perplexity Report:\\n\")\n",
    "print(df_ppl.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-12-22T11:06:46.873485Z",
     "iopub.status.idle": "2025-12-22T11:06:46.873923Z",
     "shell.execute_reply": "2025-12-22T11:06:46.873749Z",
     "shell.execute_reply.started": "2025-12-22T11:06:46.873732Z"
    },
    "id": "EDWCUkMqnzBD",
    "outputId": "b0c07fc8-db4d-448b-d82a-5e23d54633bb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df_acc[['full_response', 'generated_tokens']])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ourotrace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a2fd9c293cd4618b98c7ed3dd3745b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12bd084409f44a93b549d39a37309fcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b369e93198094a8bab3605096fb80fcd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_676f6b70f0b944cd8d970ac495a10a8e",
      "value": "â€‡8/8â€‡[01:10&lt;00:00,â€‡â€‡6.41s/it]"
     }
    },
    "32572e222d4a407891a59991e68fecc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_847f95ebf29c407ea86f9b23b4749fcb",
       "IPY_MODEL_481a106b49eb45b1bb182d7c934baec5",
       "IPY_MODEL_12bd084409f44a93b549d39a37309fcb"
      ],
      "layout": "IPY_MODEL_f29d9969129545cd95aced039907b29a"
     }
    },
    "47b79b19cdd043d38f13ec5402995184": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "481a106b49eb45b1bb182d7c934baec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4eadf41e27a45aeae336f60589b6d05",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7475d66c08764d0995c587070193f7cb",
      "value": 8
     }
    },
    "676f6b70f0b944cd8d970ac495a10a8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7475d66c08764d0995c587070193f7cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7dd46686419b4028b92fcb05ce851381": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "815e577a099b4964abe93c0a564affd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "847f95ebf29c407ea86f9b23b4749fcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0bc92e668d5462f96da18eee379006d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cf726bb506474e67b4e53f80cc9feb91",
      "value": "Calculatingâ€‡PPLâ€‡(UT=4):â€‡100%"
     }
    },
    "8a39c3b2a3be4ed2a802273ad0052cf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b21f79ab1374fa5b25b85d7bb022b1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47b79b19cdd043d38f13ec5402995184",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fce981edebdb47b58b68ca70a139b1fe",
      "value": "â€‡18/50â€‡[07:14&lt;12:52,â€‡24.14s/it]"
     }
    },
    "a4eadf41e27a45aeae336f60589b6d05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0bc92e668d5462f96da18eee379006d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b369e93198094a8bab3605096fb80fcd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b36f3772bae04cc7881247c2ce805e9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce6aab4843c64132a27d50956de5f6ac",
       "IPY_MODEL_fa420ed2c6a1464ebd45dd8852c4b32e",
       "IPY_MODEL_9b21f79ab1374fa5b25b85d7bb022b1b"
      ],
      "layout": "IPY_MODEL_815e577a099b4964abe93c0a564affd7"
     }
    },
    "b429942217314e78aca11d0ec912dab8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce6aab4843c64132a27d50956de5f6ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a2fd9c293cd4618b98c7ed3dd3745b9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7dd46686419b4028b92fcb05ce851381",
      "value": "â€‡â€‡â€‡igsm:â€‡â€‡36%"
     }
    },
    "cf726bb506474e67b4e53f80cc9feb91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f29d9969129545cd95aced039907b29a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa420ed2c6a1464ebd45dd8852c4b32e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a39c3b2a3be4ed2a802273ad0052cf1",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b429942217314e78aca11d0ec912dab8",
      "value": 18
     }
    },
    "fce981edebdb47b58b68ca70a139b1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
