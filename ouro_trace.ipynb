{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28fea142a857490cb30d3d03cd076843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12cf12a13ab9432a82e6bb4fbbbf3c30",
              "IPY_MODEL_3262616cea28458997cebc26467894f2",
              "IPY_MODEL_b910086207f049bf9f6af47f699057a4"
            ],
            "layout": "IPY_MODEL_ca961250938c4a4893363349d3271be9"
          }
        },
        "12cf12a13ab9432a82e6bb4fbbbf3c30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc5ae839e25147988f68d898b165061e",
            "placeholder": "​",
            "style": "IPY_MODEL_c07a4f180e7d45c48c9e98d05e3dccd4",
            "value": "Calculating PPL (UT=1): 100%"
          }
        },
        "3262616cea28458997cebc26467894f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e6616eda7b4240861d36c90de5cb5a",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bcf9b7fe3834a92b1285c4c09e29ae0",
            "value": 8
          }
        },
        "b910086207f049bf9f6af47f699057a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a2fc68ed6de4e51a945872e9098d308",
            "placeholder": "​",
            "style": "IPY_MODEL_69ac5fc148eb4a6aa28c72af53c28881",
            "value": " 8/8 [02:14&lt;00:00,  4.74s/it]"
          }
        },
        "ca961250938c4a4893363349d3271be9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc5ae839e25147988f68d898b165061e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07a4f180e7d45c48c9e98d05e3dccd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0e6616eda7b4240861d36c90de5cb5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bcf9b7fe3834a92b1285c4c09e29ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a2fc68ed6de4e51a945872e9098d308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69ac5fc148eb4a6aa28c72af53c28881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "057d55301bb34bf5b534590c03ede139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9fe945f11134633b415ed43e93f9c87",
              "IPY_MODEL_7d0f9604a4f74151996c1fcbc9791f7a",
              "IPY_MODEL_ac2ee06109284ef296369eb922824d53"
            ],
            "layout": "IPY_MODEL_8c7ad55ffd57467fa981ccbe1fe4e3e7"
          }
        },
        "c9fe945f11134633b415ed43e93f9c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b12eec8ffde4e5e9d44e2fe3bdd2b11",
            "placeholder": "​",
            "style": "IPY_MODEL_25d6c79ebc2649faa0ef556fdfc6e4e5",
            "value": "   n_ary:   2%"
          }
        },
        "7d0f9604a4f74151996c1fcbc9791f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca77202974e04953bf132498c4890115",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6211621fe02844e698dc48684c479ac1",
            "value": 1
          }
        },
        "ac2ee06109284ef296369eb922824d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a836e59d94948a89340359011503d65",
            "placeholder": "​",
            "style": "IPY_MODEL_2b6d0217b1ec40cca7cbbcdd23d342d0",
            "value": " 1/63 [01:04&lt;33:44, 32.65s/it]"
          }
        },
        "8c7ad55ffd57467fa981ccbe1fe4e3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b12eec8ffde4e5e9d44e2fe3bdd2b11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d6c79ebc2649faa0ef556fdfc6e4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca77202974e04953bf132498c4890115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6211621fe02844e698dc48684c479ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a836e59d94948a89340359011503d65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b6d0217b1ec40cca7cbbcdd23d342d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup libraries"
      ],
      "metadata": {
        "id": "S0M-JZCdnzA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip uninstall -y transformers tokenizers accelerate -q\n",
        "!pip install \"transformers==4.56.0\" \"protobuf==5.29.3\" -q\n",
        "!pip install torch datasets -q\n",
        "!pip install pandas matplotlib seaborn tqdm wandb pyyaml\n",
        "!pip install bitsandbytes accelerate\n",
        "# !pip install -r requirements.txt\n",
        "!pip install --force-reinstall --no-cache-dir \"numpy<2.0\""
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tjC_YOBlnzA-",
        "outputId": "0fb39a5b-95f2-4f28-b0ca-264f55381aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.18.0 requires accelerate>=0.21.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.3)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.47.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
            "Using cached accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-1.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "accelerate"
                ]
              },
              "id": "688cc3822c5043c6a93d905fb2998f1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-225427337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install bitsandbytes accelerate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# !pip install -r requirements.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --force-reinstall --no-cache-dir \"numpy<2.0\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings for clean output\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
        "print(\"✅ Packages installed successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3KSZamlnzA_",
        "outputId": "7bc0dfcd-849e-414d-b90b-09d957c90258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Packages installed successfully!\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "\"Built-in libraries\"\n",
        "import re\n",
        "import sys\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import hashlib\n",
        "import glob\n",
        "import zipfile\n",
        "from io import StringIO\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "import yaml\n",
        "import logging\n",
        "import random\n",
        "\n",
        "\"Deep learning and NLP libraries\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    GenerationConfig,\n",
        "    logging as hf_logging\n",
        ")\n",
        "\n",
        "\"Data processing libraries\"\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "from IPython import get_ipython\n",
        "\n",
        "# Configure logging\n",
        "logging.getLogger(\"ContinuousBatchingLogger\").setLevel(logging.ERROR)\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW3-Anw9nzBA",
        "outputId": "139a6d9e-6454-40eb-a89d-2ac7a374d835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch Version: 2.9.0+cu126\n",
            "CUDA Available: True\n",
            "CUDA Version: 12.6\n",
            "Fri Dec 19 09:47:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8             13W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "# 1. Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    try:\n",
        "        # Ensure 'WANDB_API_KEY' is the exact name in your Colab Secrets (the key icon)\n",
        "        wandb_key = userdata.get('WANDB_API_KEY')\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Colab Secrets: {e}\")\n",
        "\n",
        "# 2. Check if running in Kaggle\n",
        "elif os.path.exists('/kaggle/input'):\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        user_secrets = UserSecretsClient()\n",
        "        wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Kaggle Secrets: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "UaOteqd3nzBA",
        "outputId": "c9373c31-0e4c-49fe-fda0-55fcbc2f3a81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not retrieve W&B API key from Colab Secrets: Secret WANDB_API_KEY does not exist.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config input/output path and clone latest repo"
      ],
      "metadata": {
        "id": "wDA1HyzsnzA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the latest github repo version\n",
        "import os\n",
        "def configure_environment_paths():\n",
        "    \"\"\"Detect environment and configure paths\"\"\"\n",
        "    try:\n",
        "        if \"google.colab\" in str(get_ipython()):\n",
        "            print(\"✅ Environment: Google Colab\")\n",
        "            base_data_path = \"/content/\"\n",
        "            base_output_path = \"/content/\"\n",
        "            environment_name = \"colab\"\n",
        "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
        "            print(\"✅ Environment: Kaggle\")\n",
        "            base_data_path = \"/kaggle/input/\"\n",
        "            base_output_path = \"/kaggle/working/\"\n",
        "            environment_name = \"kaggle\"\n",
        "        else:\n",
        "            print(\"⚠️ Environment: Local/Unknown\")\n",
        "            base_data_path = \"./data/\"\n",
        "            base_output_path = \"./output/\"\n",
        "            environment_name = \"local\"\n",
        "    except NameError:\n",
        "        print(\"⚠️ Non-interactive session. Using local paths.\")\n",
        "        base_data_path = \"./data/\"\n",
        "        base_output_path = \"./output/\"\n",
        "        environment_name = \"local\"\n",
        "\n",
        "    os.makedirs(base_output_path, exist_ok=True)\n",
        "    print(f\"📂 Data Path: {base_data_path}\")\n",
        "    print(f\"📦 Output Path: {base_output_path}\")\n",
        "\n",
        "    return base_data_path, base_output_path, environment_name\n",
        "\n",
        "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()\n",
        "%cd /content/\n",
        "torch.cuda.empty_cache()\n",
        "!rm -r -f OuroTrace\n",
        "!git clone --branch claude https://github.com/dzungphieuluuky/OuroTrace.git\n",
        "%cd OuroTrace"
      ],
      "metadata": {
        "trusted": true,
        "id": "qyaPdq3RnzA8",
        "outputId": "8ebe928b-f5d0-4af3-c92e-8bc5f611203f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Environment: Google Colab\n",
            "📂 Data Path: /content/\n",
            "📦 Output Path: /content/\n",
            "/content\n",
            "Cloning into 'OuroTrace'...\n",
            "remote: Enumerating objects: 978, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 978 (delta 142), reused 125 (delta 72), pack-reused 777 (from 1)\u001b[K\n",
            "Receiving objects: 100% (978/978), 1.48 MiB | 5.06 MiB/s, done.\n",
            "Resolving deltas: 100% (656/656), done.\n",
            "/content/OuroTrace\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from src.config_loader import load_config_from_json, post_process_config\n",
        "\n",
        "# this is the fused version when single and batch use the same predict function\n",
        "from src.new_runner import run_batch_experiment\n",
        "\n",
        "# this is the original version when single and batch use different functions\n",
        "# from src.runner import run_batch_experiment\n",
        "\n",
        "from src.evaluation import analyze_experiment_results\n",
        "\n",
        "\n",
        "# 1. Load Configuration from JSON\n",
        "config = load_config_from_json('configs/batch_ouro_1.4b_thinking.json')\n",
        "\n",
        "# 2. Post-process (Convert 'torch.float16' string to object, generate timestamps)\n",
        "config = post_process_config(config)\n",
        "\n",
        "config[\"INFERENCE_STEPS\"] = [1]\n",
        "config[\"DATA\"][\"n_ary\"][\"num_samples_per_level\"] = 100\n",
        "config[\"DATA\"][\"p_hop\"][\"num_samples_per_level\"] = 0\n",
        "config[\"DATA\"][\"igsm\"][\"num_samples_per_level\"] = 0\n",
        "config[\"reasoning_primitives\"][\"num_samples\"] = 5\n",
        "\n",
        "# 4. Execute\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "print(f\"🕒 Timestamp: {timestamp}\")\n",
        "print(\"🚀 Starting Experiment...\")\n",
        "acc_results, ppl_results, hol_results = run_batch_experiment(config)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CKc8czt-nzBB",
        "outputId": "de5a1386-456d-42ee-f340-d3dee8880aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "28fea142a857490cb30d3d03cd076843",
            "12cf12a13ab9432a82e6bb4fbbbf3c30",
            "3262616cea28458997cebc26467894f2",
            "b910086207f049bf9f6af47f699057a4",
            "ca961250938c4a4893363349d3271be9",
            "bc5ae839e25147988f68d898b165061e",
            "c07a4f180e7d45c48c9e98d05e3dccd4",
            "c0e6616eda7b4240861d36c90de5cb5a",
            "3bcf9b7fe3834a92b1285c4c09e29ae0",
            "1a2fc68ed6de4e51a945872e9098d308",
            "69ac5fc148eb4a6aa28c72af53c28881",
            "057d55301bb34bf5b534590c03ede139",
            "c9fe945f11134633b415ed43e93f9c87",
            "7d0f9604a4f74151996c1fcbc9791f7a",
            "ac2ee06109284ef296369eb922824d53",
            "8c7ad55ffd57467fa981ccbe1fe4e3e7",
            "1b12eec8ffde4e5e9d44e2fe3bdd2b11",
            "25d6c79ebc2649faa0ef556fdfc6e4e5",
            "ca77202974e04953bf132498c4890115",
            "6211621fe02844e698dc48684c479ac1",
            "6a836e59d94948a89340359011503d65",
            "2b6d0217b1ec40cca7cbbcdd23d342d0"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🕒 Timestamp: 20251219_094740\n",
            "🚀 Starting Experiment...\n",
            "🔗 Initializing W&B (timeout: 30s)...\n",
            "⚠️ W&B initialization failed: 1 validation error for Settings\n",
            "start_timeout\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value=30, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden. Continuing offline.\n",
            "\n",
            "======================================================================\n",
            "🔧 EXPERIMENT CONFIGURATION\n",
            "======================================================================\n",
            "Model Path: ByteDance/Ouro-1.4B-Thinking\n",
            "UT Steps to Test: [1]\n",
            "Data Type: torch.bfloat16\n",
            "4-bit Quantization: False\n",
            "Torch Compile: True (auto-controlled)\n",
            "Max Batch Size: 8\n",
            "Max New Tokens: 512\n",
            "Batching: Auto (enabled only for UT=1)\n",
            "Calculate Perplexity: True\n",
            "Early Exit: Auto (from model config)\n",
            "======================================================================\n",
            "\n",
            "[+] Quality monitor initialized:\n",
            "    → Garbage threshold: 30%\n",
            "    → Example similarity threshold: 85%\n",
            "    → Min samples before check: 10\n",
            "🎲 Random seed set to 42\n",
            "\n",
            "======================================================================\n",
            "📦 LOADING TEST DATASETS\n",
            "======================================================================\n",
            "⚙️ Generating new test datasets...\n",
            "✅ Generated test datasets\n",
            "\n",
            "Dataset Summary:\n",
            "   n_ary       :  500 samples\n",
            "   p_hop       :    0 samples\n",
            "   igsm        :    8 samples\n",
            "======================================================================\n",
            "\n",
            "📚 Preparing perplexity evaluation data...\n",
            "✅ Prepared 50 samples for PPL\n",
            "\n",
            "\n",
            "======================================================================\n",
            "🧪 EXPERIMENT 1/1: UT Steps = 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "⚙️  LOADING MODEL CONFIGURATION\n",
            "============================================================\n",
            "Model Path: ByteDance/Ouro-1.4B-Thinking\n",
            "Requested UT Steps: 1\n",
            "Data Type: torch.bfloat16\n",
            "4-bit Quantization: False\n",
            "Torch Compile: True\n",
            "\n",
            "→ Base config loaded\n",
            "   Original UT steps: 4\n",
            "   Original early exit: 1.0\n",
            "\n",
            "→ Modified config:\n",
            "   New UT steps: 1\n",
            "   Early exit threshold: 1.0 (from default)\n",
            "\n",
            "→ Tokenizer loaded\n",
            "   Vocab size: 49152\n",
            "   PAD token: <|im_end|>\n",
            "   EOS token: <|im_end|>\n",
            "\n",
            "→ Loading model weights...\n",
            "→ Applying torch.compile()\n",
            "\n",
            "→ Applying basic optimizations (UT=1)...\n",
            "   ✓ TF32 enabled for matmul\n",
            "   ✓ cuDNN auto-tuning enabled\n",
            "   ✓ Memory pool optimized\n",
            "\n",
            "============================================================\n",
            "✅ MODEL LOADED SUCCESSFULLY\n",
            "============================================================\n",
            "Device: cuda:0\n",
            "Model dtype: torch.bfloat16\n",
            "VERIFIED UT steps: 1\n",
            "VERIFIED early exit: 1.0\n",
            "============================================================\n",
            "\n",
            "🔧 Building task templates...\n",
            "[+] Task templates (string only) pre-computed.\n",
            "\n",
            "======================================================================\n",
            "📉 PERPLEXITY EVALUATION\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Calculating PPL (UT=1):   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28fea142a857490cb30d3d03cd076843"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1219 09:48:30.333000 60058 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Perplexity Results:\n",
            "   Perplexity: 2.1719\n",
            "   Avg Loss:   8.7754\n",
            "\n",
            "======================================================================\n",
            "🎯 ACCURACY EVALUATION\n",
            "======================================================================\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "📝 Task: N_ARY\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Total Samples: 500\n",
            "Batch Size: 8\n",
            "Strategy: Batched Processing\n",
            "\n",
            "Running 63 batches...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   n_ary:   0%|          | 0/63 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "057d55301bb34bf5b534590c03ede139"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "310 + 611 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "200 + 469 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "455 + 139 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "818 + 235 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "679 + 573 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "802 + 337 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "765 + 000 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "808 + 456 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "    test_input                                      full_response\n",
            "0  310 + 611 =  \\nStep 1: \\n\\nThe problem is that the\\n\\nThe p...\n",
            "    test_input                                      full_response\n",
            "0  200 + 469 =  \\nStep 1: \\n\\n\\nThe problem is that the\\n\\nThe...\n",
            "    test_input                                      full_response\n",
            "0  455 + 139 =  \\nStep 1: \\n\\n\\nThe\\n\\nThe\\n\\nThe\\n\\nThe\\n\\nTh...\n",
            "    test_input                                      full_response\n",
            "0  818 + 235 =  \\nStep 1: \\n\\nThe\\n\\nThe\\n\\nThe\\n\\nThe\\n\\nThe\\...\n",
            "    test_input                                      full_response\n",
            "0  679 + 573 =  \\nStep 1: \\n\\n\\nThe problem is that the\\n\\nThe...\n",
            "    test_input                                      full_response\n",
            "0  802 + 337 =  \\nStep 1: \\n\\nThe\\n\\nThe\\n\\nThe\\n\\nThe\\n\\nThe\\...\n",
            "    test_input                                      full_response\n",
            "0  765 + 000 =  \\nStep 1: \\n\\n\\nThe\\n\\nThe\\n\\nThe\\n\\nThe\\n\\nTh...\n",
            "    test_input                                      full_response\n",
            "0  808 + 456 =  \\nStep 1: \\n\\nThe problem is:\\n\\nThe problem i...\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "902 + 277 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "224 + 487 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "164 + 282 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "456 + 935 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "274 + 266 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "075 + 256 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "745 + 256 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n",
            "DEBUG: Generated prompt for task 'n_ary':\n",
            "<|im_start|>system\n",
            "You are a calculator. Given an addition problem with several numbers (e.g., '{number_1} + {number_2} + {number_3} + ... ='), show your work step by step. For each number, add it to the running total and show the calculation. After all steps, output only the final sum on a new line as [FINAL] [sum].\n",
            "Example:\n",
            "Input: {number_i} + {number_i+1} + {number_i+2} + ... =\n",
            "Output:\n",
            "Step {i}: 0 + {number_i} = {sum_i}\n",
            "Step {i+1}: {sum_i} + {number_i+1} = {sum_i+1}\n",
            "Step {i+2}: {sum_i+1} + {number_i+2} = {sum_i+2}\n",
            "...[FINAL] {final_sum}<|im_end|>\n",
            "<|im_start|>user\n",
            "425 + 923 =<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "\n",
            "   ✓ Chat format verified for input.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1217463431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🕒 Timestamp: {timestamp}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Starting Experiment...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0macc_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhol_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_batch_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/OuroTrace/src/new_runner.py\u001b[0m in \u001b[0;36mrun_batch_experiment\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    239\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                         \u001b[0;31m# Use unified predict() with list of prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                         batch_outputs = experiment.predict(\n\u001b[0m\u001b[1;32m    242\u001b[0m                             \u001b[0muser_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                             \u001b[0mtask_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OuroTrace/src/new_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, user_inputs, task_type, model, tokenizer, ut_steps, generation_config, enable_batch)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             outputs = model.generate(\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0;31m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2540\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m             \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2858\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2859\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Save Results\n",
        "df_acc = pd.DataFrame(acc_results)\n",
        "df_ppl = pd.DataFrame(ppl_results)\n",
        "df_hol = pd.DataFrame(hol_results)\n",
        "# 4. Visualization & Reporting\n",
        "if not df_acc.empty:\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n📊 VISUALIZATION\\n\" + \"=\"*50)\n",
        "\n",
        "    # Summary Tables\n",
        "    # NOTE: The variable 'results_acc' is used here, assuming it holds the raw data\n",
        "    # (list of dicts) required by 'analyze_experiment_results'.\n",
        "    summary = analyze_experiment_results(acc_results)\n",
        "    print(\"\\n--- Summary Statistics ---\")\n",
        "    print(summary)\n",
        "\n",
        "    # Plotting\n",
        "    try:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "        # Plot 1: Accuracy\n",
        "        acc_summary = df_acc.groupby(['task_type', 'ut_steps'])['is_correct'].mean().reset_index()\n",
        "        sns.barplot(data=acc_summary, x='ut_steps', y='is_correct', hue='task_type', ax=axes[0])\n",
        "        axes[0].set_title('Accuracy by UT Steps')\n",
        "        axes[0].set_ylabel('Accuracy')\n",
        "        axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
        "\n",
        "        # Plot 2: Time\n",
        "        time_summary = df_acc.groupby(['task_type', 'ut_steps'])['generation_time'].mean().reset_index()\n",
        "        sns.barplot(data=time_summary, x='ut_steps', y='generation_time', hue='task_type', ax=axes[1])\n",
        "        axes[1].set_title('Inference Time (s) by UT Steps')\n",
        "\n",
        "        # Plot 3: Token Count\n",
        "        sns.boxplot(data=df_acc, x='ut_steps', y='generated_tokens', hue='task_type', ax=axes[2])\n",
        "        axes[2].set_title('Generated Tokens Distribution')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Visualization error: {e}\")\n",
        "else:\n",
        "    print(\"⚠️ No results to visualize.\")\n",
        "\n",
        "print(\"\\n🏁 Experiment Complete.\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "0p4QBYsDnzBB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "\n",
        "def zip_all_results_folders(output_base_path: str):\n",
        "    os.makedirs(output_base_path, exist_ok=True)\n",
        "\n",
        "    search_pattern = os.path.join(output_base_path, \"results_*\")\n",
        "    results_folders = glob.glob(search_pattern)\n",
        "    results_directories = [d for d in results_folders if os.path.isdir(d)]\n",
        "\n",
        "    if not results_directories:\n",
        "        print(f\"⚠️ Không tìm thấy thư mục nào bắt đầu bằng 'results_' trong '{output_base_path}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"🔍 Tìm thấy {len(results_directories)} thư mục kết quả để nén.\")\n",
        "\n",
        "    successful_zips = 0\n",
        "\n",
        "    for folder_path in results_directories:\n",
        "        folder_name = os.path.basename(folder_path)\n",
        "        zip_filename = os.path.join(output_base_path, f\"{folder_name}.zip\")\n",
        "\n",
        "        try:\n",
        "            print(f\"\\n   -> Đang nén thư mục: {folder_name}...\")\n",
        "\n",
        "            with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                for root, _, files in os.walk(folder_path):\n",
        "                    for file in files:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        arcname = os.path.relpath(file_path, os.path.dirname(folder_path))\n",
        "                        zipf.write(file_path, arcname)\n",
        "\n",
        "            print(f\"   ✅ Đã tạo file ZIP: {os.path.basename(zip_filename)}\")\n",
        "            successful_zips += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Lỗi khi nén thư mục {folder_name}: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ HOÀN TẤT! Đã nén thành công {successful_zips} trên {len(results_directories)} thư mục kết quả.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    if 'OUTPUT_PATH' in globals():\n",
        "        zip_all_results_folders(OUTPUT_PATH)\n",
        "    else:\n",
        "        print(\"OUTPUT_PATH not defined.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"OUTPUT_PATH not defined.\")\n",
        "except Exception as e:\n",
        "    print(f\"Đã xảy ra lỗi trong quá trình nén: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "8inXWmVwnzBC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final Inspection:\\n\")\n",
        "print(\"Top 20 Accuracy Report:\\n\")\n",
        "print(df_acc.head(20))\n",
        "print(f\"Full Response:\\n\")\n",
        "print(df_acc['full_response'])\n",
        "print(\"Perplexity Report:\\n\")\n",
        "print(df_ppl.head(20))"
      ],
      "metadata": {
        "trusted": true,
        "id": "3zpz9ccInzBD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_acc[['full_response', 'generated_tokens']])"
      ],
      "metadata": {
        "trusted": true,
        "id": "EDWCUkMqnzBD"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}