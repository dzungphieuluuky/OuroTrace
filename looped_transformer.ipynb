{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:15.904450Z",
     "iopub.status.busy": "2025-12-11T16:12:15.903884Z",
     "iopub.status.idle": "2025-12-11T16:12:46.955279Z",
     "shell.execute_reply": "2025-12-11T16:12:46.954399Z",
     "shell.execute_reply.started": "2025-12-11T16:12:15.904422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "peft 0.16.0 requires accelerate>=0.21.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (6.33.0)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.12.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.2)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Using cached accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.12.0\n",
      "‚úÖ Environment detected: Kaggle Notebooks.\n",
      "üìÅ Data Path: /kaggle/input/\n",
      "üì¶ Output Path: /kaggle/working/\n",
      "Kh√¥ng ph·∫£i m√¥i tr∆∞·ªùng Colab. B·ªè qua Unzip t·ª± ƒë·ªông.\n",
      "--- Python Version ---\n",
      "3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "\n",
      "--- PyTorch Version ---\n",
      "2.6.0+cu124\n",
      "\n",
      "--- PyTorch CUDA Version ---\n",
      "12.4\n",
      "\n",
      "--- System nvidia-smi (Requires GPU) ---\n",
      "Thu Dec 11 16:12:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   65C    P0             30W /   70W |   13569MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   57C    P0             26W /   70W |    1667MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "!pip install --upgrade pip\n",
    "!pip uninstall -y transformers tokenizers accelerate -q\n",
    "\n",
    "!pip install \"transformers==4.56.0\" \"protobuf>=5.29.4\" -q\n",
    "!pip install \"torch\" datasets -q\n",
    "\n",
    "!pip install pandas matplotlib seaborn tqdm wandb pyyaml\n",
    "!pip install bitsandbytes accelerate\n",
    "# !pip install --no-dependencies \"/kaggle/input/d/dzung271828/flash-attention-wheel/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp39-cp39-linux_x86_64.whl\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from io import StringIO\n",
    "import hashlib\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "from IPython import get_ipython\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers.generation import GenerationConfig\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# ====================================================================\n",
    "# A. PH√ÅT HI·ªÜN V√Ä C·∫§U H√åNH M√îI TR∆Ø·ªúNG\n",
    "# ====================================================================\n",
    "# Silence excessive logging from transformers and ouro-cache-fix\n",
    "logging.getLogger(\"ContinuousBatchingLogger\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def configure_environment_paths():\n",
    "    \"\"\"\n",
    "    Checks the execution environment (Colab or Kaggle) and configures\n",
    "    the base paths for data and output accordingly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"google.colab\" in str(get_ipython()):\n",
    "            print(\"‚úÖ Environment detected: Google Colab.\")\n",
    "            base_data_path = \"/content/\"\n",
    "            base_output_path = \"/content/output/\"\n",
    "            environment_name = \"colab\"\n",
    "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None:\n",
    "            print(\"‚úÖ Environment detected: Kaggle Notebooks.\")\n",
    "            base_data_path = \"/kaggle/input/\"\n",
    "            base_output_path = \"/kaggle/working/\"\n",
    "            environment_name = \"kaggle\"\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Environment detected: Local or Unknown.\")\n",
    "            base_data_path = \"./data/\"\n",
    "            base_output_path = \"./output/\"\n",
    "            environment_name = \"local\"\n",
    "    except NameError:\n",
    "        print(\n",
    "            \"‚ö†Ô∏è Running in a non-interactive Python session. Using default local paths.\"\n",
    "        )\n",
    "        base_data_path = \"./data/\"\n",
    "        base_output_path = \"./output/\"\n",
    "        environment_name = \"local\"\n",
    "\n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    print(f\"üìÅ Data Path: {base_data_path}\")\n",
    "    print(f\"üì¶ Output Path: {base_output_path}\")\n",
    "\n",
    "    return base_data_path, base_output_path, environment_name\n",
    "\n",
    "\n",
    "def auto_unzip_colab_content(target_dir=\"/content/\", zip_extension=\"*.zip\"):\n",
    "    \"\"\"Automatically detects and unzips files in the specified directory.\"\"\"\n",
    "    if \"google.colab\" not in str(get_ipython()):\n",
    "        print(\"Kh√¥ng ph·∫£i m√¥i tr∆∞·ªùng Colab. B·ªè qua Unzip t·ª± ƒë·ªông.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîé Scanning directory: {target_dir} for {zip_extension} files...\")\n",
    "    zip_files = glob.glob(os.path.join(target_dir, zip_extension))\n",
    "    unzipped_count = 0\n",
    "\n",
    "    for zip_path in zip_files:\n",
    "        file_name = os.path.basename(zip_path)\n",
    "        base_name, _ = os.path.splitext(file_name)\n",
    "        expected_output_path = os.path.join(target_dir, base_name)\n",
    "\n",
    "        if (\n",
    "            os.path.exists(expected_output_path)\n",
    "            and os.path.isdir(expected_output_path)\n",
    "            and os.listdir(expected_output_path)\n",
    "        ):\n",
    "            print(f\"‚û°Ô∏è Skipping '{file_name}'. Directory already exists.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"üîì Unzipping: {file_name}...\")\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(target_dir)\n",
    "            unzipped_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error unzipping {file_name}: {e}\")\n",
    "\n",
    "    if unzipped_count == 0:\n",
    "        print(\"\\n‚ú® All found zip files were already extracted or none were found.\")\n",
    "\n",
    "\n",
    "DATA_PATH, OUTPUT_PATH, ENV = configure_environment_paths()\n",
    "auto_unzip_colab_content(DATA_PATH)\n",
    "\n",
    "print(\"--- Python Version ---\")\n",
    "print(sys.version)\n",
    "\n",
    "# 2. PyTorch Version\n",
    "print(\"\\n--- PyTorch Version ---\")\n",
    "print(torch.__version__)\n",
    "\n",
    "# 3. PyTorch's Linked CUDA Version\n",
    "print(\"\\n--- PyTorch CUDA Version ---\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Enable a GPU accelerator in your notebook.\")\n",
    "\n",
    "# 4. System CUDA Driver/Toolkit Info (Requires GPU enabled)\n",
    "print(\"\\n--- System nvidia-smi (Requires GPU) ---\")\n",
    "!nvidia-smi\n",
    "\n",
    "\n",
    "# Try to login to wandb if key is available, otherwise user will be prompted later\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    wandb_key = userdata.get(\"WANDB_API_KEY\")\n",
    "    if wandb_key:\n",
    "        wandb.login(key=wandb_key)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:46.957627Z",
     "iopub.status.busy": "2025-12-11T16:12:46.957262Z",
     "iopub.status.idle": "2025-12-11T16:12:46.965351Z",
     "shell.execute_reply": "2025-12-11T16:12:46.964694Z",
     "shell.execute_reply.started": "2025-12-11T16:12:46.957600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppress specific warnings (clean output)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*local_dir_use_symlinks.*\", category=UserWarning\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*UnsupportedFieldAttributeWarning.*\", category=UserWarning\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*Unable to register cuFFT.*\", category=UserWarning\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*Unable to register cuDNN.*\", category=UserWarning\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*Unable to register cuBLAS.*\", category=UserWarning\n",
    ")\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:46.966545Z",
     "iopub.status.busy": "2025-12-11T16:12:46.966250Z",
     "iopub.status.idle": "2025-12-11T16:12:47.302920Z",
     "shell.execute_reply": "2025-12-11T16:12:47.302017Z",
     "shell.execute_reply.started": "2025-12-11T16:12:46.966522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after 12 loops: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "# SIMULATION SIMPLE LOOPED TRANSFORMER (CH·ªà D√ôNG ƒê·ªÇ MINH HO·∫†, C√ì TH·ªÇ B·ªé N·∫æU KH√îNG C·∫¶N)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LoopedTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_loops):\n",
    "        super().__init__()\n",
    "        # M·ªôt layer Transformer duy nh·∫•t (k=1)\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead\n",
    "        )\n",
    "        self.num_loops = num_loops  # L\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, d_model)\n",
    "        # L·∫∑p l·∫°i vi·ªác √°p d·ª•ng layer n√†y L l·∫ßn\n",
    "        # ƒê√¢y ch√≠nh l√† (1 x L) looped model\n",
    "        for _ in range(self.num_loops):\n",
    "            x = self.transformer_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "loops = 12  # L = 12\n",
    "model = LoopedTransformerBlock(d_model, nhead, loops)\n",
    "\n",
    "input_tensor = torch.randn(10, 32, 512)  # (Seq, Batch, Dim)\n",
    "output = model(input_tensor)\n",
    "print(f\"Output shape after {loops} loops: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:47.304123Z",
     "iopub.status.busy": "2025-12-11T16:12:47.303838Z",
     "iopub.status.idle": "2025-12-11T16:12:47.310090Z",
     "shell.execute_reply": "2025-12-11T16:12:47.309441Z",
     "shell.execute_reply.started": "2025-12-11T16:12:47.304096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SIMULATION LOOPED-INSPIRED REGULARIZATION (CH·ªà D√ôNG ƒê·ªÇ MINH HO·∫†, C√ì TH·ªÇ B·ªé N·∫æU KH√îNG C·∫¶N)\n",
    "def cosine_regularization_loss(model, k_block_size):\n",
    "    \"\"\"\n",
    "    T√≠nh to√°n loss ƒëi·ªÅu chu·∫©n d·ª±a tr√™n Cosine Similarity gi·ªØa c√°c block.\n",
    "    Gi·∫£ s·ª≠ model.layers l√† m·ªôt nn.ModuleList ch·ª©a c√°c Transformer Layers.\n",
    "    \"\"\"\n",
    "    layers = model.layers\n",
    "    L = len(layers)\n",
    "    reg_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # Duy·ªát qua c√°c block k√≠ch th∆∞·ªõc k\n",
    "    # So s√°nh weights c·ªßa block i v√† block i+1\n",
    "    for i in range(0, L - k_block_size, k_block_size):\n",
    "        current_block = layers[i : i + k_block_size]\n",
    "        next_block = layers[i + k_block_size : i + 2 * k_block_size]\n",
    "\n",
    "        # T√≠nh cosine similarity cho t·ª´ng c·∫∑p layer t∆∞∆°ng ·ª©ng trong 2 block\n",
    "        for l1, l2 in zip(current_block, next_block):\n",
    "            # L·∫•y vector tham s·ªë (v√≠ d·ª•: weight c·ªßa self-attention)\n",
    "            w1 = l1.self_attn.in_proj_weight.view(-1)\n",
    "            w2 = l2.self_attn.in_proj_weight.view(-1)\n",
    "\n",
    "            # Cosine similarity c√†ng cao c√†ng t·ªët -> Loss l√† 1 - cosine\n",
    "            similarity = F.cosine_similarity(w1.unsqueeze(0), w2.unsqueeze(0))\n",
    "            reg_loss += 1.0 - similarity\n",
    "            count += 1\n",
    "\n",
    "    return reg_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "# Khi training:\n",
    "# total_loss = cross_entropy_loss + lambda_reg * cosine_regularization_loss(model, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:47.312469Z",
     "iopub.status.busy": "2025-12-11T16:12:47.312180Z",
     "iopub.status.idle": "2025-12-11T16:12:47.338420Z",
     "shell.execute_reply": "2025-12-11T16:12:47.337803Z",
     "shell.execute_reply.started": "2025-12-11T16:12:47.312453Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# I. C√ÅC H√ÄM H·ªñ TR·ª¢ T·∫†O D·ªÆ LI·ªÜU\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "def create_test_datasets(config):\n",
    "    \"\"\"\n",
    "    Generates HARDER algorithmic test data to differentiate UT steps performance.\n",
    "    \"\"\"\n",
    "    test_data = {}\n",
    "\n",
    "    # 1. N-ARY ADDITION (Increased difficulty)\n",
    "    if \"n_ary\" in config:\n",
    "        n_ary_data = []\n",
    "        # Use config, but default to harder levels if not specified\n",
    "        ops_levels = config[\"n_ary\"].get(\"ops_levels\", [8, 16, 24])\n",
    "        num_samples = config[\"n_ary\"].get(\"num_samples_per_level\", 30)\n",
    "\n",
    "        for n in ops_levels:\n",
    "            for _ in range(num_samples):\n",
    "                # Force \"Hard\" numbers (high probability of carries)\n",
    "                nums = [random.randint(55, 999) for _ in range(n)]\n",
    "                prompt_str = str(nums)\n",
    "                target_str = str(sum(nums))\n",
    "\n",
    "                n_ary_data.append(\n",
    "                    {\n",
    "                        \"prompt\": prompt_str,\n",
    "                        \"expected_answer\": target_str,\n",
    "                        \"difficulty\": f\"{n}_ops\",\n",
    "                        \"task_type\": \"n_ary\",  # Explicitly add task type here for safety\n",
    "                    }\n",
    "                )\n",
    "        test_data[\"n_ary\"] = n_ary_data\n",
    "\n",
    "    # 2. P-HOP INDUCTION (The Shuffle Fix)\n",
    "    if \"p_hop\" in config:\n",
    "        p_hop_data = []\n",
    "        all_letters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "        hop_levels = config[\"p_hop\"].get(\"hop_levels\", [4, 8, 12])\n",
    "        num_samples = config[\"p_hop\"].get(\"num_samples_per_level\", 30)\n",
    "\n",
    "        for hops in hop_levels:\n",
    "            for _ in range(num_samples):\n",
    "                if hops + 1 > len(all_letters):\n",
    "                    continue\n",
    "\n",
    "                nodes = random.sample(all_letters, hops + 1)\n",
    "                # Create the chain\n",
    "                facts = [f\"{nodes[i]}->{nodes[i + 1]}\" for i in range(len(nodes) - 1)]\n",
    "\n",
    "                # CRITICAL: Shuffle the facts so the model can't just read forward\n",
    "                random.shuffle(facts)\n",
    "\n",
    "                facts_str = \", \".join(facts)\n",
    "                start_node = nodes[0]\n",
    "                end_node = nodes[-1]\n",
    "                full_prompt = (\n",
    "                    f\"Facts: {facts_str}. Start: {start_node}. Find: {end_node}.\"\n",
    "                )\n",
    "\n",
    "                p_hop_data.append(\n",
    "                    {\n",
    "                        \"prompt\": full_prompt,\n",
    "                        \"expected_answer\": end_node,\n",
    "                        \"difficulty\": f\"{hops}_hops_shuffled\",\n",
    "                        \"task_type\": \"p_hop\",\n",
    "                    }\n",
    "                )\n",
    "        test_data[\"p_hop\"] = p_hop_data\n",
    "\n",
    "    # 3. iGSM (Added Distractors)\n",
    "    if \"igsm\" in config:\n",
    "        igsm_data = []\n",
    "        num_total_samples = config[\"igsm\"].get(\"num_samples_total\", 50)\n",
    "\n",
    "        names = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Evan\"]\n",
    "        items = [\"coins\", \"widgets\", \"gadgets\"]\n",
    "\n",
    "        for _ in range(num_total_samples):\n",
    "            # A longer, iterative problem (accumulation)\n",
    "            steps = random.randint(4, 7)\n",
    "            current_val = random.randint(10, 50)\n",
    "            history = [f\"{names[0]} starts with {current_val} {items[0]}.\"]\n",
    "\n",
    "            for i in range(steps):\n",
    "                op = random.choice([\"add\", \"sub\", \"mult\"])\n",
    "                val = random.randint(2, 5)\n",
    "\n",
    "                if op == \"add\":\n",
    "                    current_val += val\n",
    "                    history.append(f\"They find {val} more.\")\n",
    "                elif op == \"sub\":\n",
    "                    # Prevent negative\n",
    "                    val = min(val, current_val)\n",
    "                    current_val -= val\n",
    "                    history.append(f\"They lose {val}.\")\n",
    "                elif op == \"mult\":\n",
    "                    current_val *= val\n",
    "                    history.append(f\"The amount multiplies by {val}.\")\n",
    "\n",
    "            # Add a distractor sentence that implies no operation\n",
    "            history.insert(\n",
    "                random.randint(1, len(history) - 1), f\"{names[1]} watches them closely.\"\n",
    "            )\n",
    "\n",
    "            prompt = \" \".join(history) + \" How many do they have now?\"\n",
    "\n",
    "            igsm_data.append(\n",
    "                {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"expected_answer\": str(current_val),\n",
    "                    \"difficulty\": f\"{steps}_step_accumulation\",\n",
    "                    \"task_type\": \"igsm\",\n",
    "                }\n",
    "            )\n",
    "        test_data[\"igsm\"] = igsm_data\n",
    "\n",
    "    return test_data\n",
    "\n",
    "\n",
    "# --- H√ÄM M·ªöI ƒê·ªÇ T·∫¢I V√Ä TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU ---\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads data from a file (assumed to be a list of records) and organizes it\n",
    "    into the required {task_type: [list_of_dicts]} format.\n",
    "\n",
    "    Assumes the input file contains fields: 'prompt', 'expected_answer', 'task_type', 'difficulty'.\n",
    "    \"\"\"\n",
    "    print(f\"Loading existing data from: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # H·ªó tr·ª£ ƒë·ªãnh d·∫°ng JSON\n",
    "        if file_path.endswith(\".json\"):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_data = json.load(f)\n",
    "        # H·ªó tr·ª£ ƒë·ªãnh d·∫°ng CSV\n",
    "        elif file_path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "            raw_data = df.to_dict(\"records\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. H√£y d√πng .json ho·∫∑c .csv.\"\n",
    "            )\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i ƒë∆∞·ªùng d·∫´n: {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi t·∫£i ho·∫∑c ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Ti·ªÅn x·ª≠ l√Ω: G√°n d·ªØ li·ªáu v√†o dictionary theo task_type\n",
    "    processed_data = {\"n_ary\": [], \"p_hop\": [], \"igsm\": []}\n",
    "\n",
    "    # Y√™u c·∫ßu m·ªói record ph·∫£i c√≥ 'task_type' ƒë·ªÉ ph√¢n lo·∫°i\n",
    "    for record in raw_data:\n",
    "        task = record.get(\"task_type\")\n",
    "        if task in processed_data:\n",
    "            # ƒê·∫£m b·∫£o c√°c tr∆∞·ªùng b·∫Øt bu·ªôc t·ªìn t·∫°i\n",
    "            if (\n",
    "                \"prompt\" in record\n",
    "                and \"expected_answer\" in record\n",
    "                and \"difficulty\" in record\n",
    "            ):\n",
    "                processed_data[task].append(record)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"C·∫£nh b√°o: Record thi·∫øu tr∆∞·ªùng b·∫Øt bu·ªôc (prompt, target, difficulty): {record}\"\n",
    "                )\n",
    "\n",
    "    print(\n",
    "        f\"‚úÖ Data loaded. N-ary: {len(processed_data['n_ary'])}, P-Hop: {len(processed_data['p_hop'])}, iGSM: {len(processed_data['igsm'])}\"\n",
    "    )\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def create_perplexity_data(num_samples=30):\n",
    "    \"\"\"\n",
    "    Generates raw text data for Perplexity (PPL) calculation,\n",
    "    mimicking the expected reasoning traces.\n",
    "    \"\"\"\n",
    "    perplexity_texts = []\n",
    "\n",
    "    # 1. N-ARY ADDITION TRACES\n",
    "    for _ in range(num_samples // 2):\n",
    "        n = random.choice([4, 6, 8])\n",
    "        nums = [random.randint(10, 99) for _ in range(n)]\n",
    "        start_trace = f\"System: You are a mechanical calculation engine. Output the accumulation steps strictly.\\nUser: Sum: {str(nums)}\\nAssistant: Current Sum: 0\\n\"\n",
    "        current_sum = 0\n",
    "        reasoning_steps = []\n",
    "        for num in nums:\n",
    "            prev_sum = current_sum\n",
    "            current_sum += num\n",
    "            reasoning_steps.append(\n",
    "                f\"Add {num}: {prev_sum} + {num} = {current_sum}\\nCurrent Sum: {current_sum}\"\n",
    "            )\n",
    "        reasoning_str = \"\\n\".join(reasoning_steps)\n",
    "        final_trace = f\"\\nFinal: {current_sum}\"\n",
    "        full_text = start_trace + reasoning_str + final_trace\n",
    "        perplexity_texts.append(full_text)\n",
    "\n",
    "    # 2. P-HOP INDUCTION TRACES\n",
    "    all_letters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "    for _ in range(num_samples // 2):\n",
    "        hops = random.choice([3, 4, 5])\n",
    "        nodes = random.sample(all_letters, hops + 1)\n",
    "        facts = [f\"{nodes[i]}->{nodes[i + 1]}\" for i in range(len(nodes) - 1)]\n",
    "        facts_str = \", \".join(facts)\n",
    "        start_node = nodes[0]\n",
    "        end_node = nodes[-1]\n",
    "        full_prompt = f\"Facts: {facts_str}. Start: {start_node}. Find: {end_node}.\"\n",
    "        start_trace = f\"System: You are a logic engine. Trace the chain of implications step-by-step.\\nUser: {full_prompt}\\nAssistant: Current Node: {start_node}\\n\"\n",
    "        reasoning_steps = []\n",
    "        current_node = start_node\n",
    "        for i in range(hops):\n",
    "            next_node = nodes[i + 1]\n",
    "            reasoning_steps.append(\n",
    "                f\"Rule Matches: {current_node} -> {next_node}\\nNext Node: {next_node}\\nCurrent Node: {next_node}\"\n",
    "            )\n",
    "            current_node = next_node\n",
    "        reasoning_str = \"\\n\".join(reasoning_steps[:-1])\n",
    "        final_trace = f\"\\nFinal: {end_node}\"\n",
    "        full_text = start_trace + reasoning_str + final_trace\n",
    "        perplexity_texts.append(full_text)\n",
    "\n",
    "    return perplexity_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:47.339694Z",
     "iopub.status.busy": "2025-12-11T16:12:47.339426Z",
     "iopub.status.idle": "2025-12-11T16:12:47.371868Z",
     "shell.execute_reply": "2025-12-11T16:12:47.371075Z",
     "shell.execute_reply.started": "2025-12-11T16:12:47.339679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# II. CLASS TH·ª∞C HI·ªÜN TH√ç NGHI·ªÜM\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "class OuroThinkingExperiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        dtype=torch.float16,\n",
    "        use_4bit_quant=False,\n",
    "        use_torch_compile=False,\n",
    "    ):\n",
    "        self.model_path = model_path\n",
    "        self.dtype = dtype\n",
    "        self.use_4bit_quant = use_4bit_quant\n",
    "        self.use_torch_compile = use_torch_compile\n",
    "        self.tokenizer = None  # S·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o trong load_model_with_ut_steps\n",
    "\n",
    "    def load_model_with_ut_steps(self, total_ut_steps, early_exit_threshold):\n",
    "        \"\"\"Load model with specific UT steps configuration\"\"\"\n",
    "        # 1. Chu·∫©n b·ªã Quantization Configuration (L∆∞·ª£ng t·ª≠ h√≥a)\n",
    "        quantization_config = None\n",
    "        if self.use_4bit_quant:\n",
    "            print(\" -> √Åp d·ª•ng L∆∞·ª£ng t·ª≠ h√≥a 4-bit (gi·∫£m VRAM & tƒÉng t·ªëc ƒë·ªô)\")\n",
    "            # Y√™u c·∫ßu th∆∞ vi·ªán 'bitsandbytes'\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"Loading config for UT steps: {total_ut_steps}, Early Exit: {early_exit_threshold}\"\n",
    "        )\n",
    "        config = AutoConfig.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        config.total_ut_steps = total_ut_steps\n",
    "        config.early_exit_threshold = early_exit_threshold\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_path, trust_remote_code=True, padding_side=\"left\"\n",
    "        )\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"AutoTokenizer: padding_side: {tokenizer.padding_side}\")\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            config=config,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=\"sdpa_paged\",\n",
    "            dtype=self.dtype\n",
    "            if not self.use_4bit_quant\n",
    "            else None,  # N·∫øu d√πng 4bit, dtype ƒë∆∞·ª£c x√°c ƒë·ªãnh trong config\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=quantization_config if self.use_4bit_quant else None,\n",
    "        )\n",
    "        # 2. √Åp d·ª•ng PyTorch 2.0 Compile\n",
    "        if self.use_torch_compile:\n",
    "            print(\" -> Use torch.compile() for inference optimization\")\n",
    "            # L∆∞u √Ω: L·∫ßn ch·∫°y ƒë·∫ßu ti√™n c√≥ th·ªÉ ch·∫≠m do overhead bi√™n d·ªãch.\n",
    "            model = torch.compile(model)\n",
    "\n",
    "        print(\"Config loaded:\")\n",
    "        print(f\"   - total_ut_steps: {config.total_ut_steps}\")\n",
    "        print(f\"   - num_hidden_layers: {config.num_hidden_layers}\")\n",
    "        print(\n",
    "            f\"   - Total cache indices needed: {config.total_ut_steps * config.num_hidden_layers}\"\n",
    "        )\n",
    "\n",
    "        past_key_values = None\n",
    "        model.eval()\n",
    "        print(\n",
    "            f\"Model loaded! Using device: {model.device}, configured UT steps: {model.config.total_ut_steps}\"\n",
    "        )\n",
    "        return (\n",
    "            model,\n",
    "            tokenizer,\n",
    "            past_key_values,\n",
    "            {\n",
    "                \"total_ut_steps\": total_ut_steps,\n",
    "                \"early_exit_threshold\": early_exit_threshold,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def calculate_perplexity(\n",
    "        self, model, tokenizer, text_data, ut_steps, max_length=2048, stride=512\n",
    "    ):\n",
    "        \"\"\"Calculates Perplexity (PPL) using the sliding window method (Fixed).\"\"\"\n",
    "        device = model.device\n",
    "        model.eval()\n",
    "\n",
    "        # Handle empty data case\n",
    "        if not text_data or not text_data[0]:\n",
    "            print(\"‚ö†Ô∏è Warning: Text data for PPL is empty.\")\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "        text_data_concat = text_data[0]\n",
    "        encodings = tokenizer(\n",
    "            text_data_concat,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length * 2,\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        attention_mask = encodings.attention_mask.to(device)\n",
    "\n",
    "        if input_ids.size(1) < 2:\n",
    "            print(\"‚ö†Ô∏è Warning: Input text too short for PPL.\")\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Sliding window calculation\n",
    "        for i in tqdm(\n",
    "            range(0, input_ids.size(1), stride), desc=f\"Calculating PPL (UT={ut_steps})\"\n",
    "        ):\n",
    "            end_loc = min(i + max_length, input_ids.size(1))\n",
    "            input_slice = input_ids[:, i:end_loc]\n",
    "\n",
    "            # Prepare targets\n",
    "            target_slice = input_slice.clone()\n",
    "\n",
    "            # Optimized Masking Logic:\n",
    "            # If not the first window (i > 0), mask the context tokens (the overlap)\n",
    "            # We only want to evaluate the *new* tokens introduced by the stride\n",
    "            if i > 0:\n",
    "                # The length of the overlap context\n",
    "                context_len = input_slice.size(1) - stride\n",
    "                if context_len > 0:\n",
    "                    target_slice[:, :context_len] = -100\n",
    "\n",
    "            # Safety: Ensure we don't calculate loss if all targets are masked\n",
    "            if (target_slice != -100).sum() == 0:\n",
    "                continue\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                outputs = model(\n",
    "                    input_ids=input_slice,\n",
    "                    attention_mask=attention_mask[:, i:end_loc],\n",
    "                    labels=target_slice,\n",
    "                )\n",
    "\n",
    "            # Handle NaN loss from model (e.g. fp16 overflow)\n",
    "            if torch.isnan(outputs.loss):\n",
    "                continue\n",
    "\n",
    "            # Standard HF PPL reduction involves multiplying back by the count\n",
    "            # outputs.loss is the mean loss of the batch\n",
    "            num_valid_tokens = (target_slice != -100).sum().item()\n",
    "            neg_log_likelihood = outputs.loss * num_valid_tokens\n",
    "\n",
    "            total_loss += neg_log_likelihood.item()\n",
    "            total_tokens += num_valid_tokens\n",
    "\n",
    "        # Avoid Division by Zero\n",
    "        if total_tokens == 0:\n",
    "            print(\"Total Tokens are 0!\")\n",
    "            return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "        return perplexity, avg_loss\n",
    "\n",
    "    def _build_task_templates(self, tokenizer):\n",
    "        \"\"\"Pre-compute and tokenize the static parts of prompts for SPEED.\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_templates = {}\n",
    "\n",
    "        task_configs = {\n",
    "            \"n_ary\": {\n",
    "                \"system\": \"You are a mechanical calculation engine. Output the accumulation steps strictly.\",\n",
    "                \"example_user\": \"Sum: [10, 20, 30]\",\n",
    "                \"example_asst\": \"Current Sum: 0\\nAdd 10: 0 + 10 = 10\\nCurrent Sum: 10\\nAdd 20: 10 + 20 = 30\\nCurrent Sum: 30\\nAdd 30: 30 + 30 = 60\\nFinal: 60\",\n",
    "                \"force_start\": \"Current Sum: 0\",\n",
    "                \"input_prefix\": \"Sum: \",\n",
    "            },\n",
    "            \"p_hop\": {\n",
    "                \"system\": \"You are a logic engine. Trace the chain of implications step-by-step.\",\n",
    "                \"example_user\": \"Facts: A->B, B->C, C->D. Start: A. Find: D.\",\n",
    "                \"example_asst\": \"Current Node: A\\nRule Matches: A -> B\\nNext Node: B\\nCurrent Node: B\\nRule Matches: B -> C\\nNext Node: C\\nCurrent Node: C\\nRule Matches: C -> D\\nNext Node: D\\nFinal: D\",\n",
    "                \"force_start\": \"Current Node:\",\n",
    "                \"input_prefix\": \"Trace: \",\n",
    "            },\n",
    "            \"igsm\": {\n",
    "                \"system\": \"You are a precise math solver. Extract variables first, then solve step-by-step.\",\n",
    "                \"example_user\": \"Question: Bob bought 3 packs of 5 apples. He ate 2. How many left?\",\n",
    "                \"example_asst\": \"Goal: Find remaining apples\\nVariables: Packs=3, ApplesPerPack=5, Eaten=2\\nStep 1: Calculate total apples bought. 3 * 5 = 15.\\nStep 2: Subtract eaten apples. 15 - 2 = 13.\\nFinal: 13\",\n",
    "                \"force_start\": \"Goal:\",\n",
    "                \"input_prefix\": \"Question: \",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        for task_type, config in task_configs.items():\n",
    "            # 1. Build the static message history (system + example)\n",
    "            static_messages = [\n",
    "                {\"role\": \"system\", \"content\": config[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": config[\"example_user\"]},\n",
    "                {\"role\": \"assistant\", \"content\": config[\"example_asst\"]},\n",
    "            ]\n",
    "            # Apply chat template to the static part\n",
    "            static_prompt_text = tokenizer.apply_chat_template(\n",
    "                static_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            # Tokenize the static part ONCE\n",
    "            static_inputs = tokenizer(static_prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "            # 2. Tokenize the force_start prefix that the model must begin with\n",
    "            force_start_tokens = tokenizer(\n",
    "                config[\"force_start\"], return_tensors=\"pt\", add_special_tokens=False\n",
    "            )\n",
    "\n",
    "            self.task_templates[task_type] = {\n",
    "                \"static_input_ids\": static_inputs.input_ids,\n",
    "                \"static_attention_mask\": static_inputs.attention_mask,\n",
    "                \"force_start_ids\": force_start_tokens.input_ids,\n",
    "                \"input_prefix\": config[\"input_prefix\"],\n",
    "                \"force_start_text\": config[\"force_start\"],\n",
    "            }\n",
    "        print(\"[+] Pre-computed task templates for faster inference.\")\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict_with_metrics(\n",
    "        self, prompt, task_type, model, tokenizer, ut_steps, cache\n",
    "    ):\n",
    "        \"\"\"Enhanced prediction for Accuracy, Inference Time, and Token Count.\"\"\"\n",
    "\n",
    "        # --- Task-specific templates ---\n",
    "        # (Gi·ªØ nguy√™n ph·∫ßn templates c·ªßa b·∫°n)\n",
    "        if task_type == \"n_ary\":\n",
    "            system_msg = \"You are a mechanical calculation engine. Output the accumulation steps strictly.\"\n",
    "            example_user = \"Sum: [10, 20, 30]\"\n",
    "            example_assistant = \"Current Sum: 0\\nAdd 10: 0 + 10 = 10\\nCurrent Sum: 10\\nAdd 20: 10 + 20 = 30\\nCurrent Sum: 30\\nAdd 30: 30 + 30 = 60\\nFinal: 60\"\n",
    "            force_start = \"Current Sum: 0\"\n",
    "            user_input = f\"Sum: {prompt}\"\n",
    "        elif task_type == \"p_hop\":\n",
    "            system_msg = (\n",
    "                \"You are a logic engine. Trace the chain of implications step-by-step.\"\n",
    "            )\n",
    "            example_user = \"Facts: A->B, B->C, C->D. Start: A. Find: D.\"\n",
    "            example_assistant = \"Current Node: A\\nRule Matches: A -> B\\nNext Node: B\\nCurrent Node: B\\nRule Matches: B -> C\\nNext Node: C\\nCurrent Node: C\\nRule Matches: C -> D\\nNext Node: D\\nFinal: D\"\n",
    "            force_start = \"Current Node:\"\n",
    "            user_input = f\"Trace: {prompt}\"\n",
    "        elif task_type == \"igsm\":\n",
    "            system_msg = \"You are a precise math solver. Extract variables first, then solve step-by-step.\"\n",
    "            example_user = (\n",
    "                \"Question: Bob bought 3 packs of 5 apples. He ate 2. How many left?\"\n",
    "            )\n",
    "            example_assistant = \"Goal: Find remaining apples\\nVariables: Packs=3, ApplesPerPack=5, Eaten=2\\nStep 1: Calculate total apples bought. 3 * 5 = 15.\\nStep 2: Subtract eaten apples. 15 - 2 = 13.\\nFinal: 13\"\n",
    "            force_start = \"Goal:\"\n",
    "            user_input = f\"Question: {prompt}\"\n",
    "\n",
    "        # Construct messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": example_user},\n",
    "            {\"role\": \"assistant\", \"content\": example_assistant},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "\n",
    "        # Tokenize and generate\n",
    "        input_text = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        final_input_text = input_text + force_start\n",
    "        inputs = tokenizer(final_input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Measure generation time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Now generate normally\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        # Parse results and calculate generated tokens\n",
    "        prompt_length = inputs.input_ids.shape[1]\n",
    "        generated_tokens = outputs.sequences[0][prompt_length:]\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        full_response = force_start + response\n",
    "        generated_tokens_count = len(generated_tokens)\n",
    "\n",
    "        # Extract prediction\n",
    "        if task_type == \"p_hop\":\n",
    "            match = re.search(r\"Final:\\s*(\\w+)\", full_response)\n",
    "            pred = match.group(1) if match else \"Error\"\n",
    "        else:\n",
    "            match = re.search(r\"Final:\\s*(\\d+)\", full_response)\n",
    "            pred = match.group(1) if match else \"0\"\n",
    "\n",
    "        return {\n",
    "            \"full_response\": full_response,\n",
    "            \"prediction\": pred,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"generated_tokens\": generated_tokens_count,  # ƒê√É TH√äM\n",
    "            \"ut_steps\": ut_steps,\n",
    "        }\n",
    "\n",
    "    def predict_with_metrics_optimized(\n",
    "        self, user_input, task_type, model, tokenizer, ut_steps, generation_config=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        OPTIMIZED PREDICTION: Uses pre-computed templates, no external cache.\n",
    "\n",
    "        This is 4-10x faster than the original and ensures the Ouro model's\n",
    "        internal loop mechanism works correctly.\n",
    "        \"\"\"\n",
    "        # Ensure templates are built\n",
    "        if not hasattr(self, \"task_templates\") or task_type not in self.task_templates:\n",
    "            self._build_task_templates(tokenizer)\n",
    "\n",
    "        template = self.task_templates[task_type]\n",
    "        device = model.device\n",
    "\n",
    "        # 1. CONSTRUCT INPUT FROM PRE-COMPUTED TEMPLATES\n",
    "        # Static context (system + example)\n",
    "        input_ids = template[\"static_input_ids\"].to(device)\n",
    "        attention_mask = template[\"static_attention_mask\"].to(device)\n",
    "\n",
    "        # User input tokens\n",
    "        user_query = template[\"input_prefix\"] + user_input\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        user_tokens = tokenizer(\n",
    "            user_query, padding=True, return_tensors=\"pt\", add_special_tokens=False\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        # Force-start tokens (what model must begin generating)\n",
    "        force_start_ids = template[\"force_start_ids\"].to(device)\n",
    "\n",
    "        # Concatenate: [static_context, user_input, force_start]\n",
    "        input_ids = torch.cat([input_ids, user_tokens, force_start_ids], dim=1)\n",
    "        attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "        # 2. GENERATION (DETERMINISTIC & FAST)\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # CRITICAL: DO NOT pass cache parameter - let Ouro manage its own\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,  # Internal cache only\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,  # Faster without scores\n",
    "            **(generation_config or {}),\n",
    "        )\n",
    "\n",
    "        generation_time = time.perf_counter() - start_time\n",
    "\n",
    "        # 3. DECODE & PARSE RESULTS\n",
    "        prompt_length = input_ids.shape[1]\n",
    "        generated_ids = outputs.sequences[0, prompt_length:]\n",
    "        generated_tokens_count = generated_ids.shape[0]\n",
    "\n",
    "        # Decode the full response\n",
    "        force_start_text = template[\"force_start_text\"]\n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        full_response = force_start_text + generated_text\n",
    "\n",
    "        # 4. ROBUST ANSWER EXTRACTION\n",
    "        pred = self._extract_final_answer(full_response, task_type)\n",
    "\n",
    "        return {\n",
    "            \"full_response\": full_response,\n",
    "            \"prediction\": pred,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"generated_tokens\": generated_tokens_count,\n",
    "            \"input_tokens\": input_ids.shape[1],\n",
    "            \"ut_steps\": ut_steps,\n",
    "        }\n",
    "\n",
    "    def _extract_final_answer(self, full_response, task_type):\n",
    "        \"\"\"\n",
    "        Robust answer extraction with multiple fallback patterns.\n",
    "        \"\"\"\n",
    "        pred = \"0\"  # Default fallback\n",
    "\n",
    "        try:\n",
    "            if task_type == \"p_hop\":\n",
    "                # Multiple patterns for logic tasks\n",
    "                patterns = [\n",
    "                    r\"Final\\s*:\\s*(\\w+)\",  # \"Final: D\"\n",
    "                    r\"Next Node\\s*:\\s*(\\w+)$\",  # Last \"Next Node: X\"\n",
    "                    r\"Answer\\s*:\\s*(\\w+)\",  # \"Answer: D\"\n",
    "                    r\"Result\\s*:\\s*(\\w+)\",  # \"Result: D\"\n",
    "                ]\n",
    "                for pattern in patterns:\n",
    "                    match = re.search(pattern, full_response, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        pred = match.group(1).strip()\n",
    "                        break\n",
    "                else:\n",
    "                    pred = \"Error\"\n",
    "            else:\n",
    "                # For numeric tasks, find the last number after Final/Answer\n",
    "                patterns = [\n",
    "                    r\"Final\\s*:\\s*([-+]?\\d*\\.?\\d+)\",  # \"Final: 123\"\n",
    "                    r\"Answer\\s*:\\s*([-+]?\\d*\\.?\\d+)\",  # \"Answer: 123\"\n",
    "                    r\"Result\\s*:\\s*([-+]?\\d*\\.?\\d+)\",  # \"Result: 123\"\n",
    "                    r\"=\\s*([-+]?\\d*\\.?\\d+)$\",  # \"= 123\" at end\n",
    "                ]\n",
    "                all_matches = []\n",
    "                for pattern in patterns:\n",
    "                    matches = re.findall(pattern, full_response, re.IGNORECASE)\n",
    "                    all_matches.extend(matches)\n",
    "\n",
    "                if all_matches:\n",
    "                    # Take the last match (most likely the final answer)\n",
    "                    pred = all_matches[-1]\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Parsing error: {e}\")\n",
    "            pred = \"ParseError\"\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:47.372824Z",
     "iopub.status.busy": "2025-12-11T16:12:47.372621Z",
     "iopub.status.idle": "2025-12-11T16:12:47.395117Z",
     "shell.execute_reply": "2025-12-11T16:12:47.394340Z",
     "shell.execute_reply.started": "2025-12-11T16:12:47.372808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_test_id(task_type, difficulty, prompt):\n",
    "    \"\"\"\n",
    "    Generate a unique test ID (same as original).\n",
    "    \"\"\"\n",
    "    import hashlib\n",
    "\n",
    "    unique_str = f\"{task_type}_{difficulty}_{prompt}\"\n",
    "    return hashlib.md5(unique_str.encode()).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:47.396557Z",
     "iopub.status.busy": "2025-12-11T16:12:47.396165Z",
     "iopub.status.idle": "2025-12-11T16:12:47.435994Z",
     "shell.execute_reply": "2025-12-11T16:12:47.435367Z",
     "shell.execute_reply.started": "2025-12-11T16:12:47.396535Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Main Experiment Function with W&B Integration\n",
    "# ====================================================================\n",
    "# III. H√ÄM CH·∫†Y TH√ç NGHI·ªÜM CH√çNH (UPDATED)\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "def run_experiment_with_config(config: dict):\n",
    "    \"\"\"\n",
    "    Runs the full experiment comparing Accuracy, Inference Time, and Perplexity.\n",
    "    Integrated with Weights & Biases for tracking.\n",
    "    \"\"\"\n",
    "    # 1. Setup W&B\n",
    "    use_wandb = config.get(\"WANDB\", {}).get(\"enabled\", False)\n",
    "    if use_wandb:\n",
    "        run = wandb.init(\n",
    "            project=config[\"WANDB\"].get(\"project\", \"ouro-looped-transformer\"),\n",
    "            entity=config[\"WANDB\"].get(\"entity\", None),\n",
    "            name=config[\"WANDB\"].get(\"run_name\", f\"run_{int(time.time())}\"),\n",
    "            config=config,  # Automatically logs the config dictionary\n",
    "        )\n",
    "\n",
    "    # 2. Extract params\n",
    "    model_path = config[\"MODEL\"][\"path\"]\n",
    "    dtype = config[\"MODEL\"][\"dtype\"]\n",
    "    ut_steps_to_test = config[\"INFERENCE_STEPS\"]\n",
    "    data_config = config[\"DATA\"]\n",
    "    eval_settings = config[\"EVAL_SETTINGS\"]\n",
    "    calculate_ppl = eval_settings[\"calculate_perplexity\"]\n",
    "    early_exit_threshold = eval_settings[\"early_exit_threshold\"]\n",
    "\n",
    "    experiment = OuroThinkingExperiment(\n",
    "        model_path,\n",
    "        dtype,\n",
    "        use_4bit_quant=config[\"MODEL\"].get(\"use_4bit_quant\", False),\n",
    "        use_torch_compile=config[\"MODEL\"].get(\"use_torch_compile\", False),\n",
    "    )\n",
    "\n",
    "    # 3. Prepare Data\n",
    "    if data_config[\"load_existing\"]:\n",
    "        test_datasets = load_and_preprocess_data(data_config[\"data_file_path\"])\n",
    "    else:\n",
    "        print(\"Generating new test datasets...\")\n",
    "        test_datasets = create_test_datasets(data_config)\n",
    "\n",
    "    perplexity_results = []\n",
    "    perplexity_data_for_eval = []\n",
    "    if calculate_ppl:\n",
    "        ppl_num_samples = eval_settings[\"ppl_num_samples\"]\n",
    "        # Fix bug: Create data once to ensure consistency across steps\n",
    "        perplexity_data_raw = create_perplexity_data(num_samples=ppl_num_samples)\n",
    "        # Combine into a single string for sliding window PPL\n",
    "        perplexity_data_for_eval = [\"\\n\\n\".join(perplexity_data_raw)]\n",
    "        print(f\"Perplexity Dataset size: {len(perplexity_data_raw)} texts.\")\n",
    "\n",
    "    all_accuracy_time_results = []\n",
    "\n",
    "    # 4. Main Loop\n",
    "    for ut_steps in ut_steps_to_test:\n",
    "        print(\n",
    "            f\"\\n{'=' * 60}\\n--- STARTING EXPERIMENT WITH {ut_steps} UT STEPS ---\\n{'=' * 60}\"\n",
    "        )\n",
    "\n",
    "        model, tokenizer, cache, ut_config = experiment.load_model_with_ut_steps(\n",
    "            ut_steps, early_exit_threshold\n",
    "        )\n",
    "\n",
    "        # --- PART 1: PERPLEXITY ---\n",
    "        if calculate_ppl and perplexity_data_for_eval:\n",
    "            ppl, avg_loss = experiment.calculate_perplexity(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                perplexity_data_for_eval,\n",
    "                ut_steps,\n",
    "                max_length=eval_settings[\"ppl_max_length\"],\n",
    "                stride=eval_settings[\"ppl_stride\"],\n",
    "            )\n",
    "            perplexity_results.append(\n",
    "                {\"ut_steps\": ut_steps, \"perplexity\": ppl, \"avg_loss\": avg_loss}\n",
    "            )\n",
    "            print(f\"‚úÖ PPL Result (UT={ut_steps}): PPL={ppl:.4f}, Loss={avg_loss:.4f}\")\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\"perplexity\": ppl, \"val_loss\": avg_loss, \"ut_steps\": ut_steps}\n",
    "                )\n",
    "\n",
    "        # --- PART 2: ACCURACY & TIME ---\n",
    "        for task_type, items in test_datasets.items():\n",
    "            print(f\"\\n--- Running Task: {task_type} ({len(items)} samples) ---\")\n",
    "            task_results = []\n",
    "\n",
    "            # Create a W&B Table for this task/step to inspect predictions\n",
    "            if use_wandb:\n",
    "                table = wandb.Table(\n",
    "                    columns=[\n",
    "                        \"input\",\n",
    "                        \"target\",\n",
    "                        \"prediction\",\n",
    "                        \"is_correct\",\n",
    "                        \"tokens\",\n",
    "                        \"time\",\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            for item in tqdm(items, desc=f\"Task {task_type}\"):\n",
    "                prompt = item[\"prompt\"]\n",
    "                target = item[\"expected_answer\"]\n",
    "\n",
    "                result = experiment.predict_with_metrics(\n",
    "                    prompt=prompt,\n",
    "                    task_type=task_type,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    cache=cache,\n",
    "                    ut_steps=ut_config[\"total_ut_steps\"],\n",
    "                )\n",
    "\n",
    "                is_correct = str(result[\"prediction\"]).strip() == str(target).strip()\n",
    "\n",
    "                result_entry = {\n",
    "                    \"task_type\": task_type,\n",
    "                    \"difficulty\": item[\"difficulty\"],\n",
    "                    \"test_input\": prompt,\n",
    "                    \"expected_answer\": target,\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"test_id\": generate_test_id(task_type, item[\"difficulty\"], prompt),\n",
    "                    **result,  # Merges generated tokens, time, etc.\n",
    "                }\n",
    "\n",
    "                task_results.append(result_entry)\n",
    "                all_accuracy_time_results.append(result_entry)\n",
    "\n",
    "                if use_wandb:\n",
    "                    table.add_data(\n",
    "                        prompt[:100] + \"...\",\n",
    "                        str(target),\n",
    "                        str(result[\"prediction\"]),\n",
    "                        is_correct,\n",
    "                        result[\"generated_tokens\"],\n",
    "                        result[\"generation_time\"],\n",
    "                    )\n",
    "\n",
    "            # Calculate Task Metrics\n",
    "            accuracy = (\n",
    "                sum(r[\"is_correct\"] for r in task_results) / len(task_results)\n",
    "                if task_results\n",
    "                else 0\n",
    "            )\n",
    "            avg_time = (\n",
    "                sum(r[\"generation_time\"] for r in task_results) / len(task_results)\n",
    "                if task_results\n",
    "                else 0\n",
    "            )\n",
    "            avg_tokens = (\n",
    "                sum(r[\"generated_tokens\"] for r in task_results) / len(task_results)\n",
    "                if task_results\n",
    "                else 0\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"üìä {task_type} (UT={ut_steps}): Acc={accuracy:.2%}, Time={avg_time:.2f}s, Tokens={avg_tokens:.1f}\"\n",
    "            )\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        f\"{task_type}/accuracy\": accuracy,\n",
    "                        f\"{task_type}/avg_time\": avg_time,\n",
    "                        f\"{task_type}/avg_tokens\": avg_tokens,\n",
    "                        f\"{task_type}/predictions_table\": table,\n",
    "                        \"ut_steps\": ut_steps,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    return all_accuracy_time_results, perplexity_results\n",
    "\n",
    "\n",
    "# RUN EXPERIMENT FUNCTION WITH CACHE WITH OPTIMIZATION\n",
    "def run_robust_experiment_with_config(config: dict):\n",
    "    \"\"\"\n",
    "    RUN ROBUST EXPERIMENT: Optimized for speed, determinism, and accurate UT step comparison.\n",
    "\n",
    "    Key Improvements:\n",
    "    1. ‚úÖ Deterministic generation (no external cache interference)\n",
    "    2. ‚úÖ Pre-computed prompt templates (4-10x speedup)\n",
    "    3. ‚úÖ Proper memory management (prevents OOM)\n",
    "    4. ‚úÖ Task-aware difficulty sampling for reliable metrics\n",
    "\n",
    "    Returns: (all_accuracy_time_results, perplexity_results)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. CHECK W&B SETUP BEFORE STARTING\n",
    "    use_wandb = config.get(\"WANDB\", {}).get(\"enabled\", False)\n",
    "    run = None\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb_config = config[\"WANDB\"]\n",
    "        print(f\"üîó Initializing W&B (timeout: {wandb_config.get('timeout', 30)}s)...\")\n",
    "\n",
    "        # Set environment variables to prevent hangs\n",
    "        os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"  # 5 minutes max\n",
    "        os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
    "\n",
    "        try:\n",
    "            # Initialize with timeout\n",
    "            run = wandb.init(\n",
    "                project=wandb_config.get(\"project\", \"ouro-looped-transformer\"),\n",
    "                entity=wandb_config.get(\"entity\", None),\n",
    "                name=wandb_config.get(\"run_name\", f\"run_{int(time.time())}\"),\n",
    "                config=config,\n",
    "                # ‚≠ê CRITICAL PARAMETERS TO PREVENT HANGS\n",
    "                mode=wandb_config.get(\"mode\", \"online\"),\n",
    "                reinit=wandb_config.get(\"reinit\", True),\n",
    "                resume=wandb_config.get(\"resume\", \"allow\"),\n",
    "                anonymous=wandb_config.get(\"anonymous\", \"never\"),\n",
    "                # Timeout settings\n",
    "                settings=wandb.Settings(\n",
    "                    start_timeout=wandb_config.get(\"timeout\", 30),\n",
    "                    _disable_stats=True,  # Disable system stats for speed\n",
    "                    _disable_meta=True,  # Disable metadata collection\n",
    "                    console=\"off\",  # Reduce console output\n",
    "                ),\n",
    "            )\n",
    "            print(\"‚úÖ W&B initialized successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è W&B initialization failed: {e}\")\n",
    "            print(\"Continuing without W&B...\")\n",
    "            use_wandb = False\n",
    "            run = None\n",
    "\n",
    "    # 2. Extract params\n",
    "    model_path = config[\"MODEL\"][\"path\"]\n",
    "    dtype = config[\"MODEL\"][\"dtype\"]\n",
    "    ut_steps_to_test = config[\"INFERENCE_STEPS\"]\n",
    "    data_config = config[\"DATA\"]\n",
    "    eval_settings = config[\"EVAL_SETTINGS\"]\n",
    "    calculate_ppl = eval_settings[\"calculate_perplexity\"]\n",
    "    early_exit_threshold = eval_settings[\"early_exit_threshold\"]\n",
    "\n",
    "    # 3. Initialize experiment with performance optimizations\n",
    "    experiment = OuroThinkingExperiment(\n",
    "        model_path,\n",
    "        dtype,\n",
    "        use_4bit_quant=config[\"MODEL\"].get(\n",
    "            \"use_4bit_quant\", True\n",
    "        ),  # Default to True for speed\n",
    "        use_torch_compile=config[\"MODEL\"].get(\n",
    "            \"use_torch_compile\", True\n",
    "        ),  # Default to True\n",
    "    )\n",
    "\n",
    "    # 4. ENFORCE DETERMINISM for reproducible comparisons\n",
    "    torch.manual_seed(42)  # Fixed seed for all runs\n",
    "    print(\"[‚úì] Deterministic mode enabled for reliable UT step comparison\")\n",
    "\n",
    "    # 5. Prepare Data (keep existing logic)\n",
    "    if data_config[\"load_existing\"]:\n",
    "        test_datasets = load_and_preprocess_data(data_config[\"data_file_path\"])\n",
    "    else:\n",
    "        print(\"Generating new test datasets...\")\n",
    "        test_datasets = create_test_datasets(data_config)\n",
    "\n",
    "    # 6. Prepare Perplexity Data (FIXED: create once for consistency)\n",
    "    perplexity_results = []\n",
    "    perplexity_data_for_eval = []\n",
    "    if calculate_ppl:\n",
    "        ppl_num_samples = eval_settings[\"ppl_num_samples\"]\n",
    "        perplexity_data_raw = create_perplexity_data(num_samples=ppl_num_samples)\n",
    "        perplexity_data_for_eval = [\"\\n\\n\".join(perplexity_data_raw)]\n",
    "        print(\n",
    "            f\"Perplexity Dataset: {len(perplexity_data_raw)} texts, {len(perplexity_data_for_eval[0])} chars\"\n",
    "        )\n",
    "\n",
    "    all_accuracy_time_results = []\n",
    "\n",
    "    # 7. MAIN LOOP: Test each UT step configuration\n",
    "    for ut_steps in ut_steps_to_test:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"üß™ ROBUST EXPERIMENT: UT Steps = {ut_steps}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        # A. Load fresh model instance (CRITICAL: avoid contamination)\n",
    "        model, tokenizer, cache, ut_config = experiment.load_model_with_ut_steps(\n",
    "            ut_steps, early_exit_threshold\n",
    "        )\n",
    "\n",
    "        # B. PRE-COMPUTE TEMPLATES (SPEED OPTIMIZATION)\n",
    "        # This must be done AFTER tokenizer is loaded\n",
    "        if not hasattr(experiment, \"_templates_precomputed\"):\n",
    "            experiment._build_task_templates(tokenizer)\n",
    "            experiment._templates_precomputed = True\n",
    "\n",
    "        # C. PART 1: PERPLEXITY EVALUATION (if enabled)\n",
    "        if calculate_ppl and perplexity_data_for_eval:\n",
    "            print(f\"üìâ Calculating Perplexity for UT={ut_steps}...\")\n",
    "            ppl, avg_loss = experiment.calculate_perplexity(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                perplexity_data_for_eval,\n",
    "                ut_steps,\n",
    "                max_length=eval_settings[\"ppl_max_length\"],\n",
    "                stride=eval_settings[\"ppl_stride\"],\n",
    "            )\n",
    "            perplexity_results.append(\n",
    "                {\"ut_steps\": ut_steps, \"perplexity\": ppl, \"avg_loss\": avg_loss}\n",
    "            )\n",
    "            print(f\"‚úÖ PPL (UT={ut_steps}): {ppl:.4f} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\"perplexity\": ppl, \"val_loss\": avg_loss, \"ut_steps\": ut_steps}\n",
    "                )\n",
    "\n",
    "        # D. PART 2: ACCURACY & INFERENCE TIME EVALUATION\n",
    "        # Use optimized generation parameters for fair comparison\n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": 1024,  # Increased from 256 for complex reasoning\n",
    "            \"do_sample\": False,  # Greedy decoding for determinism\n",
    "            \"repetition_penalty\": 1.0,  # Slight penalty to avoid loops\n",
    "            \"num_beams\": 1,  # Faster than beam search\n",
    "        }\n",
    "\n",
    "        for task_type, items in test_datasets.items():\n",
    "            print(f\"\\n  üìù Task: {task_type} ({len(items)} samples)\")\n",
    "            task_results = []\n",
    "            task_start_time = time.time()\n",
    "\n",
    "            # Create W&B table for detailed analysis\n",
    "            if use_wandb:\n",
    "                table = wandb.Table(\n",
    "                    columns=[\n",
    "                        \"difficulty\",\n",
    "                        \"input\",\n",
    "                        \"target\",\n",
    "                        \"prediction\",\n",
    "                        \"is_correct\",\n",
    "                        \"tokens\",\n",
    "                        \"time\",\n",
    "                        \"ut_steps\",\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Process each test item\n",
    "            for item in tqdm(items, desc=f\"  {task_type}\", leave=False):\n",
    "                prompt = item[\"prompt\"]\n",
    "                target = item[\"expected_answer\"]\n",
    "\n",
    "                # OPTIMIZED PREDICTION CALL (NO CACHE PARAMETER!)\n",
    "                result = experiment.predict_with_metrics_optimized(\n",
    "                    user_input=prompt,\n",
    "                    task_type=task_type,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    ut_steps=ut_steps,\n",
    "                    generation_config=generation_config,\n",
    "                )\n",
    "\n",
    "                # Robust correctness evaluation\n",
    "                pred_str = str(result[\"prediction\"]).strip().lower()\n",
    "                target_str = str(target).strip().lower()\n",
    "\n",
    "                # Handle numeric vs symbolic comparisons\n",
    "                if task_type == \"p_hop\":\n",
    "                    # For logic chains, compare letters directly\n",
    "                    is_correct = pred_str == target_str\n",
    "                else:\n",
    "                    # For numeric tasks, try numeric comparison\n",
    "                    try:\n",
    "                        pred_num = float(pred_str) if pred_str else 0\n",
    "                        target_num = float(target_str) if target_str else 0\n",
    "                        is_correct = (\n",
    "                            abs(pred_num - target_num) < 0.001\n",
    "                        )  # Tolerance for floats\n",
    "                    except ValueError:\n",
    "                        is_correct = pred_str == target_str  # Fallback to string\n",
    "\n",
    "                result_entry = {\n",
    "                    \"task_type\": task_type,\n",
    "                    \"difficulty\": item.get(\"difficulty\", \"unknown\"),\n",
    "                    \"test_input\": prompt,\n",
    "                    \"expected_answer\": target,\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"test_id\": generate_test_id(\n",
    "                        task_type, item.get(\"difficulty\", \"\"), prompt\n",
    "                    ),\n",
    "                    \"ut_steps\": ut_steps,  # Explicitly track UT steps\n",
    "                    **result,  # Merges time, tokens, prediction, etc.\n",
    "                }\n",
    "\n",
    "                task_results.append(result_entry)\n",
    "                all_accuracy_time_results.append(result_entry)\n",
    "\n",
    "                if use_wandb:\n",
    "                    table.add_data(\n",
    "                        item.get(\"difficulty\", \"unknown\"),\n",
    "                        prompt[:150] + (\"...\" if len(prompt) > 150 else \"\"),\n",
    "                        str(target),\n",
    "                        str(result[\"prediction\"]),\n",
    "                        is_correct,\n",
    "                        result[\"generated_tokens\"],\n",
    "                        result[\"generation_time\"],\n",
    "                        ut_steps,\n",
    "                    )\n",
    "\n",
    "            # Calculate task-level metrics\n",
    "            if task_results:\n",
    "                accuracy = sum(r[\"is_correct\"] for r in task_results) / len(\n",
    "                    task_results\n",
    "                )\n",
    "                avg_time = sum(r[\"generation_time\"] for r in task_results) / len(\n",
    "                    task_results\n",
    "                )\n",
    "                avg_tokens = sum(r[\"generated_tokens\"] for r in task_results) / len(\n",
    "                    task_results\n",
    "                )\n",
    "                task_duration = time.time() - task_start_time\n",
    "\n",
    "                print(\n",
    "                    f\"    üìä Results: Acc={accuracy:.2%} | Time={avg_time:.3f}s | \"\n",
    "                    f\"Tokens={avg_tokens:.0f} | Total={task_duration:.1f}s\"\n",
    "                )\n",
    "\n",
    "                if use_wandb:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            f\"{task_type}/accuracy\": accuracy,\n",
    "                            f\"{task_type}/avg_time\": avg_time,\n",
    "                            f\"{task_type}/avg_tokens\": avg_tokens,\n",
    "                            f\"{task_type}/total_duration\": task_duration,\n",
    "                            f\"{task_type}/predictions_table\": table,\n",
    "                            \"ut_steps\": ut_steps,\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                print(f\"    ‚ö†Ô∏è No results for {task_type}\")\n",
    "\n",
    "        # E. CRITICAL: CLEAN UP before next UT configuration\n",
    "        print(f\"\\n  üßπ Cleaning up UT={ut_steps} model...\")\n",
    "        del model, tokenizer, cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # Optional: Add a small delay to ensure cleanup\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        print(f\"‚úÖ Completed UT={ut_steps}\")\n",
    "\n",
    "    # 8. FINAL CLEANUP AND RESULTS\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"üèÅ EXPERIMENT COMPLETE\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Save results to files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Save accuracy results\n",
    "    if all_accuracy_time_results:\n",
    "        df_accuracy = pd.DataFrame(all_accuracy_time_results)\n",
    "        accuracy_file = f\"results_accuracy_{timestamp}.csv\"\n",
    "        df_accuracy.to_csv(accuracy_file, index=False)\n",
    "        print(f\"üìÅ Accuracy results saved: {accuracy_file}\")\n",
    "\n",
    "    # Save perplexity results\n",
    "    if perplexity_results:\n",
    "        df_ppl = pd.DataFrame(perplexity_results)\n",
    "        ppl_file = f\"results_perplexity_{timestamp}.csv\"\n",
    "        df_ppl.to_csv(ppl_file, index=False)\n",
    "        print(f\"üìÅ Perplexity results saved: {ppl_file}\")\n",
    "\n",
    "    # 7. Final W&B cleanup\n",
    "    if use_wandb and run:\n",
    "        try:\n",
    "            # Save final summary\n",
    "            df_results = pd.DataFrame(stats_results)\n",
    "\n",
    "            # Create summary table\n",
    "            summary_table = wandb.Table(dataframe=df_results)\n",
    "            run.log({\"results_summary\": summary_table})\n",
    "\n",
    "            # Calculate final metrics\n",
    "            final_accuracy = (\n",
    "                df_results[\"is_correct\"].mean() if not df_results.empty else 0\n",
    "            )\n",
    "            run.summary[\"final_accuracy\"] = final_accuracy\n",
    "            run.summary[\"total_samples\"] = len(df_results)\n",
    "\n",
    "            print(f\"‚úÖ Final accuracy logged to W&B: {final_accuracy:.2%}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Final W&B logging failed: {e}\")\n",
    "\n",
    "        finally:\n",
    "            # ‚≠ê ALWAYS finish the run to prevent hanging\n",
    "            wandb.finish()\n",
    "            print(\"‚úÖ W&B run completed successfully.\")\n",
    "\n",
    "    return all_accuracy_time_results, perplexity_results\n",
    "\n",
    "\n",
    "# Cell 9: Configuration (With W&B Support)\n",
    "# ====================================================================\n",
    "# CONFIGURATION DICTIONARY (UPDATED)\n",
    "# ====================================================================\n",
    "\n",
    "ExperimentConfig = {\n",
    "    # --- MODEL & ENVIRONMENT SETTINGS ---\n",
    "    \"MODEL\": {\n",
    "        \"path\": \"ByteDance/Ouro-1.4B-Thinking\",\n",
    "        \"dtype\": torch.float16,\n",
    "        \"use_4bit_quant\": False,\n",
    "        \"use_torch_compile\": True,\n",
    "    },\n",
    "    # --- UT STEPS / REASONING DEPTH ---\n",
    "    \"INFERENCE_STEPS\": [1, 2, 4, 8],  # Example: Run for different steps\n",
    "    # --- EVALUATION CONTROL ---\n",
    "    \"EVAL_SETTINGS\": {\n",
    "        \"calculate_perplexity\": True,  # Enable to test PPL fix\n",
    "        \"early_exit_threshold\": -1.0,\n",
    "        \"ppl_num_samples\": 30,\n",
    "        \"ppl_max_length\": 2048,\n",
    "        \"ppl_stride\": 512,\n",
    "    },\n",
    "    # --- W&B TRACKING (NEW) ---\n",
    "    \"WANDB\": {\n",
    "        \"enabled\": False,  # Enable W&B\n",
    "        \"project\": \"ouro-robust-analysis\",\n",
    "        \"run_name\": f\"test_run_{datetime.now().strftime('%H%M%S')}\",\n",
    "        \"entity\": None,\n",
    "        # ‚≠ê NEW: Anti-hang settings\n",
    "        \"timeout\": 30,  # Seconds to wait for W&B\n",
    "        \"anonymous\": \"never\",  # \"never\", \"allow\", or \"must\"\n",
    "        \"reinit\": True,  # Allow multiple runs in same session\n",
    "        \"resume\": \"allow\",  # \"allow\", \"must\", \"never\", \"auto\"\n",
    "        # For offline/network issues\n",
    "        \"mode\": \"offline\",  # \"online\", \"offline\", \"disabled\"\n",
    "        \"save_code\": False,  # Don't save code to avoid timeouts\n",
    "    },\n",
    "    # --- DATA CONFIGURATION ---\n",
    "    \"DATA\": {\n",
    "        \"load_existing\": False,\n",
    "        \"data_file_path\": \"path/to/your/existing_test_data.json\",\n",
    "        # 1. N-Ary Addition: Test working memory and arithmetic stability\n",
    "        \"n_ary\": {\n",
    "            # 4 is easy, 8 is medium, 12 starts to challenge 1.4B models\n",
    "            \"ops_levels\": [4, 8, 12],\n",
    "            # 50 samples ensures <15% margin of error\n",
    "            \"num_samples_per_level\": 1,\n",
    "        },\n",
    "        # 2. P-Hop Induction: Test multi-step logic chaining\n",
    "        \"p_hop\": {\n",
    "            # 2 hops is direct, 4 is standard, 6 tests \"long-horizon\" reasoning\n",
    "            \"hop_levels\": [2, 4, 6],\n",
    "            \"num_samples_per_level\": 1,\n",
    "        },\n",
    "        # 3. iGSM: Test math word problem understanding\n",
    "        \"igsm\": {\n",
    "            # 100 total (50 Type A, 50 Type B) gives a solid baseline\n",
    "            \"num_samples_total\": 1\n",
    "        },\n",
    "    },\n",
    "}\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:47.437258Z",
     "iopub.status.busy": "2025-12-11T16:12:47.436989Z",
     "iopub.status.idle": "2025-12-11T16:12:47.482686Z",
     "shell.execute_reply": "2025-12-11T16:12:47.481834Z",
     "shell.execute_reply.started": "2025-12-11T16:12:47.437211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# BATCH-OPTIMIZED EXPERIMENT WITH BACKWARD COMPATIBILITY\n",
    "# ====================================================================\n",
    "\n",
    "from linecache import cache\n",
    "\n",
    "\n",
    "class OuroBatchExperiment(OuroThinkingExperiment):\n",
    "    \"\"\"\n",
    "    Experiment class optimized for Transformers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        dtype=torch.float16,\n",
    "        use_4bit_quant=False,\n",
    "        use_torch_compile=False,\n",
    "        max_batch_size=4,\n",
    "        max_new_token=1024,\n",
    "    ):\n",
    "        super().__init__(model_path, dtype, use_4bit_quant, use_torch_compile)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_new_token = max_new_token\n",
    "\n",
    "    def prepare_batch_inputs(self, prompts: list[str], task_type: str) -> dict:\n",
    "        \"\"\"\n",
    "        Prepares inputs specifically for model.generate_batch().\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"task_templates\") or task_type not in self.task_templates:\n",
    "            raise ValueError(\"Templates not built. Call _build_task_templates first.\")\n",
    "\n",
    "        template = self.task_templates[task_type]\n",
    "\n",
    "        # 1. Tokenize all user prompts at once (Fast Rust Tokenizer)\n",
    "        batch_texts = [template[\"input_prefix\"] + p for p in prompts]\n",
    "        user_encodings = self.tokenizer(batch_texts, add_special_tokens=False)\n",
    "        user_ids_batch = [user_ids for user_ids in user_encodings[\"input_ids\"]]\n",
    "\n",
    "        # 2. Get static parts\n",
    "        static_ids = template[\"static_input_ids\"].squeeze(0).tolist()\n",
    "        force_ids = template[\"force_start_ids\"].squeeze(0).tolist()\n",
    "\n",
    "        # 3. Stitch sequences into a LIST of 1D Tensors\n",
    "        input_tensor_list = []\n",
    "\n",
    "        for user_ids in user_ids_batch:\n",
    "            # Stitch: [Static] + [User] + [Force Start]\n",
    "            full_seq = static_ids + user_ids + force_ids\n",
    "            input_tensor_list.append(full_seq)\n",
    "\n",
    "        return {\"input_tensor_list\": input_tensor_list, \"batch_size\": len(prompts)}\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def batch_predict_with_metrics(\n",
    "        self,\n",
    "        prompts: list[str],\n",
    "        task_type: str,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        ut_steps: int,\n",
    "        generation_config: Optional[dict] = None,\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Executes prediction using model.generate_batch() following the tutorial pattern.\n",
    "        \"\"\"\n",
    "        if not prompts:\n",
    "            return []\n",
    "\n",
    "        # 1. Prepare Inputs (List of 1D Tensors)\n",
    "        batch_data = self.prepare_batch_inputs(prompts, task_type)\n",
    "        simple_batch_inputs = batch_data[\"input_tensor_list\"]\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            max_new_tokens=self.max_new_token,\n",
    "            use_cuda_graph=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            max_batch_tokens=self.max_batch_size * self.max_new_token,\n",
    "        )\n",
    "\n",
    "        # 3. Generate (Continuous Batching)\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # === THE TUTORIAL METHOD ===\n",
    "        batch_outputs = model.generate_batch(\n",
    "            inputs=simple_batch_inputs,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "\n",
    "        batch_generation_time = time.perf_counter() - start_time\n",
    "\n",
    "        # 4. Process Results\n",
    "        results = [None] * len(prompts)  # Pre-allocate to maintain order\n",
    "        template = self.task_templates[task_type]\n",
    "\n",
    "        # Iterate over the batch_outputs (which is a dict-like object where keys are request indices)\n",
    "        # Note: In generate_batch, if input is a list, keys are usually integers 0, 1, 2...\n",
    "        for idx, (request_id, output) in enumerate(batch_outputs.items()):\n",
    "            # According to tutorial: output.generated_tokens contains the IDs\n",
    "            generated_ids = output.generated_tokens\n",
    "\n",
    "            # Decode\n",
    "            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Reconstruct full response for answer extraction\n",
    "            # We assume the model generated the continuation of the \"Force Start\"\n",
    "            full_response = template[\"force_start_text\"] + generated_text\n",
    "\n",
    "            # Extract prediction\n",
    "            pred = self._extract_final_answer(full_response, task_type)\n",
    "\n",
    "            # Calculate metrics\n",
    "            sample_time = batch_generation_time / len(prompts)\n",
    "\n",
    "            results[idx] = {\n",
    "                \"full_response\": full_response,\n",
    "                \"prediction\": pred,\n",
    "                \"generation_time\": sample_time,\n",
    "                \"generated_tokens\": len(generated_ids),\n",
    "                \"input_tokens\": len(simple_batch_inputs[idx]),\n",
    "                \"ut_steps\": ut_steps,\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# UPDATED RUN EXPERIMENT FUNCTION WITH BATCH SUPPORT\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "def run_batch_compatible_experiment(config: dict):\n",
    "    \"\"\"\n",
    "    RUN ROBUST EXPERIMENT WITH BATCH SUPPORT:\n",
    "    Optimized for speed while maintaining EXACT output format compatibility.\n",
    "\n",
    "    Returns: (all_accuracy_time_results, perplexity_results) - SAME AS ORIGINAL\n",
    "    \"\"\"\n",
    "    # 1. CHECK W&B SETUP BEFORE STARTING (same as original)\n",
    "    use_wandb = config.get(\"WANDB\", {}).get(\"enabled\", False)\n",
    "    run = None\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb_config = config[\"WANDB\"]\n",
    "        print(f\"üîó Initializing W&B (timeout: {wandb_config.get('timeout', 30)}s)...\")\n",
    "\n",
    "        os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "        os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
    "\n",
    "        try:\n",
    "            run = wandb.init(\n",
    "                project=wandb_config.get(\"project\", \"ouro-looped-transformer\"),\n",
    "                entity=wandb_config.get(\"entity\", None),\n",
    "                name=wandb_config.get(\"run_name\", f\"run_{int(time.time())}\"),\n",
    "                config=config,\n",
    "                mode=wandb_config.get(\"mode\", \"online\"),\n",
    "                reinit=wandb_config.get(\"reinit\", True),\n",
    "                resume=wandb_config.get(\"resume\", \"allow\"),\n",
    "                anonymous=wandb_config.get(\"anonymous\", \"never\"),\n",
    "                settings=wandb.Settings(\n",
    "                    start_timeout=wandb_config.get(\"timeout\", 30),\n",
    "                    _disable_stats=True,\n",
    "                    _disable_meta=True,\n",
    "                    console=\"off\",\n",
    "                ),\n",
    "            )\n",
    "            print(\"‚úÖ W&B initialized successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è W&B initialization failed: {e}\")\n",
    "            print(\"Continuing without W&B...\")\n",
    "            use_wandb = False\n",
    "            run = None\n",
    "\n",
    "    # 2. Extract params (same as original)\n",
    "    model_path = config[\"MODEL\"][\"path\"]\n",
    "    dtype = config[\"MODEL\"][\"dtype\"]\n",
    "    ut_steps_to_test = config[\"INFERENCE_STEPS\"]\n",
    "    data_config = config[\"DATA\"]\n",
    "    eval_settings = config[\"EVAL_SETTINGS\"]\n",
    "    calculate_ppl = eval_settings[\"calculate_perplexity\"]\n",
    "    early_exit_threshold = eval_settings[\"early_exit_threshold\"]\n",
    "\n",
    "    # 3. Initialize batch-aware experiment\n",
    "    experiment = OuroBatchExperiment(\n",
    "        model_path,\n",
    "        dtype,\n",
    "        use_4bit_quant=config[\"MODEL\"].get(\"use_4bit_quant\", True),\n",
    "        use_torch_compile=config[\"MODEL\"].get(\"use_torch_compile\", True),\n",
    "        max_batch_size=config.get(\"OPTIMIZATION\", {}).get(\"max_batch_size\", 4),\n",
    "        max_new_token=config.get(\"OPTIMIZATION\", {}).get(\"max_new_token\", 1024),\n",
    "    )\n",
    "\n",
    "    # 4. ENFORCE DETERMINISM (same as original)\n",
    "    torch.manual_seed(42)\n",
    "    print(\"Deterministic mode enabled for reliable UT step comparison\")\n",
    "\n",
    "    # 5. Prepare Data (same as original)\n",
    "    if data_config[\"load_existing\"]:\n",
    "        test_datasets = load_and_preprocess_data(data_config[\"data_file_path\"])\n",
    "    else:\n",
    "        print(\"Generating new test datasets...\")\n",
    "        test_datasets = create_test_datasets(data_config)\n",
    "\n",
    "    # 6. Prepare Perplexity Data (same as original)\n",
    "    perplexity_results = []\n",
    "    perplexity_data_for_eval = []\n",
    "    if calculate_ppl:\n",
    "        ppl_num_samples = eval_settings[\"ppl_num_samples\"]\n",
    "        perplexity_data_raw = create_perplexity_data(num_samples=ppl_num_samples)\n",
    "        perplexity_data_for_eval = [\"\\n\\n\".join(perplexity_data_raw)]\n",
    "        print(\n",
    "            f\"Perplexity Dataset: {len(perplexity_data_raw)} texts, {len(perplexity_data_for_eval[0])} chars\"\n",
    "        )\n",
    "\n",
    "    all_accuracy_time_results = []\n",
    "\n",
    "    # 7. MAIN LOOP: Test each UT step configuration\n",
    "    for ut_steps in ut_steps_to_test:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"üß™ ROBUST EXPERIMENT WITH BATCH: UT Steps = {ut_steps}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        # A. Load fresh model instance (same as original)\n",
    "        model, tokenizer, cache, ut_config = experiment.load_model_with_ut_steps(\n",
    "            ut_steps, early_exit_threshold\n",
    "        )\n",
    "\n",
    "        # B. PRE-COMPUTE TEMPLATES (same as original)\n",
    "        if not hasattr(experiment, \"_templates_precomputed\"):\n",
    "            experiment._build_task_templates(tokenizer)\n",
    "            experiment._templates_precomputed = True\n",
    "\n",
    "        # C. PART 1: PERPLEXITY EVALUATION (same as original)\n",
    "        if calculate_ppl and perplexity_data_for_eval:\n",
    "            print(f\"üìâ Calculating Perplexity for UT={ut_steps}...\")\n",
    "            ppl, avg_loss = experiment.calculate_perplexity(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                perplexity_data_for_eval,\n",
    "                ut_steps,\n",
    "                max_length=eval_settings[\"ppl_max_length\"],\n",
    "                stride=eval_settings[\"ppl_stride\"],\n",
    "            )\n",
    "            perplexity_results.append(\n",
    "                {\"ut_steps\": ut_steps, \"perplexity\": ppl, \"avg_loss\": avg_loss}\n",
    "            )\n",
    "            print(f\"‚úÖ PPL (UT={ut_steps}): {ppl:.4f} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\"perplexity\": ppl, \"val_loss\": avg_loss, \"ut_steps\": ut_steps}\n",
    "                )\n",
    "\n",
    "        # D. PART 2: ACCURACY & INFERENCE TIME EVALUATION WITH BATCH OPTIMIZATION\n",
    "\n",
    "        # Determine batch processing strategy\n",
    "        enable_batch = config.get(\"OPTIMIZATION\", {}).get(\"enable_batch\", True)\n",
    "\n",
    "        for task_type, items in test_datasets.items():\n",
    "            print(f\"\\n  üìù Task: {task_type} ({len(items)} samples)\")\n",
    "            task_results = []\n",
    "            task_start_time = time.time()\n",
    "\n",
    "            # Create W&B table (same as original)\n",
    "            if use_wandb:\n",
    "                table = wandb.Table(\n",
    "                    columns=[\n",
    "                        \"difficulty\",\n",
    "                        \"input\",\n",
    "                        \"target\",\n",
    "                        \"prediction\",\n",
    "                        \"is_correct\",\n",
    "                        \"tokens\",\n",
    "                        \"time\",\n",
    "                        \"ut_steps\",\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Determine batch size for this task type\n",
    "            if enable_batch:\n",
    "                task_batch_sizes = {\n",
    "                    \"n_ary\": min(8, experiment.max_batch_size),\n",
    "                    \"p_hop\": min(4, experiment.max_batch_size),\n",
    "                    \"igsm\": min(2, experiment.max_batch_size),\n",
    "                }\n",
    "                batch_size = task_batch_sizes.get(task_type, 1)\n",
    "                print(f\"    Using batch size: {batch_size}\")\n",
    "            else:\n",
    "                batch_size = 1\n",
    "\n",
    "            # Process items (either batched or sequential)\n",
    "            if batch_size > 1 and len(items) >= 2:\n",
    "                # BATCH PROCESSING MODE\n",
    "                for batch_start in range(0, len(items), batch_size):\n",
    "                    batch_end = min(batch_start + batch_size, len(items))\n",
    "                    batch_items = items[batch_start:batch_end]\n",
    "\n",
    "                    # Extract prompts and targets\n",
    "                    batch_prompts = [item[\"prompt\"] for item in batch_items]\n",
    "                    batch_targets = [item[\"expected_answer\"] for item in batch_items]\n",
    "\n",
    "                    # Run batch prediction\n",
    "                    batch_results = experiment.batch_predict_with_metrics(\n",
    "                        prompts=batch_prompts,\n",
    "                        task_type=task_type,\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        ut_steps=ut_steps,\n",
    "                    )\n",
    "\n",
    "                    # Process each result in the batch\n",
    "                    for result, item in zip(batch_results, batch_items):\n",
    "                        # Robust correctness evaluation (SAME LOGIC AS ORIGINAL)\n",
    "                        pred_str = str(result[\"prediction\"]).strip().lower()\n",
    "                        target_str = str(item[\"expected_answer\"]).strip().lower()\n",
    "\n",
    "                        if task_type == \"p_hop\":\n",
    "                            is_correct = pred_str == target_str\n",
    "                        else:\n",
    "                            try:\n",
    "                                pred_num = float(pred_str) if pred_str else 0\n",
    "                                target_num = float(target_str) if target_str else 0\n",
    "                                is_correct = abs(pred_num - target_num) < 0.001\n",
    "                            except ValueError:\n",
    "                                is_correct = pred_str == target_str\n",
    "\n",
    "                        # Create result entry with EXACT SAME KEYS as original\n",
    "                        result_entry = {\n",
    "                            \"task_type\": task_type,  # Note: original uses 'task_type', not 'task'\n",
    "                            \"difficulty\": item.get(\"difficulty\", \"unknown\"),\n",
    "                            \"test_input\": item[\"prompt\"],\n",
    "                            \"expected_answer\": item[\"expected_answer\"],\n",
    "                            \"is_correct\": is_correct,\n",
    "                            \"test_id\": generate_test_id(\n",
    "                                task_type, item.get(\"difficulty\", \"\"), item[\"prompt\"]\n",
    "                            ),\n",
    "                            \"ut_steps\": ut_steps,\n",
    "                            **result,  # This adds: full_response, prediction, generation_time, generated_tokens, input_tokens\n",
    "                        }\n",
    "\n",
    "                        task_results.append(result_entry)\n",
    "                        all_accuracy_time_results.append(result_entry)\n",
    "\n",
    "                        if use_wandb:\n",
    "                            table.add_data(\n",
    "                                item.get(\"difficulty\", \"unknown\"),\n",
    "                                item[\"prompt\"][:150]\n",
    "                                + (\"...\" if len(item[\"prompt\"]) > 150 else \"\"),\n",
    "                                str(item[\"expected_answer\"]),\n",
    "                                str(result[\"prediction\"]),\n",
    "                                is_correct,\n",
    "                                result[\"generated_tokens\"],\n",
    "                                result[\"generation_time\"],\n",
    "                                ut_steps,\n",
    "                            )\n",
    "            else:\n",
    "                # SEQUENTIAL PROCESSING (fallback, same as original)\n",
    "                for item in tqdm(items, desc=f\"  {task_type}\", leave=False):\n",
    "                    # Use original method for compatibility\n",
    "                    result = experiment.predict_with_metrics_optimized(\n",
    "                        user_input=item[\"prompt\"],\n",
    "                        task_type=task_type,\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        ut_steps=ut_steps,\n",
    "                        generation_config={\n",
    "                            \"max_new_tokens\": 1024,\n",
    "                            \"do_sample\": False,\n",
    "                            \"repetition_penalty\": 1.0,\n",
    "                            \"num_beams\": 1,\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "                    # Robust correctness evaluation (SAME AS ORIGINAL)\n",
    "                    pred_str = str(result[\"prediction\"]).strip().lower()\n",
    "                    target_str = str(item[\"expected_answer\"]).strip().lower()\n",
    "\n",
    "                    if task_type == \"p_hop\":\n",
    "                        is_correct = pred_str == target_str\n",
    "                    else:\n",
    "                        try:\n",
    "                            pred_num = float(pred_str) if pred_str else 0\n",
    "                            target_num = float(target_str) if target_str else 0\n",
    "                            is_correct = abs(pred_num - target_num) < 0.001\n",
    "                        except ValueError:\n",
    "                            is_correct = pred_str == target_str\n",
    "\n",
    "                    # Create result entry with EXACT SAME KEYS as original\n",
    "                    result_entry = {\n",
    "                        \"task_type\": task_type,\n",
    "                        \"difficulty\": item.get(\"difficulty\", \"unknown\"),\n",
    "                        \"test_input\": item[\"prompt\"],\n",
    "                        \"expected_answer\": item[\"expected_answer\"],\n",
    "                        \"is_correct\": is_correct,\n",
    "                        \"test_id\": generate_test_id(\n",
    "                            task_type, item.get(\"difficulty\", \"\"), item[\"prompt\"]\n",
    "                        ),\n",
    "                        \"ut_steps\": ut_steps,\n",
    "                        **result,\n",
    "                    }\n",
    "\n",
    "                    task_results.append(result_entry)\n",
    "                    all_accuracy_time_results.append(result_entry)\n",
    "\n",
    "                    if use_wandb:\n",
    "                        table.add_data(\n",
    "                            item.get(\"difficulty\", \"unknown\"),\n",
    "                            item[\"prompt\"][:150]\n",
    "                            + (\"...\" if len(item[\"prompt\"]) > 150 else \"\"),\n",
    "                            str(item[\"expected_answer\"]),\n",
    "                            str(result[\"prediction\"]),\n",
    "                            is_correct,\n",
    "                            result[\"generated_tokens\"],\n",
    "                            result[\"generation_time\"],\n",
    "                            ut_steps,\n",
    "                        )\n",
    "\n",
    "            # Calculate task-level metrics (SAME AS ORIGINAL)\n",
    "            if task_results:\n",
    "                accuracy = sum(r[\"is_correct\"] for r in task_results) / len(\n",
    "                    task_results\n",
    "                )\n",
    "                avg_time = sum(r[\"generation_time\"] for r in task_results) / len(\n",
    "                    task_results\n",
    "                )\n",
    "                avg_tokens = sum(r[\"generated_tokens\"] for r in task_results) / len(\n",
    "                    task_results\n",
    "                )\n",
    "                task_duration = time.time() - task_start_time\n",
    "\n",
    "                print(\n",
    "                    f\"    üìä Results: Acc={accuracy:.2%} | Time={avg_time:.3f}s | \"\n",
    "                    f\"Tokens={avg_tokens:.0f} | Total={task_duration:.1f}s\"\n",
    "                )\n",
    "\n",
    "                if use_wandb:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            f\"{task_type}/accuracy\": accuracy,\n",
    "                            f\"{task_type}/avg_time\": avg_time,\n",
    "                            f\"{task_type}/avg_tokens\": avg_tokens,\n",
    "                            f\"{task_type}/total_duration\": task_duration,\n",
    "                            f\"{task_type}/predictions_table\": table,\n",
    "                            \"ut_steps\": ut_steps,\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                print(f\"    ‚ö†Ô∏è No results for {task_type}\")\n",
    "\n",
    "        # E. CRITICAL: CLEAN UP before next UT configuration (same as original)\n",
    "        print(f\"\\n  üßπ Cleaning up UT={ut_steps} model...\")\n",
    "        del model, tokenizer, cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # Optional: Add a small delay to ensure cleanup\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        print(f\"‚úÖ Completed UT={ut_steps}\")\n",
    "\n",
    "    # 8. FINAL CLEANUP AND RESULTS (SAME AS ORIGINAL)\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"üèÅ EXPERIMENT COMPLETE\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Save results to files (SAME FORMAT AS ORIGINAL)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Save accuracy results with same format\n",
    "    if all_accuracy_time_results:\n",
    "        df_accuracy = pd.DataFrame(all_accuracy_time_results)\n",
    "        accuracy_file = f\"results_accuracy_{timestamp}.csv\"\n",
    "        df_accuracy.to_csv(accuracy_file, index=False)\n",
    "        print(f\"üìÅ Accuracy results saved: {accuracy_file}\")\n",
    "\n",
    "    # Save perplexity results with same format\n",
    "    if perplexity_results:\n",
    "        df_ppl = pd.DataFrame(perplexity_results)\n",
    "        ppl_file = f\"results_perplexity_{timestamp}.csv\"\n",
    "        df_ppl.to_csv(ppl_file, index=False)\n",
    "        print(f\"üìÅ Perplexity results saved: {ppl_file}\")\n",
    "\n",
    "    # Generate summary statistics with same format\n",
    "    if all_accuracy_time_results:\n",
    "        print(\"\\nüìà SUMMARY STATISTICS:\")\n",
    "        summary_stats = analyze_experiment_results(\n",
    "            all_accuracy_time_results, perplexity_results\n",
    "        )\n",
    "        print(summary_stats.to_string())\n",
    "\n",
    "    # 9. Final W&B cleanup (same as original)\n",
    "    if use_wandb and run:\n",
    "        try:\n",
    "            # Calculate final metrics\n",
    "            final_accuracy = (\n",
    "                df_accuracy[\"is_correct\"].mean() if not df_accuracy.empty else 0\n",
    "            )\n",
    "\n",
    "            run.summary[\"final_accuracy\"] = final_accuracy\n",
    "            run.summary[\"total_samples\"] = len(df_accuracy)\n",
    "\n",
    "            print(f\"‚úÖ Final accuracy logged to W&B: {final_accuracy:.2%}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Final W&B logging failed: {e}\")\n",
    "\n",
    "        finally:\n",
    "            wandb.finish()\n",
    "            print(\"‚úÖ W&B run completed successfully.\")\n",
    "\n",
    "    # Return EXACTLY the same format as original run_robust_experiment_with_config\n",
    "    return all_accuracy_time_results, perplexity_results\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# COMPATIBILITY WRAPPER FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "def analyze_experiment_results(accuracy_results, perplexity_results=None):\n",
    "    \"\"\"\n",
    "    Compatibility wrapper for results analysis.\n",
    "    Returns pandas DataFrame with summary statistics.\n",
    "    \"\"\"\n",
    "    if not accuracy_results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(accuracy_results)\n",
    "\n",
    "    # Group by UT steps and task type (same logic as original)\n",
    "    summary = (\n",
    "        df.groupby([\"ut_steps\", \"task_type\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"is_correct\": [\"mean\", \"count\", \"std\"],\n",
    "                \"generation_time\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "                \"generated_tokens\": [\"mean\", \"std\"],\n",
    "            }\n",
    "        )\n",
    "        .round(3)\n",
    "    )\n",
    "\n",
    "    # Flatten column names\n",
    "    summary.columns = [\"_\".join(col).strip() for col in summary.columns.values]\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# COMPATIBLE CONFIGURATION\n",
    "# ====================================================================\n",
    "\n",
    "BatchCompatibleConfig = {\n",
    "    # --- MODEL & ENVIRONMENT SETTINGS (same keys) ---\n",
    "    \"MODEL\": {\n",
    "        \"path\": \"ByteDance/Ouro-1.4B-Thinking\",\n",
    "        \"dtype\": torch.float16,  # Changed back to match original\n",
    "        \"use_4bit_quant\": False,\n",
    "        \"use_torch_compile\": True,\n",
    "    },\n",
    "    # --- UT STEPS / REASONING DEPTH (same keys) ---\n",
    "    \"INFERENCE_STEPS\": [1],\n",
    "    # --- EVALUATION CONTROL (same keys) ---\n",
    "    \"EVAL_SETTINGS\": {\n",
    "        \"calculate_perplexity\": True,\n",
    "        \"early_exit_threshold\": -1.0,\n",
    "        \"ppl_num_samples\": 30,\n",
    "        \"ppl_max_length\": 2048,\n",
    "        \"ppl_stride\": 512,\n",
    "    },\n",
    "    # --- W&B TRACKING (same keys) ---\n",
    "    \"WANDB\": {\n",
    "        \"enabled\": True,\n",
    "        \"project\": \"ouro-looped-transformer\",\n",
    "        \"run_name\": f\"run_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        \"entity\": None,\n",
    "        \"mode\": \"offline\",\n",
    "    },\n",
    "    # --- DATA CONFIGURATION (same keys) ---\n",
    "    \"DATA\": {\n",
    "        \"load_existing\": False,\n",
    "        \"data_file_path\": \"\",\n",
    "        \"n_ary\": {\"ops_levels\": [4, 8], \"num_samples_per_level\": 20},\n",
    "        \"p_hop\": {\"hop_levels\": [2, 4], \"num_samples_per_level\": 20},\n",
    "        \"igsm\": {\"num_samples_total\": 20},\n",
    "    },\n",
    "    # --- BATCH OPTIMIZATION (NEW but optional) ---\n",
    "    \"OPTIMIZATION\": {\n",
    "        \"enable_batch\": True,\n",
    "        \"max_batch_size\": 4,\n",
    "        \"min_items_for_batch\": 2,\n",
    "        \"max_new_token\": 1024,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:12:47.484029Z",
     "iopub.status.busy": "2025-12-11T16:12:47.483740Z",
     "iopub.status.idle": "2025-12-11T16:23:43.064858Z",
     "shell.execute_reply": "2025-12-11T16:23:43.064200Z",
     "shell.execute_reply.started": "2025-12-11T16:12:47.484002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïí Experiment Timestamp: 20251211_161247\n",
      "\n",
      "======================================================================\n",
      "STARTING COMPREHENSIVE EXPERIMENT\n",
      "======================================================================\n",
      "üîó Initializing W&B (timeout: 30s)...\n",
      "‚ö†Ô∏è W&B initialization failed: 1 validation error for Settings\n",
      "start_timeout\n",
      "  Extra inputs are not permitted [type=extra_forbidden, input_value=30, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\n",
      "Continuing without W&B...\n",
      "Deterministic mode enabled for reliable UT step comparison\n",
      "Generating new test datasets...\n",
      "Perplexity Dataset: 30 texts, 10824 chars\n",
      "\n",
      "======================================================================\n",
      "üß™ ROBUST EXPERIMENT WITH BATCH: UT Steps = 1\n",
      "======================================================================\n",
      "Loading config for UT steps: 1, Early Exit: -1.0\n",
      "AutoTokenizer: padding_side: left\n",
      " -> Use torch.compile() for inference optimization\n",
      "Config loaded:\n",
      "   - total_ut_steps: 1\n",
      "   - num_hidden_layers: 24\n",
      "   - Total cache indices needed: 24\n",
      "Model loaded! Using device: cuda:0, configured UT steps: 1\n",
      "[+] Pre-computed task templates for faster inference.\n",
      "üìâ Calculating Perplexity for UT=1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea7bfc3b76c431abb0cd368058ecd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating PPL (UT=1):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PPL (UT=1): 16.2854 | Loss: 2.7903\n",
      "\n",
      "  üìù Task: n_ary (40 samples)\n",
      "    Using batch size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.54s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.46s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.43s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.46s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.10s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:08<00:00,  2.00s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.42request/s]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:07<00:00,  1.97s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.38request/s]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:07<00:00,  1.94s/request]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìä Results: Acc=15.00% | Time=1.418s | Tokens=88 | Total=56.7s\n",
      "\n",
      "  üìù Task: p_hop (40 samples)\n",
      "    Using batch size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:44<00:00, 11.13s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:44<00:00, 11.23s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:44<00:00, 11.09s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:46<00:00, 11.51s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:42<00:00, 10.70s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:46<00:00, 11.55s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:45<00:00, 11.33s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:45<00:00, 11.40s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:44<00:00, 11.06s/request]\n",
      "Solving 4 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:45<00:00, 11.49s/request]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìä Results: Acc=0.00% | Time=11.265s | Tokens=780 | Total=450.6s\n",
      "\n",
      "  üìù Task: igsm (20 samples)\n",
      "    Using batch size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.11s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.71s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.70s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.55s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.41s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.71s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.59s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.10s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:40<00:00, 20.19s/request]\n",
      "Solving 2 requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.69s/request]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìä Results: Acc=0.00% | Time=5.002s | Tokens=179 | Total=100.1s\n",
      "\n",
      "  üßπ Cleaning up UT=1 model...\n",
      "‚úÖ Completed UT=1\n",
      "\n",
      "======================================================================\n",
      "üèÅ EXPERIMENT COMPLETE\n",
      "======================================================================\n",
      "üìÅ Accuracy results saved: results_accuracy_20251211_162342.csv\n",
      "üìÅ Perplexity results saved: results_perplexity_20251211_162342.csv\n",
      "\n",
      "üìà SUMMARY STATISTICS:\n",
      "                    is_correct_mean  is_correct_count  is_correct_std  generation_time_mean  generation_time_std  generation_time_min  generation_time_max  generated_tokens_mean  generated_tokens_std\n",
      "ut_steps task_type                                                                                                                                                                                     \n",
      "1        igsm                  0.00                20           0.000                 5.002                5.214                2.728               20.211                179.150               200.346\n",
      "         n_ary                 0.15                40           0.362                 1.418                0.510                0.426                2.018                 88.350                51.445\n",
      "         p_hop                 0.00                40           0.000                11.265                0.252               10.712               11.561                779.575               403.445\n",
      "\n",
      "==================================================\n",
      "RESULTS SAVED\n",
      "==================================================\n",
      "üìÑ Accuracy Results: /kaggle/working/ouro_acc_results_20251211_161247.csv\n",
      "üìÑ Perplexity Results: /kaggle/working/ouro_ppl_results_20251211_161247.csv\n",
      "‚öôÔ∏è Config YAML:      /kaggle/working/ouro_config_20251211_161247.yaml\n",
      "\n",
      "--- FINAL PERPLEXITY RESULTS ---\n",
      "   ut_steps  perplexity  avg_loss\n",
      "0         1    16.28541   2.79027\n",
      "\n",
      "--- ACCURACY/TIME SAMPLE (Head) ---\n",
      "  task_type  ut_steps  is_correct  generation_time prediction\n",
      "0     n_ary         1       False         1.549132       1950\n",
      "1     n_ary         1       False         1.549132       2860\n",
      "2     n_ary         1        True         1.549132       2235\n",
      "3     n_ary         1        True         1.549132       2568\n",
      "4     n_ary         1       False         1.478654       1309\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Execution and Saving with Timestamp & YAML\n",
    "import os\n",
    "\n",
    "# 1. Generate Timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"üïí Experiment Timestamp: {timestamp}\")\n",
    "\n",
    "# 2. Run Experiment\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING COMPREHENSIVE EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# results_accuracy_time, results_ppl = run_experiment_with_config(ExperimentConfig)\n",
    "# results_accuracy_time, results_ppl = run_robust_experiment_with_config(ExperimentConfig)\n",
    "results_accuracy_time, results_ppl = run_batch_compatible_experiment(\n",
    "    BatchCompatibleConfig\n",
    ")\n",
    "# 3. Process Results\n",
    "df_acc = pd.DataFrame(results_accuracy_time)\n",
    "df_ppl = pd.DataFrame(results_ppl)\n",
    "\n",
    "# 4. Define Output Filenames with Timestamp\n",
    "# Add timestamp to filenames for better management\n",
    "output_acc_filename = f\"ouro_acc_results_{timestamp}.csv\"\n",
    "output_ppl_filename = f\"ouro_ppl_results_{timestamp}.csv\"\n",
    "output_config_filename = f\"ouro_config_{timestamp}.yaml\"\n",
    "\n",
    "output_acc_path = os.path.join(OUTPUT_PATH, output_acc_filename)\n",
    "output_ppl_path = os.path.join(OUTPUT_PATH, output_ppl_filename)\n",
    "output_config_path = os.path.join(OUTPUT_PATH, output_config_filename)\n",
    "\n",
    "# 5. Save DataFrames\n",
    "df_acc.to_csv(output_acc_path, index=False)\n",
    "if not df_ppl.empty:\n",
    "    df_ppl.to_csv(output_ppl_path, index=False)\n",
    "\n",
    "\n",
    "# 6. Save Config as YAML (Optimization)\n",
    "def sanitize_config(cfg):\n",
    "    \"\"\"Helper to convert non-serializable objects (like torch.dtype) to strings.\"\"\"\n",
    "    clean = {}\n",
    "    for k, v in cfg.items():\n",
    "        if isinstance(v, dict):\n",
    "            clean[k] = sanitize_config(v)\n",
    "        elif isinstance(v, list):\n",
    "            clean[k] = (\n",
    "                v  # Lists are usually fine, but deeper check might be needed if list contains obj\n",
    "            )\n",
    "        elif str(type(v)).find(\"torch.\") != -1:  # Catch torch.float16, etc.\n",
    "            clean[k] = str(v)\n",
    "        else:\n",
    "            clean[k] = v\n",
    "    return clean\n",
    "\n",
    "\n",
    "clean_config = sanitize_config(ExperimentConfig)\n",
    "with open(output_config_path, \"w\") as f:\n",
    "    yaml.dump(clean_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RESULTS SAVED\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìÑ Accuracy Results: {output_acc_path}\")\n",
    "print(f\"üìÑ Perplexity Results: {output_ppl_path}\")\n",
    "print(f\"‚öôÔ∏è Config YAML:      {output_config_path}\")\n",
    "\n",
    "# Print summary\n",
    "if not df_ppl.empty:\n",
    "    print(\"\\n--- FINAL PERPLEXITY RESULTS ---\")\n",
    "    print(df_ppl)\n",
    "\n",
    "print(\"\\n--- ACCURACY/TIME SAMPLE (Head) ---\")\n",
    "print(\n",
    "    df_acc[\n",
    "        [\"task_type\", \"ut_steps\", \"is_correct\", \"generation_time\", \"prediction\"]\n",
    "    ].head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:23:43.065914Z",
     "iopub.status.busy": "2025-12-11T16:23:43.065693Z",
     "iopub.status.idle": "2025-12-11T16:23:43.077097Z",
     "shell.execute_reply": "2025-12-11T16:23:43.076498Z",
     "shell.execute_reply.started": "2025-12-11T16:23:43.065896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_process_existing_results(data_source, is_filepath=True):\n",
    "    \"\"\"\n",
    "    Loads experiment results data, calculates the number of generated tokens,\n",
    "    and prepares the DataFrame for analysis.\n",
    "\n",
    "    Args:\n",
    "        data_source (str): The file path to the CSV or the raw CSV string data.\n",
    "        is_filepath (bool): True if data_source is a file path, False if it's\n",
    "                            the raw CSV string content.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A processed DataFrame ready for analysis, including\n",
    "                      the 'generated_tokens' feature.\n",
    "    \"\"\"\n",
    "    if is_filepath:\n",
    "        # Load from file path\n",
    "        try:\n",
    "            df = pd.read_csv(data_source)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n '{data_source}'.\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        # Load from raw string data\n",
    "        try:\n",
    "            df = pd.read_csv(StringIO(data_source))\n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói khi ƒë·ªçc d·ªØ li·ªáu chu·ªói: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    # --- 1. T√≠nh to√°n generated_tokens ---\n",
    "    # ƒê·∫øm tokens (s·ª≠ d·ª•ng kho·∫£ng tr·∫Øng ƒë·ªÉ ∆∞·ªõc t√≠nh) trong full_response.\n",
    "    # Th√™m 1 cho c√°c chu·ªói r·ªóng ƒë·ªÉ tr√°nh l·ªói chia cho 0 trong m·ªôt s·ªë ph√©p t√≠nh sau n√†y.\n",
    "    if \"generated_tokens\" in df.columns:\n",
    "        print(\"C·ªôt 'generated_tokens' ƒë√£ t·ªìn t·∫°i. B·ªè qua b∆∞·ªõc t√≠nh to√°n.\")\n",
    "    else:\n",
    "        df[\"generated_tokens\"] = df[\"full_response\"].apply(\n",
    "            lambda x: len(re.findall(r\"\\S+\", str(x))) if pd.notna(x) else 1\n",
    "        )\n",
    "\n",
    "    # --- 2. Chu·∫©n h√≥a ki·ªÉu d·ªØ li·ªáu ---\n",
    "    # Chuy·ªÉn ƒë·ªïi c√°c c·ªôt s·ªë sang ƒë·ªãnh d·∫°ng s·ªë ch√≠nh x√°c\n",
    "\n",
    "    # C·ªôt 'prediction' c√≥ th·ªÉ ch·ª©a s·ªë ho·∫∑c chu·ªói 'Final: X', n√™n c·∫ßn l√†m s·∫°ch tr∆∞·ªõc\n",
    "    def clean_prediction(pred):\n",
    "        if isinstance(pred, str):\n",
    "            # C·ªë g·∫Øng tr√≠ch xu·∫•t s·ªë t·ª´ c√°c ƒë·ªãnh d·∫°ng c√≥ th·ªÉ c√≥\n",
    "            match = re.search(r\"[\\d\\.]+$\", pred)\n",
    "            return float(match.group(0)) if match else None\n",
    "        return pred\n",
    "\n",
    "    df[\"prediction\"] = df[\"prediction\"].apply(clean_prediction)\n",
    "\n",
    "    # √âp ki·ªÉu c√°c c·ªôt s·ªë\n",
    "    numeric_cols = [\"prediction\", \"generation_time\", \"ut_steps\", \"expected_answer\"]\n",
    "    for col in numeric_cols:\n",
    "        # S·ª≠ d·ª•ng 'coerce' ƒë·ªÉ bi·∫øn c√°c gi√° tr·ªã kh√¥ng h·ª£p l·ªá th√†nh NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # √âp ki·ªÉu c√°c c·ªôt boolean/categorical\n",
    "    df[\"is_correct\"] = df[\"is_correct\"].astype(bool)\n",
    "    df[\"ut_steps\"] = df[\"ut_steps\"].astype(\"Int64\")  # S·ª≠ d·ª•ng Int64 ƒë·ªÉ cho ph√©p NaN\n",
    "\n",
    "    # C·∫Øt b·ªõt kho·∫£ng tr·∫Øng ·ªü ƒë·∫ßu/cu·ªëi c·ªßa c√°c c·ªôt chu·ªói\n",
    "    for col in [\"full_response\", \"task_type\", \"difficulty\", \"test_input\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    print(f\"‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i v√† x·ª≠ l√Ω th√†nh c√¥ng. T·ªïng s·ªë m·∫´u: {len(df)}\")\n",
    "    print(\n",
    "        f\"Th√™m c·ªôt 'generated_tokens' v·ªõi gi√° tr·ªã trung b√¨nh: {df['generated_tokens'].mean():.2f}\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def concatenate_results_files(\n",
    "    directory_path, output_filename=\"master_experiment_results.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenates all CSV files found in the specified directory into a single\n",
    "    DataFrame and saves it to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the folder containing the daily result CSV files.\n",
    "        output_filename (str): The name of the final merged CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: The final concatenated DataFrame, or None if no files are found.\n",
    "    \"\"\"\n",
    "    # X√¢y d·ª±ng pattern t√¨m ki·∫øm file CSV (t√¨m t·∫•t c·∫£ c√°c file .csv)\n",
    "    search_pattern = os.path.join(directory_path, \"*.csv\")\n",
    "\n",
    "    # S·ª≠ d·ª•ng glob ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c file ph√π h·ª£p\n",
    "    stats_files = glob.glob(search_pattern)\n",
    "\n",
    "    if not stats_files:\n",
    "        print(f\"Kh√¥ng t√¨m th·∫•y file CSV n√†o trong th∆∞ m·ª•c: {directory_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"T√¨m th·∫•y {len(stats_files)} file CSV. ƒêang ti·∫øn h√†nh gh√©p n·ªëi...\")\n",
    "\n",
    "    # T·∫°o danh s√°ch DataFrames\n",
    "    list_of_dfs = []\n",
    "\n",
    "    for filename in stats_files:\n",
    "        try:\n",
    "            # ƒê·ªçc t·ª´ng file CSV\n",
    "            df = pd.read_csv(filename)\n",
    "            list_of_dfs.append(df)\n",
    "            print(f\"ƒê√£ ƒë·ªçc file: {os.path.basename(filename)} ({len(df)} h√†ng)\")\n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói khi ƒë·ªçc file {filename}: {e}. B·ªè qua file n√†y.\")\n",
    "            continue\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(\"Kh√¥ng c√≥ DataFrame n√†o h·ª£p l·ªá ƒë·ªÉ gh√©p n·ªëi.\")\n",
    "        return None\n",
    "\n",
    "    # Gh√©p n·ªëi t·∫•t c·∫£ DataFrames\n",
    "    final_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    # L∆∞u file k·∫øt qu·∫£\n",
    "    output_path = os.path.join(directory_path, output_filename)\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"‚úÖ HO√ÄN T·∫§T: ƒê√£ gh√©p n·ªëi {len(list_of_dfs)} file.\")\n",
    "    print(f\"T·ªïng s·ªë h√†ng trong file cu·ªëi c√πng: {len(final_df)}\")\n",
    "    print(f\"File k·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_path}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:23:43.078355Z",
     "iopub.status.busy": "2025-12-11T16:23:43.078039Z",
     "iopub.status.idle": "2025-12-11T16:23:43.828101Z",
     "shell.execute_reply": "2025-12-11T16:23:43.827533Z",
     "shell.execute_reply.started": "2025-12-11T16:23:43.078330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "V·∫º ƒê·ªí TH·ªä TR·ª∞C QUAN H√ìA\n",
      "==================================================\n",
      "--- B·∫¢NG T√ìM T·∫ÆT K·∫æT QU·∫¢ THEO ƒê·ªò KH√ì V√Ä UT STEPS ---\n",
      "| task_type   | difficulty          |   ut_steps |   Accuracy |   Avg_Time |   Avg_Tokens |   Count |\n",
      "|:------------|:--------------------|-----------:|-----------:|-----------:|-------------:|--------:|\n",
      "| igsm        | 4_step_accumulation |          1 |       0.00 |    3.26324 |       136    |       4 |\n",
      "| igsm        | 5_step_accumulation |          1 |       0.00 |    6.42534 |       303.2  |       5 |\n",
      "| igsm        | 6_step_accumulation |          1 |       0.00 |    3.34128 |       142.6  |       5 |\n",
      "| igsm        | 7_step_accumulation |          1 |       0.00 |    6.35944 |       135    |       6 |\n",
      "| n_ary       | 4_ops               |          1 |       0.25 |    1.41237 |       105.9  |      20 |\n",
      "| n_ary       | 8_ops               |          1 |       0.05 |    1.42444 |        70.8  |      20 |\n",
      "| p_hop       | 2_hops_shuffled     |          1 |       0.00 |   11.1468  |       675.45 |      20 |\n",
      "| p_hop       | 4_hops_shuffled     |          1 |       0.00 |   11.3824  |       883.7  |      20 |\n",
      "\n",
      "--- B·∫¢NG T√ìM T·∫ÆT CHUNG (Task v√† UT Steps) ---\n",
      "| task_type   |   ut_steps |   Accuracy |   Avg_Time |\n",
      "|:------------|-----------:|-----------:|-----------:|\n",
      "| igsm        |          1 |       0    |    5.00213 |\n",
      "| n_ary       |          1 |       0.15 |    1.41841 |\n",
      "| p_hop       |          1 |       0    |   11.2646  |\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# IV. PH√ÇN T√çCH V√Ä TR·ª∞C QUAN H√ìA\n",
    "# ====================================================================\n",
    "\n",
    "# S·ª≠ d·ª•ng df_acc t·ª´ Cell 6\n",
    "if \"df_acc\" in locals() and not df_acc.empty:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"V·∫º ƒê·ªí TH·ªä TR·ª∞C QUAN H√ìA\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. T·∫°o b√°o c√°o t√≥m t·∫Øt\n",
    "    summary_report = (\n",
    "        df_acc.groupby([\"task_type\", \"difficulty\", \"ut_steps\"])\n",
    "        .agg(\n",
    "            Accuracy=(\"is_correct\", \"mean\"),\n",
    "            Avg_Time=(\"generation_time\", \"mean\"),\n",
    "            Avg_Tokens=(\"generated_tokens\", \"mean\"),\n",
    "            Count=(\"test_id\", \"count\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    print(\"--- B·∫¢NG T√ìM T·∫ÆT K·∫æT QU·∫¢ THEO ƒê·ªò KH√ì V√Ä UT STEPS ---\")\n",
    "    print(\n",
    "        summary_report.to_markdown(index=False, floatfmt=(\".2f\", \".2%\", \".2f\", \".2f\"))\n",
    "    )\n",
    "\n",
    "    # 2. T·∫°o b√°o c√°o chung (kh√¥ng ph√¢n t√°ch theo difficulty)\n",
    "    overall_summary = (\n",
    "        df_acc.groupby([\"task_type\", \"ut_steps\"])\n",
    "        .agg(\n",
    "            Accuracy=(\"is_correct\", \"mean\"),\n",
    "            Avg_Time=(\"generation_time\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- B·∫¢NG T√ìM T·∫ÆT CHUNG (Task v√† UT Steps) ---\")\n",
    "    print(overall_summary.to_markdown(index=False, floatfmt=(\".2%\", \".2f\")))\n",
    "\n",
    "    # 3. Tr·ª±c quan h√≥a d·ªØ li·ªáu\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Plot A: Accuracy vs UT Steps\n",
    "    sns.barplot(\n",
    "        data=overall_summary, x=\"ut_steps\", y=\"Accuracy\", hue=\"task_type\", ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(\"Accuracy vs. UT Steps\")\n",
    "    axes[0].set_xlabel(\"Total UT Steps\")\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "    axes[0].set_yticks([i / 10 for i in range(11)])\n",
    "    axes[0].yaxis.set_major_formatter(\n",
    "        plt.FuncFormatter(lambda y, _: \"{:.0%}\".format(y))\n",
    "    )\n",
    "\n",
    "    # Plot B: Inference Time vs UT Steps\n",
    "    time_summary = (\n",
    "        df_acc.groupby([\"ut_steps\", \"task_type\"])\n",
    "        .agg(Avg_Time=(\"generation_time\", \"mean\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    sns.barplot(\n",
    "        data=time_summary, x=\"ut_steps\", y=\"Avg_Time\", hue=\"task_type\", ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(\"Inference Time vs. UT Steps\")\n",
    "    axes[1].set_xlabel(\"Total UT Steps\")\n",
    "    axes[1].set_ylabel(\"Average Inference Time (s)\")\n",
    "\n",
    "    # Plot C: Verbosity (Tokens Generated)\n",
    "    sns.boxplot(\n",
    "        data=df_acc, x=\"ut_steps\", y=\"generated_tokens\", hue=\"task_type\", ax=axes[2]\n",
    "    )\n",
    "    axes[2].set_title(\"Token Count (Verbosity) vs. UT Steps\")\n",
    "    axes[2].set_xlabel(\"Total UT Steps\")\n",
    "    axes[2].set_ylabel(\"Generated Token Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y DataFrame k·∫øt qu·∫£ (df_acc) ƒë·ªÉ tr·ª±c quan h√≥a.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T16:23:43.829011Z",
     "iopub.status.busy": "2025-12-11T16:23:43.828806Z",
     "iopub.status.idle": "2025-12-11T16:24:34.938424Z",
     "shell.execute_reply": "2025-12-11T16:24:34.937628Z",
     "shell.execute_reply.started": "2025-12-11T16:23:43.828994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dffa0402f546718905c4c4cf53f8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ebcaabecb748ca99dfb3291211a272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e268d0c8d75042d6965f66ce76f8f391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0496c12267eb4ef5b6914886b95916d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1216930531304f8f846cdaeb410acc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b089ff9865f4c8c90ae46e2d0d88f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ec16a26b824405b27e34b48b133faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fea7f28aaa4b8884dd2ce92f3c7ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9622b1332f74c1a89002e60e7c272c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada2c403b79e478e9893b2be7e5ec9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05639e0224404e2fb92eb83615d93bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d095342c9447f08d597a825af1a859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddf00f58383413bb1a78d8697e69a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1462a788cc4b5bb81c35d748e9de5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "socratic/train-00000-of-00001.parquet:   0%|          | 0.00/2.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09656b55b5d4094a9f2a5c63016b6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "socratic/test-00000-of-00001.parquet:   0%|          | 0.00/487k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43bba5829964e64a37d6df5f630d797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e45dcd4e2046d89e9626ff914e966f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8a2e3f2d2b4c0592f6f1cedcf4bd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in generation loop: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 46.19 MiB is free. Process 12782 has 14.69 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 24.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/continuous_batching/continuous_api.py\", line 671, in _run_generation_loop\n",
      "    self._inner_generation_loop(batch_processor)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/metrics.py\", line 154, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/continuous_batching/continuous_api.py\", line 700, in _inner_generation_loop\n",
      "    self._generation_step(batch_processor)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/metrics.py\", line 154, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/continuous_batching/continuous_api.py\", line 602, in _generation_step\n",
      "    logits = self._model_forward(batch_data)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/metrics.py\", line 154, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/continuous_batching/continuous_api.py\", line 610, in _model_forward\n",
      "    return self.model(**batch_data).logits\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 940, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 480, in forward\n",
      "    outputs: BaseModelOutputWithPast = self.model(\n",
      "                                       ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 1064, in wrapper\n",
      "    outputs = func(self, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 410, in forward\n",
      "    hidden_states = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 260, in forward\n",
      "    hidden_states, _ = self.self_attn(\n",
      "                       ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 216, in forward\n",
      "    attn_output, attn_weights = attention_interface(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_paged.py\", line 40, in sdpa_attention_paged_forward\n",
      "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 46.19 MiB is free. Process 12782 has 14.69 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 24.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Solving 50 requests:   0%|          | 0/50 [00:12<?, ?request/s]ERROR:ContinuousBatchingLogger:Error in generation loop: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 46.19 MiB is free. Process 12782 has 14.69 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 24.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/continuous_batching/continuous_api.py\", line 671, in _run_generation_loop\n",
      "    self._inner_generation_loop(batch_processor)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/metrics.py\", line 154, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/continuous_batching/continuous_api.py\", line 700, in _inner_generation_loop\n",
      "    self._generation_step(batch_processor)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/metrics.py\", line 154, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/continuous_batching/continuous_api.py\", line 602, in _generation_step\n",
      "    logits = self._model_forward(batch_data)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/metrics.py\", line 154, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/continuous_batching/continuous_api.py\", line 610, in _model_forward\n",
      "    return self.model(**batch_data).logits\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 940, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 480, in forward\n",
      "    outputs: BaseModelOutputWithPast = self.model(\n",
      "                                       ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 1064, in wrapper\n",
      "    outputs = func(self, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 410, in forward\n",
      "    hidden_states = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 260, in forward\n",
      "    hidden_states, _ = self.self_attn(\n",
      "                       ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 216, in forward\n",
      "    attn_output, attn_weights = attention_interface(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_paged.py\", line 40, in sdpa_attention_paged_forward\n",
      "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 46.19 MiB is free. Process 12782 has 14.69 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 24.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Generation thread terminated unexpectedly.                      \n",
      "Solving 50 requests:   0%|          | 0/50 [00:13<?, ?request/s]ERROR:ContinuousBatchingLogger:Generation thread terminated unexpectedly.\n",
      "Solving 50 requests:   0%|          | 0/50 [00:13<?, ?request/s]\n"
     ]
    }
   ],
   "source": [
    "# TUTORIAL CODE FOR REFERENCE\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    attn_implementation=\"sdpa_paged\",\n",
    "    device_map=\"cuda\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\", padding_side=\"left\"\n",
    ")\n",
    "\n",
    "dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n",
    "dataset = dataset.select(range(50))\n",
    "tokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\n",
    "simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=32,\n",
    "    use_cuda_graph=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=False,\n",
    "    max_batch_tokens=512,\n",
    ")\n",
    "\n",
    "batch_outputs = model.generate_batch(\n",
    "    inputs=simple_batch_inputs,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "for request_id, output in batch_outputs.items():\n",
    "    generated_text = tokenizer.decode(output.generated_tokens, skip_special_tokens=True)\n",
    "    print(f\"Request {request_id} output: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8831282,
     "sourceId": 13862134,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8988780,
     "sourceId": 14111350,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8988959,
     "sourceId": 14111576,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 524172,
     "modelInstanceId": 509506,
     "sourceId": 672476,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
